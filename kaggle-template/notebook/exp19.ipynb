{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp19: Optunaによるハイパーパラメータ再最適化\n",
    "\n",
    "**ベースライン**: exp16 (MAE=48.98, 最良スコア)\n",
    "\n",
    "**背景**:\n",
    "- exp16が50特徴量で最良スコア（WeightedEnsemble_A MAE=48.98）\n",
    "- しかし、exp05のOptunaパラメータは古い特徴量セット（exp05時点）で最適化されたもの\n",
    "- exp16の特徴量セット（レジーム変化 + acc_get特徴量）に対して再最適化が必要\n",
    "\n",
    "**最適化対象**:\n",
    "1. **Ridge** - alphaの再最適化\n",
    "2. **ExtraTrees** - n_estimators, max_depth, min_samples等\n",
    "3. **HistGradientBoosting** - learning_rate, max_depth, l2_regularization等\n",
    "4. **CatBoost** - iterations, learning_rate, depth, l2_leaf_reg等\n",
    "5. **WeightedEnsemble_A** - 4モデルの最適な重み（Validationセットで最適化）\n",
    "\n",
    "**最適化戦略**:\n",
    "- Train/Validation/Testの3分割を使用\n",
    "- Validationセットでハイパーパラメータを最適化\n",
    "- Testセットで最終評価\n",
    "- 各モデル100trials実行\n",
    "\n",
    "**期待効果**:\n",
    "- exp16 (MAE=48.98) からさらに2-5%の改善を期待"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# データの読み込みと特徴量作成（exp16と同じ）\n",
    "# ==================================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Step 1: データの読み込み\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    calender = pd.read_csv('../input/calender_data.csv')\n",
    "    cm_data = pd.read_csv('../input/cm_data.csv')\n",
    "    gt_service = pd.read_csv('../input/gt_service_name.csv')\n",
    "    acc_get = pd.read_csv('../input/regi_acc_get_data_transform.csv')\n",
    "    call_data = pd.read_csv('../input/regi_call_data_transform.csv')\n",
    "    \n",
    "    calender['cdr_date'] = pd.to_datetime(calender['cdr_date'])\n",
    "    cm_data['cdr_date'] = pd.to_datetime(cm_data['cdr_date'])\n",
    "    acc_get['cdr_date'] = pd.to_datetime(acc_get['cdr_date'])\n",
    "    call_data['cdr_date'] = pd.to_datetime(call_data['cdr_date'])\n",
    "    gt_service['week'] = pd.to_datetime(gt_service['week'])\n",
    "    \n",
    "    print(f\"\\nデータ期間: {call_data['cdr_date'].min()} ~ {call_data['cdr_date'].max()}\")\n",
    "    \n",
    "    return calender, cm_data, gt_service, acc_get, call_data\n",
    "\n",
    "def merge_datasets(calender, cm_data, gt_service, acc_get, call_data):\n",
    "    df = call_data.copy()\n",
    "    df = df.merge(calender, on='cdr_date', how='left')\n",
    "    df = df.merge(cm_data, on='cdr_date', how='left')\n",
    "    df = df.merge(acc_get, on='cdr_date', how='left')\n",
    "    \n",
    "    gt_service_daily = []\n",
    "    for idx, row in gt_service.iterrows():\n",
    "        week_start = row['week']\n",
    "        for i in range(7):\n",
    "            date = week_start + timedelta(days=i)\n",
    "            gt_service_daily.append({'cdr_date': date, 'search_cnt': row['search_cnt']})\n",
    "    \n",
    "    gt_daily = pd.DataFrame(gt_service_daily)\n",
    "    df = df.merge(gt_daily, on='cdr_date', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_all_features(df):\n",
    "    \"\"\"exp16と同じ特徴量を作成\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 基本時系列特徴量\n",
    "    df['year'] = df['cdr_date'].dt.year\n",
    "    df['month'] = df['cdr_date'].dt.month\n",
    "    df['day_of_month'] = df['cdr_date'].dt.day\n",
    "    df['quarter'] = df['cdr_date'].dt.quarter\n",
    "    df['day_of_year'] = df['cdr_date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['cdr_date'].dt.isocalendar().week\n",
    "    df['days_from_start'] = (df['cdr_date'] - df['cdr_date'].min()).dt.days\n",
    "    df['is_month_start'] = (df['day_of_month'] <= 5).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] >= 25).astype(int)\n",
    "    \n",
    "    # ラグ特徴量\n",
    "    for lag in [1, 2, 3, 5, 7, 14, 30]:\n",
    "        df[f'lag_{lag}'] = df['call_num'].shift(lag)\n",
    "    \n",
    "    # 移動平均特徴量\n",
    "    for window in [3, 7, 14, 30]:\n",
    "        df[f'ma_{window}'] = df['call_num'].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        df[f'ma_std_{window}'] = df['call_num'].shift(1).rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    # 集約特徴量\n",
    "    df['cm_7d'] = df['cm_flg'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    df['gt_ma_7'] = df['search_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    df['acc_ma_7'] = df['acc_get_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    \n",
    "    df['dow_avg'] = np.nan\n",
    "    for dow in df['dow'].unique():\n",
    "        mask = df['dow'] == dow\n",
    "        df.loc[mask, 'dow_avg'] = df.loc[mask, 'call_num'].shift(1).expanding().mean()\n",
    "    \n",
    "    # acc_get特徴量（exp16）\n",
    "    df['acc_get_lag7'] = df['acc_get_cnt'].shift(7)\n",
    "    df['acc_get_sum_14d'] = df['acc_get_cnt'].shift(1).rolling(window=14, min_periods=1).sum()\n",
    "    \n",
    "    # レジーム変化特徴量（exp15）\n",
    "    tax_date = pd.Timestamp('2019-10-01')\n",
    "    rush_date = pd.Timestamp('2019-09-30')\n",
    "    \n",
    "    df['days_to_2019_10_01'] = (tax_date - df['cdr_date']).dt.days\n",
    "    df['is_pre_2019_10_01'] = (df['cdr_date'] < tax_date).astype(int)\n",
    "    df['is_post_2019_10_01'] = (df['cdr_date'] >= tax_date).astype(int)\n",
    "    df['days_to_2019_09_30'] = (rush_date - df['cdr_date']).dt.days\n",
    "    df['is_pre_2019_09_30'] = (df['cdr_date'] < rush_date).astype(int)\n",
    "    df['is_post_2019_09_30'] = (df['cdr_date'] >= rush_date).astype(int)\n",
    "    \n",
    "    rush_start = rush_date - pd.Timedelta(days=90)\n",
    "    df['is_rush_period'] = ((df['cdr_date'] >= rush_start) & (df['cdr_date'] <= rush_date)).astype(int)\n",
    "    \n",
    "    adaptation_end = tax_date + pd.Timedelta(days=30)\n",
    "    df['is_adaptation_period'] = ((df['cdr_date'] >= tax_date) & (df['cdr_date'] <= adaptation_end)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"データ読み込み・特徴量作成関数を定義しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# データ準備\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"*\" * 80)\n",
    "print(\"exp19: Optunaによるハイパーパラメータ再最適化\")\n",
    "print(\"*\" * 80)\n",
    "\n",
    "calender, cm_data, gt_service, acc_get, call_data = load_and_preprocess_data()\n",
    "df = merge_datasets(calender, cm_data, gt_service, acc_get, call_data)\n",
    "df = create_all_features(df)\n",
    "\n",
    "# 翌日の入電数を目的変数にする\n",
    "df['target_next_day'] = df['call_num'].shift(-1)\n",
    "df = df.dropna(subset=['target_next_day']).reset_index(drop=True)\n",
    "\n",
    "# 平日のみ\n",
    "df_model = df[df['dow'].isin([1, 2, 3, 4, 5])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n平日データ数: {len(df_model)}行\")\n",
    "print(f\"期間: {df_model['cdr_date'].min()} ~ {df_model['cdr_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Train/Validation/Test 分割\n",
    "# ==================================================================================\n",
    "\n",
    "# exp16と同じ特徴量リスト（50個）\n",
    "feature_cols = [\n",
    "    'dow', 'day_of_month', 'month', 'quarter', 'year', \n",
    "    'days_from_start', 'day_of_year', 'week_of_year',\n",
    "    'is_month_start', 'is_month_end',\n",
    "    'woy', 'wom', 'day_before_holiday_flag',\n",
    "    'cm_flg', 'acc_get_cnt', 'search_cnt',\n",
    "    'cm_7d', 'gt_ma_7', 'acc_ma_7', 'dow_avg',\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_5', 'lag_7', 'lag_14', 'lag_30',\n",
    "    'ma_3', 'ma_7', 'ma_14', 'ma_30',\n",
    "    'ma_std_3', 'ma_std_7', 'ma_std_14', 'ma_std_30',\n",
    "    'days_to_2019_10_01', 'is_pre_2019_10_01', 'is_post_2019_10_01',\n",
    "    'days_to_2019_09_30', 'is_pre_2019_09_30', 'is_post_2019_09_30',\n",
    "    'is_rush_period', 'is_adaptation_period',\n",
    "    'acc_get_lag7', 'acc_get_sum_14d'\n",
    "]\n",
    "\n",
    "# 欠損値を除去\n",
    "df_clean = df_model.dropna(subset=feature_cols + ['target_next_day']).copy()\n",
    "\n",
    "# Train/Val/Test分割（時系列考慮）\n",
    "max_date = df_clean['cdr_date'].max()\n",
    "test_months = 2\n",
    "val_months = 2\n",
    "\n",
    "test_start = max_date - pd.Timedelta(days=30*test_months)\n",
    "val_start = test_start - pd.Timedelta(days=30*val_months)\n",
    "\n",
    "train_df = df_clean[df_clean['cdr_date'] < val_start].copy()\n",
    "val_df = df_clean[(df_clean['cdr_date'] >= val_start) & (df_clean['cdr_date'] < test_start)].copy()\n",
    "test_df = df_clean[df_clean['cdr_date'] >= test_start].copy()\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['target_next_day']\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['target_next_day']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['target_next_day']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"データ分割\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Train: {len(X_train)}件 ({train_df['cdr_date'].min()} ~ {train_df['cdr_date'].max()})\")\n",
    "print(f\"Val  : {len(X_val)}件 ({val_df['cdr_date'].min()} ~ {val_df['cdr_date'].max()})\")\n",
    "print(f\"Test : {len(X_test)}件 ({test_df['cdr_date'].min()} ~ {test_df['cdr_date'].max()})\")\n",
    "print(f\"\\n特徴量数: {len(feature_cols)}個\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Optuna最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 評価関数\n",
    "# ==================================================================================\n",
    "\n",
    "def calculate_wape(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'WAPE': calculate_wape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "print('評価関数を定義しました')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 1. Ridge最適化\n",
    "# ==================================================================================\n",
    "\n",
    "def optimize_ridge(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 200.0, log=True)\n",
    "    \n",
    "    model = Ridge(alpha=alpha, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    return mae\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Ridge最適化開始（100 trials）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "study_ridge = optuna.create_study(direction='minimize', study_name='ridge')\n",
    "study_ridge.optimize(optimize_ridge, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "best_ridge_params = study_ridge.best_params\n",
    "print(f\"\\nBest Ridge params: {best_ridge_params}\")\n",
    "print(f\"Best validation MAE: {study_ridge.best_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 2. ExtraTrees最適化\n",
    "# ==================================================================================\n",
    "\n",
    "def optimize_extratrees(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    }\n",
    "    \n",
    "    model = ExtraTreesRegressor(**params, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    return mae\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ExtraTrees最適化開始（100 trials）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "study_extra = optuna.create_study(direction='minimize', study_name='extratrees')\n",
    "study_extra.optimize(optimize_extratrees, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "best_extra_params = study_extra.best_params\n",
    "print(f\"\\nBest ExtraTrees params: {best_extra_params}\")\n",
    "print(f\"Best validation MAE: {study_extra.best_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 3. HistGradientBoosting最適化\n",
    "# ==================================================================================\n",
    "\n",
    "def optimize_histgb(trial):\n",
    "    params = {\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 50),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.0, 20.0)\n",
    "    }\n",
    "    \n",
    "    model = HistGradientBoostingRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    return mae\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HistGradientBoosting最適化開始（100 trials）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "study_hist = optuna.create_study(direction='minimize', study_name='histgb')\n",
    "study_hist.optimize(optimize_histgb, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "best_hist_params = study_hist.best_params\n",
    "print(f\"\\nBest HistGradientBoosting params: {best_hist_params}\")\n",
    "print(f\"Best validation MAE: {study_hist.best_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 4. CatBoost最適化\n",
    "# ==================================================================================\n",
    "\n",
    "def optimize_catboost(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 20.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    }\n",
    "    \n",
    "    model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    return mae\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CatBoost最適化開始（100 trials）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "study_catboost = optuna.create_study(direction='minimize', study_name='catboost')\n",
    "study_catboost.optimize(optimize_catboost, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "best_catboost_params = study_catboost.best_params\n",
    "print(f\"\\nBest CatBoost params: {best_catboost_params}\")\n",
    "print(f\"Best validation MAE: {study_catboost.best_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 最適化されたモデルで評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 最適パラメータで各モデルを訓練\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"最適パラメータでモデル訓練\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Ridge\n",
    "print(\"\\n[1/4] Ridge...\")\n",
    "ridge_final = Ridge(**best_ridge_params, random_state=42)\n",
    "ridge_final.fit(X_train, y_train)\n",
    "ridge_val_pred = ridge_final.predict(X_val)\n",
    "ridge_test_pred = ridge_final.predict(X_test)\n",
    "ridge_val_metrics = evaluate_model(y_val, ridge_val_pred)\n",
    "ridge_test_metrics = evaluate_model(y_test, ridge_test_pred)\n",
    "print(f\"  Val MAE: {ridge_val_metrics['MAE']:.2f}\")\n",
    "print(f\"  Test MAE: {ridge_test_metrics['MAE']:.2f}\")\n",
    "\n",
    "# 2. ExtraTrees\n",
    "print(\"\\n[2/4] ExtraTrees...\")\n",
    "extra_final = ExtraTreesRegressor(**best_extra_params, random_state=42, n_jobs=-1)\n",
    "extra_final.fit(X_train, y_train)\n",
    "extra_val_pred = extra_final.predict(X_val)\n",
    "extra_test_pred = extra_final.predict(X_test)\n",
    "extra_val_metrics = evaluate_model(y_val, extra_val_pred)\n",
    "extra_test_metrics = evaluate_model(y_test, extra_test_pred)\n",
    "print(f\"  Val MAE: {extra_val_metrics['MAE']:.2f}\")\n",
    "print(f\"  Test MAE: {extra_test_metrics['MAE']:.2f}\")\n",
    "\n",
    "# 3. HistGradientBoosting\n",
    "print(\"\\n[3/4] HistGradientBoosting...\")\n",
    "hist_final = HistGradientBoostingRegressor(**best_hist_params, random_state=42)\n",
    "hist_final.fit(X_train, y_train)\n",
    "hist_val_pred = hist_final.predict(X_val)\n",
    "hist_test_pred = hist_final.predict(X_test)\n",
    "hist_val_metrics = evaluate_model(y_val, hist_val_pred)\n",
    "hist_test_metrics = evaluate_model(y_test, hist_test_pred)\n",
    "print(f\"  Val MAE: {hist_val_metrics['MAE']:.2f}\")\n",
    "print(f\"  Test MAE: {hist_test_metrics['MAE']:.2f}\")\n",
    "\n",
    "# 4. CatBoost\n",
    "print(\"\\n[4/4] CatBoost...\")\n",
    "catboost_final = CatBoostRegressor(**best_catboost_params, random_state=42, verbose=0)\n",
    "catboost_final.fit(X_train, y_train)\n",
    "catboost_val_pred = catboost_final.predict(X_val)\n",
    "catboost_test_pred = catboost_final.predict(X_test)\n",
    "catboost_val_metrics = evaluate_model(y_val, catboost_val_pred)\n",
    "catboost_test_metrics = evaluate_model(y_test, catboost_test_pred)\n",
    "print(f\"  Val MAE: {catboost_val_metrics['MAE']:.2f}\")\n",
    "print(f\"  Test MAE: {catboost_test_metrics['MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# WeightedEnsemble_Aの最適化（Validationセットで重みを最適化）\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WeightedEnsemble_A: 重み最適化\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validationセットで重みを最適化\n",
    "val_predictions = {\n",
    "    'Ridge': ridge_val_pred,\n",
    "    'CatBoost': catboost_val_pred,\n",
    "    'ExtraTrees': extra_val_pred,\n",
    "    'HistGradientBoosting': hist_val_pred\n",
    "}\n",
    "\n",
    "def optimize_ensemble_weights(predictions_dict, y_true, model_names):\n",
    "    preds_matrix = np.column_stack([predictions_dict[name] for name in model_names])\n",
    "    \n",
    "    def objective(weights):\n",
    "        ensemble_pred = preds_matrix @ weights\n",
    "        return mean_absolute_error(y_true, ensemble_pred)\n",
    "    \n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
    "    bounds = [(0, 1) for _ in range(len(model_names))]\n",
    "    initial_weights = np.ones(len(model_names)) / len(model_names)\n",
    "    \n",
    "    result = minimize(objective, initial_weights, method='SLSQP',\n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "model_names = ['Ridge', 'CatBoost', 'ExtraTrees', 'HistGradientBoosting']\n",
    "optimal_weights = optimize_ensemble_weights(val_predictions, y_val, model_names)\n",
    "\n",
    "print(\"\\n最適な重み:\")\n",
    "for name, weight in zip(model_names, optimal_weights):\n",
    "    print(f\"  {name}: {weight:.4f}\")\n",
    "\n",
    "# Validationセットで評価\n",
    "ensemble_val_pred = np.column_stack([val_predictions[name] for name in model_names]) @ optimal_weights\n",
    "ensemble_val_metrics = evaluate_model(y_val, ensemble_val_pred)\n",
    "print(f\"\\nVal MAE: {ensemble_val_metrics['MAE']:.2f}\")\n",
    "\n",
    "# Testセットで評価\n",
    "test_predictions = {\n",
    "    'Ridge': ridge_test_pred,\n",
    "    'CatBoost': catboost_test_pred,\n",
    "    'ExtraTrees': extra_test_pred,\n",
    "    'HistGradientBoosting': hist_test_pred\n",
    "}\n",
    "ensemble_test_pred = np.column_stack([test_predictions[name] for name in model_names]) @ optimal_weights\n",
    "ensemble_test_metrics = evaluate_model(y_test, ensemble_test_pred)\n",
    "print(f\"Test MAE: {ensemble_test_metrics['MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 結果の集計と保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 結果の集計\n",
    "# ==================================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "results_data = [\n",
    "    {'model': 'Ridge', 'set': 'Validation', **ridge_val_metrics},\n",
    "    {'model': 'Ridge', 'set': 'Test', **ridge_test_metrics},\n",
    "    {'model': 'ExtraTrees', 'set': 'Validation', **extra_val_metrics},\n",
    "    {'model': 'ExtraTrees', 'set': 'Test', **extra_test_metrics},\n",
    "    {'model': 'HistGradientBoosting', 'set': 'Validation', **hist_val_metrics},\n",
    "    {'model': 'HistGradientBoosting', 'set': 'Test', **hist_test_metrics},\n",
    "    {'model': 'CatBoost', 'set': 'Validation', **catboost_val_metrics},\n",
    "    {'model': 'CatBoost', 'set': 'Test', **catboost_test_metrics},\n",
    "    {'model': 'WeightedEnsemble_A', 'set': 'Validation', **ensemble_val_metrics},\n",
    "    {'model': 'WeightedEnsemble_A', 'set': 'Test', **ensemble_test_metrics}\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp19 最終結果\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# 最適パラメータの保存\n",
    "optimized_params = {\n",
    "    'Ridge': best_ridge_params,\n",
    "    'ExtraTrees': best_extra_params,\n",
    "    'HistGradientBoosting': best_hist_params,\n",
    "    'CatBoost': best_catboost_params,\n",
    "    'WeightedEnsemble_A_weights': {name: float(weight) for name, weight in zip(model_names, optimal_weights)}\n",
    "}\n",
    "\n",
    "# CSV保存\n",
    "output_dir = '../output/exp19'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "results_df.to_csv(f'{output_dir}/optuna_results.csv', index=False)\n",
    "\n",
    "import json\n",
    "with open(f'{output_dir}/optimized_params.json', 'w') as f:\n",
    "    json.dump(optimized_params, f, indent=2)\n",
    "\n",
    "print(f\"\\n結果を保存しました: {output_dir}/\")\n",
    "print(f\"  - optuna_results.csv\")\n",
    "print(f\"  - optimized_params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# exp16との比較\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp16 vs exp19 比較\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# exp16のベストスコア（WeightedEnsemble_A）\n",
    "exp16_best_mae = 48.98  # exp16の結果から\n",
    "exp19_test_mae = ensemble_test_metrics['MAE']\n",
    "\n",
    "improvement = exp16_best_mae - exp19_test_mae\n",
    "improvement_pct = (improvement / exp16_best_mae) * 100\n",
    "\n",
    "print(f\"\\nWeightedEnsemble_A:\")\n",
    "print(f\"  exp16 (旧パラメータ): {exp16_best_mae:.2f}\")\n",
    "print(f\"  exp19 (Optuna最適化): {exp19_test_mae:.2f}\")\n",
    "print(f\"  差分: {improvement:+.2f} ({improvement_pct:+.1f}%)\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"\\n✅ 改善成功！ {improvement:.2f}MAEの向上\")\n",
    "else:\n",
    "    print(f\"\\n❌ 悪化 {abs(improvement):.2f}MAEの低下\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"最適化完了\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**exp19: Optunaによるハイパーパラメータ再最適化**\n",
    "\n",
    "### ベースライン:\n",
    "- exp16 (WeightedEnsemble_A MAE=48.98)\n",
    "\n",
    "### 最適化内容:\n",
    "1. **Ridge** - alphaを再最適化\n",
    "2. **ExtraTrees** - n_estimators, max_depth等を最適化\n",
    "3. **HistGradientBoosting** - learning_rate, max_depth等を最適化\n",
    "4. **CatBoost** - iterations, learning_rate, depth等を最適化\n",
    "5. **WeightedEnsemble_A** - Validationセットで重みを最適化\n",
    "\n",
    "### 最適化戦略:\n",
    "- Train/Validation/Testの3分割\n",
    "- Validationセットでハイパーパラメータを最適化\n",
    "- 各モデル100 trials実行\n",
    "- Testセットで最終評価\n",
    "\n",
    "### 結果:\n",
    "- exp16と比較してMAEの改善/悪化を確認\n",
    "- 最適化されたパラメータをJSON形式で保存\n",
    "\n",
    "### 出力ファイル:\n",
    "1. `optuna_results.csv` - 各モデルのVal/Test結果\n",
    "2. `optimized_params.json` - 最適化されたパラメータ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
