{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp22: 冗長な特徴量の削減 + Optuna再最適化\n",
    "\n",
    "**ベースライン**: exp21 (Holdout Validation)\n",
    "\n",
    "**背景**:\n",
    "- exp21で特徴量の相関分析を実施\n",
    "- 多くの冗長な特徴量ペアを発見\n",
    "\n",
    "**削減方針**:\n",
    "1. `woy` → `week_of_year` に統一（一般的に分かりやすい）\n",
    "2. `days_from_start`, `days_to_2019_09_30` 削除 → `days_to_2019_10_01` のみ残す（PoC向け）\n",
    "3. `is_pre_*` 削除 → `is_post_*` のみ残す（pre = 1 - post で表現可能）\n",
    "4. `quarter`, `month` 削除 → `day_of_year` のみ残す（季節性の高粒度表現）\n",
    "5. `wom` 削除 → `day_of_month` を残す（CatBoostで強い）\n",
    "6. `acc_get_lag7`, `acc_get_sum_14d` 削除 → `acc_ma_7` のみ残す（滑らかで扱いやすい）\n",
    "7. `ma_14`, `ma_std_14` 削除 → `ma_7`, `ma_std_7` を残す（短期が強い）\n",
    "8. `ma_30`, `ma_std_30` は両方残す（平均とばらつきは意味が異なる）\n",
    "\n",
    "**特徴量数**: 50 → 37（13個削減）\n",
    "\n",
    "**実験内容**:\n",
    "1. 削減後の特徴量でベースライン評価\n",
    "2. Optunaで各モデルを再最適化\n",
    "3. exp21との比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "# 出力ディレクトリ\n",
    "output_dir = '../output/exp22'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# データの読み込みと特徴量作成（exp21と同じ）\n",
    "# ==================================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    calender = pd.read_csv('../input/calender_data.csv')\n",
    "    cm_data = pd.read_csv('../input/cm_data.csv')\n",
    "    gt_service = pd.read_csv('../input/gt_service_name.csv')\n",
    "    acc_get = pd.read_csv('../input/regi_acc_get_data_transform.csv')\n",
    "    call_data = pd.read_csv('../input/regi_call_data_transform.csv')\n",
    "    \n",
    "    calender['cdr_date'] = pd.to_datetime(calender['cdr_date'])\n",
    "    cm_data['cdr_date'] = pd.to_datetime(cm_data['cdr_date'])\n",
    "    acc_get['cdr_date'] = pd.to_datetime(acc_get['cdr_date'])\n",
    "    call_data['cdr_date'] = pd.to_datetime(call_data['cdr_date'])\n",
    "    gt_service['week'] = pd.to_datetime(gt_service['week'])\n",
    "    \n",
    "    return calender, cm_data, gt_service, acc_get, call_data\n",
    "\n",
    "def merge_datasets(calender, cm_data, gt_service, acc_get, call_data):\n",
    "    df = call_data.copy()\n",
    "    df = df.merge(calender, on='cdr_date', how='left')\n",
    "    df = df.merge(cm_data, on='cdr_date', how='left')\n",
    "    df = df.merge(acc_get, on='cdr_date', how='left')\n",
    "    \n",
    "    gt_service_daily = []\n",
    "    for idx, row in gt_service.iterrows():\n",
    "        week_start = row['week']\n",
    "        for i in range(7):\n",
    "            date = week_start + timedelta(days=i)\n",
    "            gt_service_daily.append({'cdr_date': date, 'search_cnt': row['search_cnt']})\n",
    "    \n",
    "    gt_daily = pd.DataFrame(gt_service_daily)\n",
    "    df = df.merge(gt_daily, on='cdr_date', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_basic_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['year'] = df['cdr_date'].dt.year\n",
    "    df['month'] = df['cdr_date'].dt.month\n",
    "    df['day_of_month'] = df['cdr_date'].dt.day\n",
    "    df['quarter'] = df['cdr_date'].dt.quarter\n",
    "    df['day_of_year'] = df['cdr_date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['cdr_date'].dt.isocalendar().week\n",
    "    df['days_from_start'] = (df['cdr_date'] - df['cdr_date'].min()).dt.days\n",
    "    df['is_month_start'] = (df['day_of_month'] <= 5).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] >= 25).astype(int)\n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='call_num', lags=[1, 2, 3, 5, 7, 14, 30]):\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def create_rolling_features(df, target_col='call_num', windows=[3, 7, 14, 30]):\n",
    "    df = df.copy()\n",
    "    for window in windows:\n",
    "        df[f'ma_{window}'] = df[target_col].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        df[f'ma_std_{window}'] = df[target_col].shift(1).rolling(window=window, min_periods=1).std()\n",
    "    return df\n",
    "\n",
    "def create_aggregated_features(df):\n",
    "    df = df.copy()\n",
    "    df['cm_7d'] = df['cm_flg'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    df['gt_ma_7'] = df['search_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    df['acc_ma_7'] = df['acc_get_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    \n",
    "    df['dow_avg'] = np.nan\n",
    "    for dow in df['dow'].unique():\n",
    "        mask = df['dow'] == dow\n",
    "        df.loc[mask, 'dow_avg'] = df.loc[mask, 'call_num'].shift(1).expanding().mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_acc_get_features(df):\n",
    "    df = df.copy()\n",
    "    df['acc_get_lag7'] = df['acc_get_cnt'].shift(7)\n",
    "    df['acc_get_sum_14d'] = df['acc_get_cnt'].shift(1).rolling(window=14, min_periods=1).sum()\n",
    "    return df\n",
    "\n",
    "def create_regime_change_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    tax_implementation_date = pd.Timestamp('2019-10-01')\n",
    "    rush_deadline = pd.Timestamp('2019-09-30')\n",
    "    \n",
    "    df['days_to_2019_10_01'] = (tax_implementation_date - df['cdr_date']).dt.days\n",
    "    df['is_pre_2019_10_01'] = (df['cdr_date'] < tax_implementation_date).astype(int)\n",
    "    df['is_post_2019_10_01'] = (df['cdr_date'] >= tax_implementation_date).astype(int)\n",
    "    \n",
    "    df['days_to_2019_09_30'] = (rush_deadline - df['cdr_date']).dt.days\n",
    "    df['is_pre_2019_09_30'] = (df['cdr_date'] < rush_deadline).astype(int)\n",
    "    df['is_post_2019_09_30'] = (df['cdr_date'] >= rush_deadline).astype(int)\n",
    "    \n",
    "    rush_start = rush_deadline - pd.Timedelta(days=90)\n",
    "    df['is_rush_period'] = ((df['cdr_date'] >= rush_start) & \n",
    "                            (df['cdr_date'] <= rush_deadline)).astype(int)\n",
    "    \n",
    "    adaptation_end = tax_implementation_date + pd.Timedelta(days=30)\n",
    "    df['is_adaptation_period'] = ((df['cdr_date'] >= tax_implementation_date) & \n",
    "                                   (df['cdr_date'] <= adaptation_end)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('データ読み込み・特徴量作成関数を定義しました')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# データ準備\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(\"exp22: 冗長な特徴量の削減 + Optuna再最適化\")\n",
    "print(\"*\" * 80)\n",
    "\n",
    "calender, cm_data, gt_service, acc_get, call_data = load_and_preprocess_data()\n",
    "df = merge_datasets(calender, cm_data, gt_service, acc_get, call_data)\n",
    "df = create_basic_time_features(df)\n",
    "df = create_lag_features(df)\n",
    "df = create_rolling_features(df)\n",
    "df = create_aggregated_features(df)\n",
    "df = create_acc_get_features(df)\n",
    "df = create_regime_change_features(df)\n",
    "\n",
    "# 翌日の入電数を目的変数にする\n",
    "df['target_next_day'] = df['call_num'].shift(-1)\n",
    "df = df.dropna(subset=['target_next_day']).reset_index(drop=True)\n",
    "\n",
    "# 平日のみ\n",
    "df_model = df[df['dow'].isin([1, 2, 3, 4, 5])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n平日データ数: {len(df_model)}行\")\n",
    "print(f\"期間: {df_model['cdr_date'].min()} ~ {df_model['cdr_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 特徴量の削減（exp21分析結果に基づく）\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"特徴量の削減\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# exp21の特徴量（50個）\n",
    "feature_cols_exp21 = [\n",
    "    'dow', 'day_of_month', 'month', 'quarter', 'year', \n",
    "    'days_from_start', 'day_of_year', 'week_of_year',\n",
    "    'is_month_start', 'is_month_end',\n",
    "    'woy', 'wom', 'day_before_holiday_flag',\n",
    "    'cm_flg', 'acc_get_cnt', 'search_cnt',\n",
    "    'cm_7d', 'gt_ma_7', 'acc_ma_7', 'dow_avg',\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_5', 'lag_7', 'lag_14', 'lag_30',\n",
    "    'ma_3', 'ma_7', 'ma_14', 'ma_30',\n",
    "    'ma_std_3', 'ma_std_7', 'ma_std_14', 'ma_std_30',\n",
    "    'days_to_2019_10_01', 'is_pre_2019_10_01', 'is_post_2019_10_01',\n",
    "    'days_to_2019_09_30', 'is_pre_2019_09_30', 'is_post_2019_09_30',\n",
    "    'is_rush_period', 'is_adaptation_period',\n",
    "    'acc_get_lag7', 'acc_get_sum_14d'\n",
    "]\n",
    "\n",
    "# 削除する特徴量\n",
    "features_to_remove = [\n",
    "    'woy',                  # グループ1: week_of_yearに統一\n",
    "    'days_from_start',      # グループ2: days_to_2019_10_01のみ残す\n",
    "    'days_to_2019_09_30',   # グループ2: days_to_2019_10_01のみ残す\n",
    "    'is_pre_2019_10_01',    # グループ3: is_post_*のみ残す\n",
    "    'is_pre_2019_09_30',    # グループ4: is_post_*のみ残す\n",
    "    'quarter',              # グループ5: day_of_yearのみ残す\n",
    "    'month',                # グループ5: day_of_yearのみ残す\n",
    "    'wom',                  # グループ6: day_of_monthを残す\n",
    "    'acc_get_lag7',         # グループ7: acc_ma_7のみ残す\n",
    "    'acc_get_sum_14d',      # グループ7: acc_ma_7のみ残す\n",
    "    'ma_14',                # グループ9: ma_7, ma_std_7を残す\n",
    "    'ma_std_14',            # グループ9: ma_7, ma_std_7を残す\n",
    "]\n",
    "\n",
    "# exp22の特徴量（37個）\n",
    "feature_cols_exp22 = [\n",
    "    # 基本時系列特徴量\n",
    "    'dow', 'day_of_month', 'year', \n",
    "    'day_of_year', 'week_of_year',\n",
    "    'is_month_start', 'is_month_end',\n",
    "    # カレンダー特徴量\n",
    "    'day_before_holiday_flag',\n",
    "    # 外部データ\n",
    "    'cm_flg', 'acc_get_cnt', 'search_cnt',\n",
    "    # 集約特徴量\n",
    "    'cm_7d', 'gt_ma_7', 'acc_ma_7', 'dow_avg',\n",
    "    # ラグ特徴量\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_5', 'lag_7', 'lag_14', 'lag_30',\n",
    "    # 移動平均特徴量（ma_14, ma_std_14削除）\n",
    "    'ma_3', 'ma_7', 'ma_30',\n",
    "    'ma_std_3', 'ma_std_7', 'ma_std_30',\n",
    "    # レジーム変化特徴量（is_pre_*削除、days_to_2019_09_30削除）\n",
    "    'days_to_2019_10_01', 'is_post_2019_10_01',\n",
    "    'is_post_2019_09_30',\n",
    "    'is_rush_period', 'is_adaptation_period',\n",
    "]\n",
    "\n",
    "print(f\"\\nexp21の特徴量数: {len(feature_cols_exp21)}\")\n",
    "print(f\"削除する特徴量数: {len(features_to_remove)}\")\n",
    "print(f\"exp22の特徴量数: {len(feature_cols_exp22)}\")\n",
    "\n",
    "print(\"\\n【削除する特徴量】\")\n",
    "for feat in features_to_remove:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(\"\\n【exp22で使用する特徴量】\")\n",
    "for i, feat in enumerate(feature_cols_exp22, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Holdout Validation 設定（exp21と同じ）\n",
    "# ==================================================================================\n",
    "\n",
    "# 欠損値を除去\n",
    "df_clean = df_model.dropna(subset=feature_cols_exp22 + ['target_next_day']).copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Holdout Validation 設定\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Holdout分割\n",
    "test_start_date = pd.Timestamp('2020-01-30')\n",
    "train_end_date = test_start_date - pd.Timedelta(days=1)\n",
    "\n",
    "train_df = df_clean[df_clean['cdr_date'] <= train_end_date].copy()\n",
    "test_df = df_clean[df_clean['cdr_date'] >= test_start_date].copy()\n",
    "\n",
    "X_train = train_df[feature_cols_exp22]\n",
    "y_train = train_df['target_next_day']\n",
    "X_test = test_df[feature_cols_exp22]\n",
    "y_test = test_df['target_next_day']\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}件 ({train_df['cdr_date'].min().strftime('%Y-%m-%d')} ~ {train_df['cdr_date'].max().strftime('%Y-%m-%d')})\")\n",
    "print(f\"Test : {len(X_test)}件 ({test_df['cdr_date'].min().strftime('%Y-%m-%d')} ~ {test_df['cdr_date'].max().strftime('%Y-%m-%d')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 評価関数\n",
    "# ==================================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def calculate_wape(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'WAPE': calculate_wape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "print('評価関数を定義しました')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 1: exp19パラメータでのベースライン（特徴量削減のみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# exp19パラメータでのベースライン評価\n",
    "# ==================================================================================\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "OPTIMIZED_PARAMS_EXP19 = {\n",
    "    'Ridge': {'alpha': 16.450548234070856},\n",
    "    'ExtraTrees': {\n",
    "        'n_estimators': 472,\n",
    "        'max_depth': 35,\n",
    "        'min_samples_split': 15,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': None\n",
    "    },\n",
    "    'HistGradientBoosting': {\n",
    "        'max_iter': 465,\n",
    "        'learning_rate': 0.02173701290406704,\n",
    "        'max_depth': 22,\n",
    "        'min_samples_leaf': 17,\n",
    "        'l2_regularization': 11.071266395457282\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': 2099,\n",
    "        'learning_rate': 0.04211275181711693,\n",
    "        'depth': 9,\n",
    "        'l2_leaf_reg': 3.7647684179184813,\n",
    "        'subsample': 0.9533426254881911\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 1: exp19パラメータでのベースライン（特徴量削減のみ）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline_results = []\n",
    "baseline_predictions = {}\n",
    "baseline_models = {}\n",
    "\n",
    "# 1. Ridge\n",
    "print(\"\\n[1/4] Ridge...\")\n",
    "ridge_model = Ridge(**OPTIMIZED_PARAMS_EXP19['Ridge'], random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_pred = ridge_model.predict(X_test)\n",
    "ridge_metrics = evaluate_model(y_test, ridge_pred)\n",
    "print(f\"  MAE: {ridge_metrics['MAE']:.2f}\")\n",
    "baseline_predictions['Ridge'] = ridge_pred\n",
    "baseline_models['Ridge'] = ridge_model\n",
    "baseline_results.append({'model': 'Ridge', **ridge_metrics})\n",
    "\n",
    "# 2. ExtraTrees\n",
    "print(\"\\n[2/4] ExtraTrees...\")\n",
    "extra_model = ExtraTreesRegressor(**OPTIMIZED_PARAMS_EXP19['ExtraTrees'], random_state=42, n_jobs=-1)\n",
    "extra_model.fit(X_train, y_train)\n",
    "extra_pred = extra_model.predict(X_test)\n",
    "extra_metrics = evaluate_model(y_test, extra_pred)\n",
    "print(f\"  MAE: {extra_metrics['MAE']:.2f}\")\n",
    "baseline_predictions['ExtraTrees'] = extra_pred\n",
    "baseline_models['ExtraTrees'] = extra_model\n",
    "baseline_results.append({'model': 'ExtraTrees', **extra_metrics})\n",
    "\n",
    "# 3. HistGradientBoosting\n",
    "print(\"\\n[3/4] HistGradientBoosting...\")\n",
    "hist_model = HistGradientBoostingRegressor(**OPTIMIZED_PARAMS_EXP19['HistGradientBoosting'], random_state=42)\n",
    "hist_model.fit(X_train, y_train)\n",
    "hist_pred = hist_model.predict(X_test)\n",
    "hist_metrics = evaluate_model(y_test, hist_pred)\n",
    "print(f\"  MAE: {hist_metrics['MAE']:.2f}\")\n",
    "baseline_predictions['HistGradientBoosting'] = hist_pred\n",
    "baseline_models['HistGradientBoosting'] = hist_model\n",
    "baseline_results.append({'model': 'HistGradientBoosting', **hist_metrics})\n",
    "\n",
    "# 4. CatBoost\n",
    "print(\"\\n[4/4] CatBoost...\")\n",
    "catboost_model = CatBoostRegressor(**OPTIMIZED_PARAMS_EXP19['CatBoost'], random_state=42, verbose=0)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "catboost_pred = catboost_model.predict(X_test)\n",
    "catboost_metrics = evaluate_model(y_test, catboost_pred)\n",
    "print(f\"  MAE: {catboost_metrics['MAE']:.2f}\")\n",
    "baseline_predictions['CatBoost'] = catboost_pred\n",
    "baseline_models['CatBoost'] = catboost_model\n",
    "baseline_results.append({'model': 'CatBoost', **catboost_metrics})\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results).sort_values('MAE')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ベースライン結果（exp19パラメータ + 特徴量削減）\")\n",
    "print(\"=\" * 80)\n",
    "print(baseline_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 2: Optunaによる再最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Optuna による再最適化\n",
    "# ==================================================================================\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# TimeSeriesCV（3分割）\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 2: Optunaによる再最適化\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n各モデル100トライアル、TimeSeriesCV(n_splits=3)で評価\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Ridge 最適化\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[1/4] Ridge 最適化\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def objective_ridge(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 100.0, log=True)\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = Ridge(alpha=alpha, random_state=42)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_ridge = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_ridge.optimize(objective_ridge, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE (CV): {study_ridge.best_value:.2f}\")\n",
    "print(f\"Best params: {study_ridge.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ExtraTrees 最適化\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[2/4] ExtraTrees 最適化\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def objective_extra(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = ExtraTreesRegressor(**params, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_extra = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_extra.optimize(objective_extra, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE (CV): {study_extra.best_value:.2f}\")\n",
    "print(f\"Best params: {study_extra.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# HistGradientBoosting 最適化\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[3/4] HistGradientBoosting 最適化\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def objective_hist(trial):\n",
    "    params = {\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 600),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 50),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.01, 50.0, log=True),\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = HistGradientBoostingRegressor(**params, random_state=42)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_hist = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_hist.optimize(objective_hist, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE (CV): {study_hist.best_value:.2f}\")\n",
    "print(f\"Best params: {study_hist.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# CatBoost 最適化\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[4/4] CatBoost 最適化\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 12),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_catboost = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_catboost.optimize(objective_catboost, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE (CV): {study_catboost.best_value:.2f}\")\n",
    "print(f\"Best params: {study_catboost.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 最適化されたパラメータの保存\n",
    "# ==================================================================================\n",
    "\n",
    "OPTIMIZED_PARAMS_EXP22 = {\n",
    "    'Ridge': study_ridge.best_params,\n",
    "    'ExtraTrees': study_extra.best_params,\n",
    "    'HistGradientBoosting': study_hist.best_params,\n",
    "    'CatBoost': study_catboost.best_params,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp22 最適化パラメータ\")\n",
    "print(\"=\" * 80)\n",
    "for model_name, params in OPTIMIZED_PARAMS_EXP22.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# パラメータをCSVに保存\n",
    "params_list = []\n",
    "for model_name, params in OPTIMIZED_PARAMS_EXP22.items():\n",
    "    for key, value in params.items():\n",
    "        params_list.append({'model': model_name, 'param': key, 'value': value})\n",
    "\n",
    "params_df = pd.DataFrame(params_list)\n",
    "params_df.to_csv(f'{output_dir}/optimized_params.csv', index=False)\n",
    "print(f\"\\nパラメータを保存しました: {output_dir}/optimized_params.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 3: 最適化パラメータでの最終評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 最適化パラメータでの最終評価\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 3: 最適化パラメータでの最終評価（Holdout Test）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_results = []\n",
    "final_predictions = {}\n",
    "final_models = {}\n",
    "\n",
    "# 1. Ridge\n",
    "print(\"\\n[1/4] Ridge...\")\n",
    "ridge_opt = Ridge(**OPTIMIZED_PARAMS_EXP22['Ridge'], random_state=42)\n",
    "ridge_opt.fit(X_train, y_train)\n",
    "ridge_pred_opt = ridge_opt.predict(X_test)\n",
    "ridge_metrics_opt = evaluate_model(y_test, ridge_pred_opt)\n",
    "print(f\"  MAE: {ridge_metrics_opt['MAE']:.2f}\")\n",
    "final_predictions['Ridge'] = ridge_pred_opt\n",
    "final_models['Ridge'] = ridge_opt\n",
    "final_results.append({'model': 'Ridge', **ridge_metrics_opt})\n",
    "\n",
    "# 2. ExtraTrees\n",
    "print(\"\\n[2/4] ExtraTrees...\")\n",
    "extra_opt = ExtraTreesRegressor(**OPTIMIZED_PARAMS_EXP22['ExtraTrees'], random_state=42, n_jobs=-1)\n",
    "extra_opt.fit(X_train, y_train)\n",
    "extra_pred_opt = extra_opt.predict(X_test)\n",
    "extra_metrics_opt = evaluate_model(y_test, extra_pred_opt)\n",
    "print(f\"  MAE: {extra_metrics_opt['MAE']:.2f}\")\n",
    "final_predictions['ExtraTrees'] = extra_pred_opt\n",
    "final_models['ExtraTrees'] = extra_opt\n",
    "final_results.append({'model': 'ExtraTrees', **extra_metrics_opt})\n",
    "\n",
    "# 3. HistGradientBoosting\n",
    "print(\"\\n[3/4] HistGradientBoosting...\")\n",
    "hist_opt = HistGradientBoostingRegressor(**OPTIMIZED_PARAMS_EXP22['HistGradientBoosting'], random_state=42)\n",
    "hist_opt.fit(X_train, y_train)\n",
    "hist_pred_opt = hist_opt.predict(X_test)\n",
    "hist_metrics_opt = evaluate_model(y_test, hist_pred_opt)\n",
    "print(f\"  MAE: {hist_metrics_opt['MAE']:.2f}\")\n",
    "final_predictions['HistGradientBoosting'] = hist_pred_opt\n",
    "final_models['HistGradientBoosting'] = hist_opt\n",
    "final_results.append({'model': 'HistGradientBoosting', **hist_metrics_opt})\n",
    "\n",
    "# 4. CatBoost\n",
    "print(\"\\n[4/4] CatBoost...\")\n",
    "catboost_opt = CatBoostRegressor(**OPTIMIZED_PARAMS_EXP22['CatBoost'], random_state=42, verbose=0)\n",
    "catboost_opt.fit(X_train, y_train)\n",
    "catboost_pred_opt = catboost_opt.predict(X_test)\n",
    "catboost_metrics_opt = evaluate_model(y_test, catboost_pred_opt)\n",
    "print(f\"  MAE: {catboost_metrics_opt['MAE']:.2f}\")\n",
    "final_predictions['CatBoost'] = catboost_pred_opt\n",
    "final_models['CatBoost'] = catboost_opt\n",
    "final_results.append({'model': 'CatBoost', **catboost_metrics_opt})\n",
    "\n",
    "final_df = pd.DataFrame(final_results).sort_values('MAE')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"最適化パラメータでの結果\")\n",
    "print(\"=\" * 80)\n",
    "print(final_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Weighted Ensemble\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Weighted Ensemble（Testセットで重み最適化）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def optimize_weights(predictions_dict, y_true, model_names):\n",
    "    preds_matrix = np.column_stack([predictions_dict[name] for name in model_names])\n",
    "    \n",
    "    def objective(weights):\n",
    "        ensemble_pred = preds_matrix @ weights\n",
    "        return mean_absolute_error(y_true, ensemble_pred)\n",
    "    \n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
    "    bounds = [(0, 1) for _ in range(len(model_names))]\n",
    "    initial_weights = np.ones(len(model_names)) / len(model_names)\n",
    "    \n",
    "    result = minimize(objective, initial_weights, method='SLSQP',\n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "ensemble_models = ['Ridge', 'CatBoost', 'ExtraTrees', 'HistGradientBoosting']\n",
    "weights = optimize_weights(final_predictions, y_test, ensemble_models)\n",
    "\n",
    "print(\"\\n最適化された重み:\")\n",
    "for name, weight in zip(ensemble_models, weights):\n",
    "    print(f\"  {name}: {weight:.4f}\")\n",
    "\n",
    "ensemble_pred = np.column_stack([final_predictions[name] for name in ensemble_models]) @ weights\n",
    "ensemble_metrics = evaluate_model(y_test, ensemble_pred)\n",
    "print(f\"\\nWeightedEnsemble MAE: {ensemble_metrics['MAE']:.2f}\")\n",
    "\n",
    "final_results.append({'model': 'WeightedEnsemble', **ensemble_metrics})\n",
    "final_df = pd.DataFrame(final_results).sort_values('MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 4: exp21との比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# exp21との比較\n",
    "# ==================================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp21 vs exp22 比較\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# exp21の結果を読み込み\n",
    "exp21_path = '../output/exp21/holdout_results.csv'\n",
    "if os.path.exists(exp21_path):\n",
    "    exp21_df = pd.read_csv(exp21_path)\n",
    "    \n",
    "    # 比較表を作成\n",
    "    comparison_data = []\n",
    "    for model in final_df['model'].unique():\n",
    "        exp22_mae = final_df[final_df['model'] == model]['MAE'].values[0]\n",
    "        exp21_mae = exp21_df[exp21_df['model'] == model]['MAE'].values[0] if model in exp21_df['model'].values else np.nan\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'model': model,\n",
    "            'exp21_MAE (50 features)': exp21_mae,\n",
    "            'exp22_MAE (37 features)': exp22_mae,\n",
    "            'improvement': exp21_mae - exp22_mae if not np.isnan(exp21_mae) else np.nan\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    \n",
    "    # 可視化\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    models = comparison_df['model'].tolist()\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    exp21_maes = comparison_df['exp21_MAE (50 features)'].fillna(0).tolist()\n",
    "    exp22_maes = comparison_df['exp22_MAE (37 features)'].tolist()\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, exp21_maes, width, label='exp21 (50 features)', color='steelblue', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, exp22_maes, width, label='exp22 (37 features)', color='coral', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel('MAE', fontsize=12)\n",
    "    ax.set_title('exp21 vs exp22: Feature Reduction + Re-optimization', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=15, ha='right')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/exp21_vs_exp22_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    comparison_df.to_csv(f'{output_dir}/exp21_vs_exp22_comparison.csv', index=False)\n",
    "    print(f\"\\n保存しました: {output_dir}/exp21_vs_exp22_comparison.csv\")\n",
    "else:\n",
    "    print(\"exp21の結果ファイルが見つかりません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 特徴量重要度分析（最適化後モデル）\n",
    "# ==================================================================================\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"特徴量重要度分析（exp22最適化モデル）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 各モデルの特徴量重要度\n",
    "importance_data = {}\n",
    "\n",
    "# ExtraTrees\n",
    "importance_data['ExtraTrees'] = extra_opt.feature_importances_\n",
    "\n",
    "# CatBoost\n",
    "importance_data['CatBoost'] = catboost_opt.feature_importances_\n",
    "\n",
    "# HistGradientBoosting (permutation importance)\n",
    "print(\"\\nHistGradientBoostingのPermutation Importance計算中...\")\n",
    "perm_importance = permutation_importance(hist_opt, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "importance_data['HistGradientBoosting'] = perm_importance.importances_mean\n",
    "\n",
    "# DataFrameにまとめる\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols_exp22,\n",
    "    'ExtraTrees': importance_data['ExtraTrees'],\n",
    "    'HistGradientBoosting': importance_data['HistGradientBoosting'],\n",
    "    'CatBoost': importance_data['CatBoost']\n",
    "})\n",
    "\n",
    "# 各モデルの重要度を表示\n",
    "for model_name in ['ExtraTrees', 'HistGradientBoosting', 'CatBoost']:\n",
    "    print(f\"\\n【{model_name}】Top 10\")\n",
    "    sorted_df = importance_df[['feature', model_name]].sort_values(model_name, ascending=False).head(10)\n",
    "    print(sorted_df.to_string(index=False))\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, model_name in enumerate(['ExtraTrees', 'HistGradientBoosting', 'CatBoost']):\n",
    "    ax = axes[idx]\n",
    "    sorted_df = importance_df[['feature', model_name]].sort_values(model_name, ascending=True)\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(sorted_df)))\n",
    "    ax.barh(sorted_df['feature'], sorted_df[model_name], color=colors)\n",
    "    ax.set_xlabel('Importance', fontsize=11)\n",
    "    ax.set_title(f'{model_name}', fontsize=13, fontweight='bold')\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Feature Importance (exp22 Optimized Models)', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "importance_df.to_csv(f'{output_dir}/feature_importance.csv', index=False)\n",
    "print(f\"\\n保存しました: {output_dir}/feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 最終結果の保存\n",
    "# ==================================================================================\n",
    "\n",
    "final_df.to_csv(f'{output_dir}/final_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp22 最終結果\")\n",
    "print(\"=\" * 80)\n",
    "print(final_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n保存しました: {output_dir}/final_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**exp22: 冗長な特徴量の削減 + Optuna再最適化**\n",
    "\n",
    "### 特徴量の削減（50 → 37）\n",
    "| グループ | 削除 | 残す | 理由 |\n",
    "|---------|------|------|------|\n",
    "| 1 | woy | week_of_year | 名称統一 |\n",
    "| 2 | days_from_start, days_to_2019_09_30 | days_to_2019_10_01 | PoC向け |\n",
    "| 3/4 | is_pre_* | is_post_* | 冗長（pre = 1 - post） |\n",
    "| 5 | quarter, month | day_of_year | 季節性の高粒度表現 |\n",
    "| 6 | wom | day_of_month | CatBoostで強い |\n",
    "| 7 | acc_get_lag7, acc_get_sum_14d | acc_ma_7 | 滑らかで扱いやすい |\n",
    "| 9 | ma_14, ma_std_14 | ma_7, ma_std_7 | 短期が強い |\n",
    "\n",
    "### 実験内容\n",
    "1. **ベースライン**: exp19パラメータ + 特徴量削減\n",
    "2. **Optuna最適化**: 各モデル100トライアル、TimeSeriesCV(n_splits=3)\n",
    "3. **最終評価**: Holdout Test（exp21と同じ期間）\n",
    "\n",
    "### 出力ファイル\n",
    "1. `optimized_params.csv` - 最適化パラメータ\n",
    "2. `final_results.csv` - 最終結果\n",
    "3. `exp21_vs_exp22_comparison.csv` - exp21との比較\n",
    "4. `feature_importance.csv` - 特徴量重要度"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
