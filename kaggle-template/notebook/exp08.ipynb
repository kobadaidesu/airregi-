{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp08: AirREGI ç¿Œæ—¥å…¥é›»äºˆæ¸¬ï¼ˆHoldout Validationï¼‰\n",
    "\n",
    "**exp03ã¨ã®é•ã„**: æ¤œè¨¼æ‰‹æ³•ã‚’Holdoutæ³•ã«å¤‰æ›´\n",
    "\n",
    "- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: exp03\n",
    "- ç‰¹å¾´é‡: exp03ã¨åŒã˜ï¼ˆ35ç‰¹å¾´é‡ï¼‰\n",
    "- ãƒ¢ãƒ‡ãƒ«: exp03ã¨åŒã˜\n",
    "- **å¤‰æ›´ç‚¹**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œã®60æ—¥ã‚’Validationã‚»ãƒƒãƒˆã¨ã—ã¦å›ºå®šåˆ†å‰²ï¼ˆHoldoutæ³•ï¼‰\n",
    "\n",
    "## Holdoutæ³•ã¨ã¯\n",
    "- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ã€ŒTrainã€ã¨ã€ŒValidationã€ã«1å›ã ã‘åˆ†å‰²\n",
    "- Train: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«ä½¿ç”¨\n",
    "- Validation: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãƒ»æ—©æœŸåœæ­¢ã«ä½¿ç”¨\n",
    "- Test: æœ€çµ‚è©•ä¾¡ï¼ˆå¤‰æ›´ãªã—ï¼‰\n",
    "\n",
    "## æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®Holdout\n",
    "```\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€ Val â”€â”€â”¤â”€â”€ Test â”€â”€â”¤\n",
    "2018-06                              2019-11  2019-12   2020-01\n",
    "```\n",
    "\n",
    "## é‡è¦ãªåŸå‰‡ï¼ˆexp03ã‹ã‚‰ç¶™æ‰¿ï¼‰\n",
    "1. æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã‚ãªã„ï¼ˆãƒ©ã‚°ç‰¹å¾´é‡ã¯éå»ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰\n",
    "2. ç§»å‹•å¹³å‡ã‚‚éå»ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨\n",
    "3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯æ™‚ç³»åˆ—ã§åˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
    "# ==================================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€æ—¥ä»˜å‹ã«å¤‰æ›\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Step 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "    calender = pd.read_csv('../input/calender_data.csv')\n",
    "    cm_data = pd.read_csv('../input/cm_data.csv')\n",
    "    gt_service = pd.read_csv('../input/gt_service_name.csv')\n",
    "    acc_get = pd.read_csv('../input/regi_acc_get_data_transform.csv')\n",
    "    call_data = pd.read_csv('../input/regi_call_data_transform.csv')\n",
    "    \n",
    "    # æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’datetimeå‹ã«å¤‰æ›\n",
    "    calender['cdr_date'] = pd.to_datetime(calender['cdr_date'])\n",
    "    cm_data['cdr_date'] = pd.to_datetime(cm_data['cdr_date'])\n",
    "    acc_get['cdr_date'] = pd.to_datetime(acc_get['cdr_date'])\n",
    "    call_data['cdr_date'] = pd.to_datetime(call_data['cdr_date'])\n",
    "    gt_service['week'] = pd.to_datetime(gt_service['week'])\n",
    "    \n",
    "    print(f\"\\nã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿: {calender.shape}\")\n",
    "    print(f\"CMãƒ‡ãƒ¼ã‚¿: {cm_data.shape}\")\n",
    "    print(f\"Google Trendsãƒ‡ãƒ¼ã‚¿: {gt_service.shape}\")\n",
    "    print(f\"ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå–å¾—ãƒ‡ãƒ¼ã‚¿: {acc_get.shape}\")\n",
    "    print(f\"å…¥é›»ãƒ‡ãƒ¼ã‚¿ï¼ˆç›®çš„å¤‰æ•°ï¼‰: {call_data.shape}\")\n",
    "    \n",
    "    return calender, cm_data, gt_service, acc_get, call_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 2: ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ\n",
    "# ==================================================================================\n",
    "\n",
    "def merge_datasets(calender, cm_data, gt_service, acc_get, call_data):\n",
    "    \"\"\"\n",
    "    å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆ\n",
    "    Google Trendsã¯é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãªã®ã§æ—¥æ¬¡ã«å±•é–‹\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 2: ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¥é›»æ•°ï¼‰ã‚’åŸºæº–ã«ãƒãƒ¼ã‚¸\n",
    "    df = call_data.copy()\n",
    "    print(f\"\\nãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿: {df.shape}\")\n",
    "    \n",
    "    # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼æƒ…å ±ã‚’ãƒãƒ¼ã‚¸\n",
    "    df = df.merge(calender, on='cdr_date', how='left')\n",
    "    print(f\"ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼æƒ…å ±ãƒãƒ¼ã‚¸å¾Œ: {df.shape}\")\n",
    "    \n",
    "    # CMæƒ…å ±ã‚’ãƒãƒ¼ã‚¸\n",
    "    df = df.merge(cm_data, on='cdr_date', how='left')\n",
    "    print(f\"CMæƒ…å ±ãƒãƒ¼ã‚¸å¾Œ: {df.shape}\")\n",
    "    \n",
    "    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå–å¾—æ•°ã‚’ãƒãƒ¼ã‚¸\n",
    "    df = df.merge(acc_get, on='cdr_date', how='left')\n",
    "    print(f\"ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå–å¾—æ•°ãƒãƒ¼ã‚¸å¾Œ: {df.shape}\")\n",
    "    \n",
    "    # Google Trendsãƒ‡ãƒ¼ã‚¿ã¯é€±æ¬¡ãªã®ã§æ—¥æ¬¡ã«å±•é–‹\n",
    "    print(\"\\nGoogle Trendsãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡â†’æ—¥æ¬¡ã«å±•é–‹...\")\n",
    "    gt_service_daily = []\n",
    "    for idx, row in gt_service.iterrows():\n",
    "        week_start = row['week']\n",
    "        for i in range(7):\n",
    "            date = week_start + timedelta(days=i)\n",
    "            gt_service_daily.append({\n",
    "                'cdr_date': date, \n",
    "                'search_cnt': row['search_cnt']\n",
    "            })\n",
    "    \n",
    "    gt_daily = pd.DataFrame(gt_service_daily)\n",
    "    df = df.merge(gt_daily, on='cdr_date', how='left')\n",
    "    print(f\"Google Trendsãƒãƒ¼ã‚¸å¾Œ: {df.shape}\")\n",
    "    \n",
    "    # æ¬ æå€¤ã®ç¢ºèª\n",
    "    print(\"\\næ¬ æå€¤ã®æ•°:\")\n",
    "    print(df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 3: åŸºæœ¬çš„ãªæ™‚ç³»åˆ—ç‰¹å¾´é‡ã®ä½œæˆ\n",
    "# ==================================================================================\n",
    "\n",
    "def create_basic_time_features(df):\n",
    "    \"\"\"\n",
    "    æ—¥ä»˜ã‹ã‚‰æ´¾ç”Ÿã™ã‚‹åŸºæœ¬çš„ãªæ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’ä½œæˆ\n",
    "    ã“ã‚Œã‚‰ã¯æœªæ¥ã®æƒ…å ±ã‚’ä½¿ã‚ãªã„ã®ã§å®‰å…¨\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 3: åŸºæœ¬çš„ãªæ™‚ç³»åˆ—ç‰¹å¾´é‡ã®ä½œæˆ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # å¹´æœˆæ—¥ã®ç‰¹å¾´é‡\n",
    "    df['year'] = df['cdr_date'].dt.year\n",
    "    df['month'] = df['cdr_date'].dt.month\n",
    "    df['day_of_month'] = df['cdr_date'].dt.day\n",
    "    df['quarter'] = df['cdr_date'].dt.quarter\n",
    "    df['day_of_year'] = df['cdr_date'].dt.dayofyear\n",
    "    \n",
    "    # é€±ã®æƒ…å ±ï¼ˆæ—¢ã«woy, womãŒã‚ã‚‹ãŒå¿µã®ãŸã‚ï¼‰\n",
    "    df['week_of_year'] = df['cdr_date'].dt.isocalendar().week\n",
    "    \n",
    "    # çµŒéæ—¥æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹æ—¥ã‹ã‚‰ã®æ—¥æ•°ï¼‰\n",
    "    df['days_from_start'] = (df['cdr_date'] - df['cdr_date'].min()).dt.days\n",
    "    \n",
    "    # æ›œæ—¥æƒ…å ±ï¼ˆæ—¢ã«dowãŒã‚ã‚‹ãŒç¢ºèªï¼‰\n",
    "    # dow: 1=æœˆ, 2=ç«, ..., 7=æ—¥\n",
    "    \n",
    "    # æœˆåˆãƒ»æœˆæœ«ãƒ•ãƒ©ã‚°\n",
    "    df['is_month_start'] = (df['day_of_month'] <= 5).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] >= 25).astype(int)\n",
    "    \n",
    "    print(\"\\nä½œæˆã—ãŸåŸºæœ¬ç‰¹å¾´é‡:\")\n",
    "    time_features = ['year', 'month', 'day_of_month', 'quarter', 'day_of_year', \n",
    "                     'week_of_year', 'days_from_start', 'is_month_start', 'is_month_end']\n",
    "    print(time_features)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 4: ãƒ©ã‚°ç‰¹å¾´é‡ã®ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã«æ³¨æ„ï¼ï¼‰\n",
    "# ==================================================================================\n",
    "\n",
    "def create_lag_features(df, target_col='call_num', lags=[1, 2, 3, 5, 7, 14, 30]):\n",
    "    \"\"\"\n",
    "    ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆéå»ã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ä½œæˆ\n",
    "    \n",
    "    é‡è¦: \n",
    "    - shift()ã‚’ä½¿ã£ã¦ã€æœªæ¥ã®æƒ…å ±ãŒæ··å…¥ã—ãªã„ã‚ˆã†ã«ã™ã‚‹\n",
    "    - lag=1ã¯1æ—¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã€lag=7ã¯7æ—¥å‰ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    \n",
    "    ä¾‹: \n",
    "    æ—¥ä»˜        call_num    lag_1    lag_7\n",
    "    2018-06-01    183        NaN      NaN\n",
    "    2018-06-02      0        183      NaN\n",
    "    2018-06-08     96         0       183  <- 7æ—¥å‰ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 4: ãƒ©ã‚°ç‰¹å¾´é‡ã®ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"\\nç›®çš„å¤‰æ•°: {target_col}\")\n",
    "    print(f\"ä½œæˆã™ã‚‹ãƒ©ã‚°: {lags}\")\n",
    "    \n",
    "    for lag in lags:\n",
    "        col_name = f'lag_{lag}'\n",
    "        df[col_name] = df[target_col].shift(lag)\n",
    "        print(f\"  ä½œæˆ: {col_name} (shift={lag})\")\n",
    "    \n",
    "    # æœ€åˆã®Næ—¥ã¯ãƒ©ã‚°ç‰¹å¾´é‡ãŒNaNã«ãªã‚‹\n",
    "    print(f\"\\næ³¨æ„: æœ€åˆã®{max(lags)}æ—¥é–“ã¯ãƒ©ã‚°ç‰¹å¾´é‡ãŒNaNã«ãªã‚Šã¾ã™\")\n",
    "    print(f\"ä¾‹: lag_30ã¯æœ€åˆã®30æ—¥é–“ãŒNaN\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 5: ç§»å‹•å¹³å‡ç‰¹å¾´é‡ã®ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã«æ³¨æ„ï¼ï¼‰\n",
    "# ==================================================================================\n",
    "\n",
    "def create_rolling_features(df, target_col='call_num', windows=[3, 7, 14, 30]):\n",
    "    \"\"\"\n",
    "    ç§»å‹•å¹³å‡ãƒ»ç§»å‹•æ¨™æº–åå·®ã‚’ä½œæˆ\n",
    "    \n",
    "    é‡è¦:\n",
    "    - rolling().mean() ã‚’ä½¿ã†å‰ã« shift(1) ã‚’é©ç”¨\n",
    "    - ã“ã‚Œã«ã‚ˆã‚Šã€å½“æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒç§»å‹•å¹³å‡ã«å«ã¾ã‚Œãªã„ï¼ˆãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ï¼‰\n",
    "    \n",
    "    ä¾‹ï¼ˆwindow=3ã®å ´åˆï¼‰:\n",
    "    æ—¥ä»˜        call_num    éå»3æ—¥ã®å¹³å‡\n",
    "    2018-06-01    183        NaN\n",
    "    2018-06-02      0        NaN\n",
    "    2018-06-03      0        NaN\n",
    "    2018-06-04    213        61.0  <- (183+0+0)/3 = 61.0\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 5: ç§»å‹•å¹³å‡ç‰¹å¾´é‡ã®ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"\\nç›®çš„å¤‰æ•°: {target_col}\")\n",
    "    print(f\"ç§»å‹•å¹³å‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {windows}\")\n",
    "    \n",
    "    for window in windows:\n",
    "        # ç§»å‹•å¹³å‡ï¼ˆå½“æ—¥ã‚’å«ã¾ãªã„ï¼shift(1)ã—ã¦ã‹ã‚‰rollingï¼‰\n",
    "        ma_col = f'ma_{window}'\n",
    "        df[ma_col] = df[target_col].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        print(f\"  ä½œæˆ: {ma_col} (éå»{window}æ—¥é–“ã®å¹³å‡)\")\n",
    "        \n",
    "        # ç§»å‹•æ¨™æº–åå·®ï¼ˆå¤‰å‹•ã®å¤§ãã•ã‚’æ‰ãˆã‚‹ï¼‰\n",
    "        std_col = f'ma_std_{window}'\n",
    "        df[std_col] = df[target_col].shift(1).rolling(window=window, min_periods=1).std()\n",
    "        print(f\"  ä½œæˆ: {std_col} (éå»{window}æ—¥é–“ã®æ¨™æº–åå·®)\")\n",
    "    \n",
    "    print(\"\\nãªãœshift(1)ãŒå¿…è¦ã‹:\")\n",
    "    print(\"  Ã— æ‚ªã„ä¾‹: rolling(3).mean() â†’ å½“æ—¥å«ã‚€3æ—¥é–“ã®å¹³å‡ï¼ˆãƒªãƒ¼ã‚±ãƒ¼ã‚¸ï¼ï¼‰\")\n",
    "    print(\"  â—‹ è‰¯ã„ä¾‹: shift(1).rolling(3).mean() â†’ éå»3æ—¥é–“ã®å¹³å‡ï¼ˆå®‰å…¨ï¼‰\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 6: ãã®ä»–ã®é›†ç´„ç‰¹å¾´é‡\n",
    "# ==================================================================================\n",
    "\n",
    "def create_aggregated_features(df):\n",
    "    \"\"\"\n",
    "    ãã®ä»–ã®æœ‰ç”¨ãªé›†ç´„ç‰¹å¾´é‡ã‚’ä½œæˆ\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 6: ãã®ä»–ã®é›†ç´„ç‰¹å¾´é‡\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # CMåŠ¹æœã®ç´¯ç©ï¼ˆéå»7æ—¥é–“ã®CMå®Ÿæ–½å›æ•°ï¼‰\n",
    "    # ã“ã‚Œã‚‚å½“æ—¥ã‚’å«ã¾ãªã„ã‚ˆã†ã«shift(1)\n",
    "    df['cm_7d'] = df['cm_flg'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    print(\"  ä½œæˆ: cm_7d (éå»7æ—¥é–“ã®CMå®Ÿæ–½å›æ•°)\")\n",
    "    \n",
    "    # Google Trendsã®ç§»å‹•å¹³å‡ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰\n",
    "    df['gt_ma_7'] = df['search_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    print(\"  ä½œæˆ: gt_ma_7 (éå»7æ—¥é–“ã®Google Trendså¹³å‡)\")\n",
    "    \n",
    "    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå–å¾—æ•°ã®ç§»å‹•å¹³å‡\n",
    "    df['acc_ma_7'] = df['acc_get_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    print(\"  ä½œæˆ: acc_ma_7 (éå»7æ—¥é–“ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå–å¾—å¹³å‡)\")\n",
    "    \n",
    "    # æ›œæ—¥ã”ã¨ã®éå»å¹³å‡ï¼ˆåŒã˜æ›œæ—¥ã®éå»ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ï¼‰\n",
    "    # ã“ã‚Œã¯å°‘ã—é«˜åº¦ã ãŒã€æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹ã®ã«æœ‰åŠ¹\n",
    "    print(\"\\n  ä½œæˆä¸­: dow_avg (åŒã˜æ›œæ—¥ã®éå»å¹³å‡)...\")\n",
    "    df['dow_avg'] = np.nan\n",
    "    for dow in df['dow'].unique():\n",
    "        mask = df['dow'] == dow\n",
    "        # å„è¡Œã«ã¤ã„ã¦ã€ãã®è¡Œã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã‚’è¨ˆç®—\n",
    "        df.loc[mask, 'dow_avg'] = df.loc[mask, 'call_num'].shift(1).expanding().mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 7: ç‰¹å¾´é‡ã®é¸æŠã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆç¿Œæ—¥äºˆæ¸¬ã«å¤‰æ›´ï¼‰\n",
    "# ==================================================================================\n",
    "\n",
    "def select_features_and_split(df, test_months=3):\n",
    "    \"\"\"\n",
    "    ç‰¹å¾´é‡ã‚’é¸æŠã—ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²\n",
    "    \n",
    "    é‡è¦: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¯æ™‚é–“é †ã«åˆ†å‰²ã™ã‚‹\n",
    "    **exp03å¤‰æ›´ç‚¹**: ç›®çš„å¤‰æ•°ã‚’ call_num.shift(-1) ã«å¤‰æ›´ï¼ˆç¿Œæ—¥äºˆæ¸¬ï¼‰\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 7: ç‰¹å¾´é‡é¸æŠã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ğŸš¨ exp03å¤‰æ›´ç‚¹: ç¿Œæ—¥ã®å…¥é›»æ•°ã‚’ç›®çš„å¤‰æ•°ã«ã™ã‚‹\n",
    "    df = df.copy()\n",
    "    df['target_next_day'] = df['call_num'].shift(-1)\n",
    "    print(\"\\nğŸ¯ exp03: ç›®çš„å¤‰æ•°ã‚’ç¿Œæ—¥ã®å…¥é›»æ•°ã«è¨­å®š (call_num.shift(-1))\")\n",
    "    \n",
    "    # æœ€å¾Œã®è¡Œã¯targetãŒNaNã«ãªã‚‹ã®ã§å‰Šé™¤\n",
    "    df = df.dropna(subset=['target_next_day']).reset_index(drop=True)\n",
    "    print(f\"ç¿Œæ—¥ãƒ‡ãƒ¼ã‚¿ãŒãªã„æœ€çµ‚è¡Œã‚’å‰Šé™¤: {len(df)}è¡Œ\")\n",
    "    \n",
    "    # å¹³æ—¥ã®ã¿ã‚’ä½¿ç”¨ï¼ˆåœŸæ—¥ã¯å…¥é›»æ•°ãŒ0ãªã®ã§äºˆæ¸¬ä¸è¦ï¼‰\n",
    "    df_model = df[df['dow'].isin([1, 2, 3, 4, 5])].copy().reset_index(drop=True)\n",
    "    print(f\"\\nå¹³æ—¥ã®ã¿ã«çµã‚Šè¾¼ã¿: {len(df)} â†’ {len(df_model)}è¡Œ\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆexp01ã¨åŒã˜ï¼‰\n",
    "    feature_cols = [\n",
    "        # åŸºæœ¬çš„ãªæ™‚ç³»åˆ—ç‰¹å¾´é‡\n",
    "        'dow', 'day_of_month', 'month', 'quarter', 'year', \n",
    "        'days_from_start', 'day_of_year', 'week_of_year',\n",
    "        'is_month_start', 'is_month_end',\n",
    "        \n",
    "        # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´é‡\n",
    "        'woy', 'wom', 'day_before_holiday_flag',\n",
    "        \n",
    "        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿\n",
    "        'cm_flg', 'acc_get_cnt', 'search_cnt',\n",
    "        \n",
    "        # é›†ç´„ç‰¹å¾´é‡\n",
    "        'cm_7d', 'gt_ma_7', 'acc_ma_7', 'dow_avg',\n",
    "        \n",
    "        # ãƒ©ã‚°ç‰¹å¾´é‡\n",
    "        'lag_1', 'lag_2', 'lag_3', 'lag_5', 'lag_7', 'lag_14', 'lag_30',\n",
    "        \n",
    "        # ç§»å‹•å¹³å‡ç‰¹å¾´é‡\n",
    "        'ma_3', 'ma_7', 'ma_14', 'ma_30',\n",
    "        'ma_std_3', 'ma_std_7', 'ma_std_14', 'ma_std_30'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "    print(\"\\nç‰¹å¾´é‡ã®ã‚«ãƒ†ã‚´ãƒª:\")\n",
    "    print(f\"  - åŸºæœ¬æ™‚ç³»åˆ—: 10å€‹\")\n",
    "    print(f\"  - ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼: 3å€‹\")\n",
    "    print(f\"  - å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: 3å€‹\")\n",
    "    print(f\"  - é›†ç´„ç‰¹å¾´é‡: 4å€‹\")\n",
    "    print(f\"  - ãƒ©ã‚°ç‰¹å¾´é‡: 7å€‹\")\n",
    "    print(f\"  - ç§»å‹•å¹³å‡: 8å€‹\")\n",
    "    \n",
    "    # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæœ€å¾Œã®3ãƒ¶æœˆã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "    split_date = df_model['cdr_date'].max() - pd.Timedelta(days=30*test_months)\n",
    "    \n",
    "    train_df = df_model[df_model['cdr_date'] < split_date].copy()\n",
    "    test_df = df_model[df_model['cdr_date'] >= split_date].copy()\n",
    "    \n",
    "    print(f\"\\næ™‚ç³»åˆ—åˆ†å‰²:\")\n",
    "    print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æœŸé–“: {train_df['cdr_date'].min()} ~ {train_df['cdr_date'].max()}\")\n",
    "    print(f\"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æœŸé–“: {test_df['cdr_date'].min()} ~ {test_df['cdr_date'].max()}\")\n",
    "    print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_df)}è¡Œ\")\n",
    "    print(f\"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(test_df)}è¡Œ\")\n",
    "    \n",
    "    # æ¬ æå€¤ãŒã‚ã‚‹è¡Œã‚’å‰Šé™¤ï¼ˆãƒ©ã‚°ç‰¹å¾´é‡ã®åˆæœŸå€¤ç­‰ï¼‰\n",
    "    train_clean = train_df.dropna(subset=feature_cols + ['target_next_day'])\n",
    "    test_clean = test_df.dropna(subset=feature_cols + ['target_next_day'])\n",
    "    \n",
    "    print(f\"\\næ¬ æå€¤é™¤å»å¾Œ:\")\n",
    "    print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_clean)}è¡Œ\")\n",
    "    print(f\"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(test_clean)}è¡Œ\")\n",
    "    \n",
    "    # Xï¼ˆç‰¹å¾´é‡ï¼‰ã¨yï¼ˆç›®çš„å¤‰æ•°ï¼‰ã«åˆ†å‰²\n",
    "    X_train = train_clean[feature_cols]\n",
    "    y_train = train_clean['target_next_day']  # ğŸš¨ exp03: ç¿Œæ—¥ã®å€¤\n",
    "    X_test = test_clean[feature_cols]\n",
    "    y_test = test_clean['target_next_day']    # ğŸš¨ exp03: ç¿Œæ—¥ã®å€¤\n",
    "    \n",
    "    # ãƒ¡ã‚¿æƒ…å ±ã‚‚ä¿å­˜ï¼ˆæ—¥ä»˜ãªã©ï¼‰\n",
    "    train_meta = train_clean[['cdr_date', 'call_num', 'target_next_day']]\n",
    "    test_meta = test_clean[['cdr_date', 'call_num', 'target_next_day']]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, train_meta, test_meta, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 8: ç‰¹å¾´é‡ã®é‡è¦åº¦ç¢ºèªç”¨é–¢æ•°\n",
    "# ==================================================================================\n",
    "\n",
    "def analyze_features(X_train, y_train, feature_cols):\n",
    "    \"\"\"\n",
    "    ç‰¹å¾´é‡ã®åŸºæœ¬çµ±è¨ˆã¨ç›¸é–¢ã‚’ç¢ºèª\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 8: ç‰¹å¾´é‡ã®åˆ†æ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # åŸºæœ¬çµ±è¨ˆé‡\n",
    "    print(\"\\nç‰¹å¾´é‡ã®åŸºæœ¬çµ±è¨ˆ:\")\n",
    "    print(X_train.describe().T[['mean', 'std', 'min', 'max']].head(10))\n",
    "    \n",
    "    # ç›®çš„å¤‰æ•°ã¨ã®ç›¸é–¢\n",
    "    print(\"\\nç›®çš„å¤‰æ•°ï¼ˆç¿Œæ—¥å…¥é›»æ•°ï¼‰ã¨ã®ç›¸é–¢ï¼ˆä¸Šä½10ï¼‰:\")\n",
    "    correlations = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'correlation': [X_train[col].corr(y_train) for col in feature_cols]\n",
    "    }).sort_values('correlation', ascending=False, key=abs)\n",
    "    \n",
    "    print(correlations.head(10))\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°\n",
    "# ==================================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    å…¨ã¦ã®å‡¦ç†ã‚’å®Ÿè¡Œ\n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"*\" * 80)\n",
    "    print(\"exp08: AirREGI ãƒ˜ãƒ«ãƒ—ãƒ‡ã‚¹ã‚¯ ç¿Œæ—¥å…¥é›»äºˆæ¸¬ (Holdout Validation)\")\n",
    "    print(\"*\" * 80)\n",
    "    \n",
    "    # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "    calender, cm_data, gt_service, acc_get, call_data = load_and_preprocess_data()\n",
    "    \n",
    "    # Step 2: ãƒ‡ãƒ¼ã‚¿çµ±åˆ\n",
    "    df = merge_datasets(calender, cm_data, gt_service, acc_get, call_data)\n",
    "    \n",
    "    # Step 3: åŸºæœ¬æ™‚ç³»åˆ—ç‰¹å¾´é‡\n",
    "    df = create_basic_time_features(df)\n",
    "    \n",
    "    # Step 4: ãƒ©ã‚°ç‰¹å¾´é‡\n",
    "    df = create_lag_features(df, target_col='call_num', lags=[1, 2, 3, 5, 7, 14, 30])\n",
    "    \n",
    "    # Step 5: ç§»å‹•å¹³å‡ç‰¹å¾´é‡\n",
    "    df = create_rolling_features(df, target_col='call_num', windows=[3, 7, 14, 30])\n",
    "    \n",
    "    # Step 6: ãã®ä»–é›†ç´„ç‰¹å¾´é‡\n",
    "    df = create_aggregated_features(df)\n",
    "    \n",
    "    # Step 7: ç‰¹å¾´é‡é¸æŠã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆç¿Œæ—¥äºˆæ¸¬ç‰ˆï¼‰\n",
    "    X_train, X_test, y_train, y_test, train_meta, test_meta, feature_cols = \\\n",
    "        select_features_and_split(df, test_months=3)\n",
    "    \n",
    "    # Step 8: ç‰¹å¾´é‡åˆ†æ\n",
    "    correlations = analyze_features(X_train, y_train, feature_cols)\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # output ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
    "    import os\n",
    "    output_dir = '../output/exp08'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"  {output_dir} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    train_data = pd.concat([train_meta.reset_index(drop=True), \n",
    "                           X_train.reset_index(drop=True)], axis=1)\n",
    "    test_data = pd.concat([test_meta.reset_index(drop=True), \n",
    "                          X_test.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    train_data.to_csv(f'{output_dir}/train_features.csv', index=False)\n",
    "    test_data.to_csv(f'{output_dir}/test_features.csv', index=False)\n",
    "    correlations.to_csv(f'{output_dir}/feature_correlations.csv', index=False)\n",
    "    \n",
    "    print(\"\\nä¿å­˜å®Œäº†:\")\n",
    "    print(f\"  - {output_dir}/train_features.csv\")\n",
    "    print(f\"  - {output_dir}/test_features.csv\")\n",
    "    print(f\"  - {output_dir}/feature_correlations.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†ï¼\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}\")\n",
    "    print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}\")\n",
    "    print(f\"ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "    print(f\"\\nğŸ¯ ç›®çš„å¤‰æ•°: ç¿Œæ—¥ã®å…¥é›»æ•° (call_num.shift(-1))\")\n",
    "    \n",
    "    return df, X_train, X_test, y_train, y_test, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********************************************************************************\n",
      "exp08: AirREGI ãƒ˜ãƒ«ãƒ—ãƒ‡ã‚¹ã‚¯ ç¿Œæ—¥å…¥é›»äºˆæ¸¬ (Holdout Validation)\n",
      "********************************************************************************\n",
      "================================================================================\n",
      "Step 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
      "================================================================================\n",
      "\n",
      "ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿: (670, 10)\n",
      "CMãƒ‡ãƒ¼ã‚¿: (762, 2)\n",
      "Google Trendsãƒ‡ãƒ¼ã‚¿: (109, 2)\n",
      "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå–å¾—ãƒ‡ãƒ¼ã‚¿: (701, 2)\n",
      "å…¥é›»ãƒ‡ãƒ¼ã‚¿ï¼ˆç›®çš„å¤‰æ•°ï¼‰: (670, 2)\n",
      "\n",
      "================================================================================\n",
      "Step 2: ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ\n",
      "================================================================================\n",
      "\n",
      "ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿: (670, 2)\n",
      "ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼æƒ…å ±ãƒãƒ¼ã‚¸å¾Œ: (670, 11)\n",
      "CMæƒ…å ±ãƒãƒ¼ã‚¸å¾Œ: (670, 12)\n",
      "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå–å¾—æ•°ãƒãƒ¼ã‚¸å¾Œ: (670, 13)\n",
      "\n",
      "Google Trendsãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡â†’æ—¥æ¬¡ã«å±•é–‹...\n",
      "Google Trendsãƒãƒ¼ã‚¸å¾Œ: (670, 14)\n",
      "\n",
      "æ¬ æå€¤ã®æ•°:\n",
      "holiday_name               632\n",
      "cdr_date                     0\n",
      "call_num                     0\n",
      "dow                          0\n",
      "dow_name                     0\n",
      "woy                          0\n",
      "wom                          0\n",
      "doy                          0\n",
      "financial_year               0\n",
      "day_before_holiday_flag      0\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Step 3: åŸºæœ¬çš„ãªæ™‚ç³»åˆ—ç‰¹å¾´é‡ã®ä½œæˆ\n",
      "================================================================================\n",
      "\n",
      "ä½œæˆã—ãŸåŸºæœ¬ç‰¹å¾´é‡:\n",
      "['year', 'month', 'day_of_month', 'quarter', 'day_of_year', 'week_of_year', 'days_from_start', 'is_month_start', 'is_month_end']\n",
      "\n",
      "================================================================================\n",
      "Step 4: ãƒ©ã‚°ç‰¹å¾´é‡ã®ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ï¼‰\n",
      "================================================================================\n",
      "\n",
      "ç›®çš„å¤‰æ•°: call_num\n",
      "ä½œæˆã™ã‚‹ãƒ©ã‚°: [1, 2, 3, 5, 7, 14, 30]\n",
      "  ä½œæˆ: lag_1 (shift=1)\n",
      "  ä½œæˆ: lag_2 (shift=2)\n",
      "  ä½œæˆ: lag_3 (shift=3)\n",
      "  ä½œæˆ: lag_5 (shift=5)\n",
      "  ä½œæˆ: lag_7 (shift=7)\n",
      "  ä½œæˆ: lag_14 (shift=14)\n",
      "  ä½œæˆ: lag_30 (shift=30)\n",
      "\n",
      "æ³¨æ„: æœ€åˆã®30æ—¥é–“ã¯ãƒ©ã‚°ç‰¹å¾´é‡ãŒNaNã«ãªã‚Šã¾ã™\n",
      "ä¾‹: lag_30ã¯æœ€åˆã®30æ—¥é–“ãŒNaN\n",
      "\n",
      "================================================================================\n",
      "Step 5: ç§»å‹•å¹³å‡ç‰¹å¾´é‡ã®ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ï¼‰\n",
      "================================================================================\n",
      "\n",
      "ç›®çš„å¤‰æ•°: call_num\n",
      "ç§»å‹•å¹³å‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: [3, 7, 14, 30]\n",
      "  ä½œæˆ: ma_3 (éå»3æ—¥é–“ã®å¹³å‡)\n",
      "  ä½œæˆ: ma_std_3 (éå»3æ—¥é–“ã®æ¨™æº–åå·®)\n",
      "  ä½œæˆ: ma_7 (éå»7æ—¥é–“ã®å¹³å‡)\n",
      "  ä½œæˆ: ma_std_7 (éå»7æ—¥é–“ã®æ¨™æº–åå·®)\n",
      "  ä½œæˆ: ma_14 (éå»14æ—¥é–“ã®å¹³å‡)\n",
      "  ä½œæˆ: ma_std_14 (éå»14æ—¥é–“ã®æ¨™æº–åå·®)\n",
      "  ä½œæˆ: ma_30 (éå»30æ—¥é–“ã®å¹³å‡)\n",
      "  ä½œæˆ: ma_std_30 (éå»30æ—¥é–“ã®æ¨™æº–åå·®)\n",
      "\n",
      "ãªãœshift(1)ãŒå¿…è¦ã‹:\n",
      "  Ã— æ‚ªã„ä¾‹: rolling(3).mean() â†’ å½“æ—¥å«ã‚€3æ—¥é–“ã®å¹³å‡ï¼ˆãƒªãƒ¼ã‚±ãƒ¼ã‚¸ï¼ï¼‰\n",
      "  â—‹ è‰¯ã„ä¾‹: shift(1).rolling(3).mean() â†’ éå»3æ—¥é–“ã®å¹³å‡ï¼ˆå®‰å…¨ï¼‰\n",
      "\n",
      "================================================================================\n",
      "Step 6: ãã®ä»–ã®é›†ç´„ç‰¹å¾´é‡\n",
      "================================================================================\n",
      "  ä½œæˆ: cm_7d (éå»7æ—¥é–“ã®CMå®Ÿæ–½å›æ•°)\n",
      "  ä½œæˆ: gt_ma_7 (éå»7æ—¥é–“ã®Google Trendså¹³å‡)\n",
      "  ä½œæˆ: acc_ma_7 (éå»7æ—¥é–“ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå–å¾—å¹³å‡)\n",
      "\n",
      "  ä½œæˆä¸­: dow_avg (åŒã˜æ›œæ—¥ã®éå»å¹³å‡)...\n",
      "\n",
      "================================================================================\n",
      "Step 7: ç‰¹å¾´é‡é¸æŠã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ exp03: ç›®çš„å¤‰æ•°ã‚’ç¿Œæ—¥ã®å…¥é›»æ•°ã«è¨­å®š (call_num.shift(-1))\n",
      "ç¿Œæ—¥ãƒ‡ãƒ¼ã‚¿ãŒãªã„æœ€çµ‚è¡Œã‚’å‰Šé™¤: 669è¡Œ\n",
      "\n",
      "å¹³æ—¥ã®ã¿ã«çµã‚Šè¾¼ã¿: 669 â†’ 477è¡Œ\n",
      "\n",
      "ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: 35\n",
      "\n",
      "ç‰¹å¾´é‡ã®ã‚«ãƒ†ã‚´ãƒª:\n",
      "  - åŸºæœ¬æ™‚ç³»åˆ—: 10å€‹\n",
      "  - ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼: 3å€‹\n",
      "  - å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: 3å€‹\n",
      "  - é›†ç´„ç‰¹å¾´é‡: 4å€‹\n",
      "  - ãƒ©ã‚°ç‰¹å¾´é‡: 7å€‹\n",
      "  - ç§»å‹•å¹³å‡: 8å€‹\n",
      "\n",
      "æ™‚ç³»åˆ—åˆ†å‰²:\n",
      "  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æœŸé–“: 2018-06-01 00:00:00 ~ 2019-12-30 00:00:00\n",
      "  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æœŸé–“: 2019-12-31 00:00:00 ~ 2020-03-30 00:00:00\n",
      "  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: 412è¡Œ\n",
      "  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: 65è¡Œ\n",
      "\n",
      "æ¬ æå€¤é™¤å»å¾Œ:\n",
      "  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: 391è¡Œ\n",
      "  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: 65è¡Œ\n",
      "\n",
      "================================================================================\n",
      "Step 8: ç‰¹å¾´é‡ã®åˆ†æ\n",
      "================================================================================\n",
      "\n",
      "ç‰¹å¾´é‡ã®åŸºæœ¬çµ±è¨ˆ:\n",
      "                        mean         std     min     max\n",
      "dow                 2.994885    1.417826     1.0     5.0\n",
      "day_of_month       15.751918     8.79728     1.0    31.0\n",
      "month               7.493606    3.297137     1.0    12.0\n",
      "quarter             2.841432    1.064785     1.0     4.0\n",
      "year             2018.664962    0.472609  2018.0  2019.0\n",
      "days_from_start   303.202046  158.220189    31.0   577.0\n",
      "day_of_year       212.491049  100.818061     1.0   365.0\n",
      "week_of_year       30.757033   14.448132     1.0    52.0\n",
      "is_month_start       0.16624    0.372773     0.0     1.0\n",
      "is_month_end        0.214834    0.411233     0.0     1.0\n",
      "\n",
      "ç›®çš„å¤‰æ•°ï¼ˆç¿Œæ—¥å…¥é›»æ•°ï¼‰ã¨ã®ç›¸é–¢ï¼ˆä¸Šä½10ï¼‰:\n",
      "                    feature  correlation\n",
      "31                 ma_std_3     0.623357\n",
      "23                    lag_5     0.599610\n",
      "12  day_before_holiday_flag    -0.571440\n",
      "28                     ma_7     0.564414\n",
      "32                 ma_std_7     0.559566\n",
      "29                    ma_14     0.535766\n",
      "33                ma_std_14     0.530289\n",
      "18                 acc_ma_7     0.516915\n",
      "14              acc_get_cnt     0.506400\n",
      "30                    ma_30     0.480352\n",
      "\n",
      "================================================================================\n",
      "ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\n",
      "================================================================================\n",
      "  ../output/exp08 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ\n",
      "\n",
      "ä¿å­˜å®Œäº†:\n",
      "  - ../output/exp08/train_features.csv\n",
      "  - ../output/exp08/test_features.csv\n",
      "  - ../output/exp08/feature_correlations.csv\n",
      "\n",
      "================================================================================\n",
      "ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†ï¼\n",
      "================================================================================\n",
      "\n",
      "è¨“ç·´ãƒ‡ãƒ¼ã‚¿: (391, 35)\n",
      "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: (65, 35)\n",
      "ç‰¹å¾´é‡æ•°: 35\n",
      "\n",
      "ğŸ¯ ç›®çš„å¤‰æ•°: ç¿Œæ—¥ã®å…¥é›»æ•° (call_num.shift(-1))\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# å®Ÿè¡Œ\n",
    "# ==================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df, X_train, X_test, y_train, y_test, feature_cols = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆexp01ã¨åŒã˜ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (391, 35) Test: (65, 35)\n",
      "Example features: ['dow', 'day_of_month', 'month', 'quarter', 'year', 'days_from_start', 'day_of_year', 'week_of_year', 'is_month_start', 'is_month_end']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 0) Loadï¼ˆæ—¢ã«ä¸Šã§ä½œæˆæ¸ˆã¿ï¼‰\n",
    "# =========================\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "print(\"Example features:\", feature_cols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metrics functions defined (with WAPE)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Metrics helperï¼ˆWAPEè¿½åŠ ç‰ˆï¼‰\n",
    "# =========================\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆMAE, RMSE, R2, WAPEï¼‰\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    import numpy as np\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # WAPE (Weighted Absolute Percentage Error)\n",
    "    # WAPE = sum(|actual - predicted|) / sum(|actual|) * 100\n",
    "    wape = (np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))) * 100\n",
    "    \n",
    "    return {\n",
    "        \"MAE\": mae, \n",
    "        \"RMSE\": rmse, \n",
    "        \"R2\": r2,\n",
    "        \"WAPE\": wape\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "def add_result(name, y_pred):\n",
    "    m = evaluate(y_test, y_pred)\n",
    "    results.append({\"Model\": name, **m})\n",
    "    \n",
    "print(\"âœ… Metrics functions defined (with WAPE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Baselinesï¼ˆè¶…é‡è¦ï¼‰\n",
    "# =========================\n",
    "# ğŸš¨ exp08å¤‰æ›´ç‚¹: ç¿Œæ—¥äºˆæ¸¬ãªã®ã§ã€baselineã¯ã€Œä»Šæ—¥ã®å€¤ = æ˜æ—¥ã®äºˆæ¸¬ã€\n",
    "\n",
    "# test_features.csvã«ã¯ call_num ã¨ target_next_day ã®ä¸¡æ–¹ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹\n",
    "test_full = pd.read_csv(\"../output/exp08/test_features.csv\", parse_dates=[\"cdr_date\"])\n",
    "baseline_current_day = test_full['call_num'].values\n",
    "\n",
    "# (A) å½“æ—¥è¸è¥²: y_hat(t+1) = call_num(t)\n",
    "add_result(\"Baseline: current_day (ä»Šæ—¥=æ˜æ—¥)\", baseline_current_day)\n",
    "\n",
    "# (B) å…ˆé€±åŒæ›œæ—¥è¸è¥²: yhat = lag_7\n",
    "if \"lag_7\" in X_test.columns:\n",
    "    add_result(\"Baseline: lag_7 (å…ˆé€±åŒæ›œæ—¥)\", X_test[\"lag_7\"].values)\n",
    "else:\n",
    "    print(\"[skip] lag_7 not found\")\n",
    "\n",
    "# (C) éå»7æ—¥å¹³å‡: yhat = ma_7\n",
    "if \"ma_7\" in X_test.columns:\n",
    "    add_result(\"Baseline: ma_7 (éå»7æ—¥å¹³å‡)\", X_test[\"ma_7\"].values)\n",
    "else:\n",
    "    print(\"[skip] ma_7 not found\")\n",
    "\n",
    "# (D) æ··åˆï¼ˆåœ°å‘³ã«å¼·ã„ã“ã¨ãŒå¤šã„ï¼‰\n",
    "if \"lag_7\" in X_test.columns:\n",
    "    yhat = 0.7 * baseline_current_day + 0.3 * X_test[\"lag_7\"].values\n",
    "    add_result(\"Baseline: 0.7*current + 0.3*lag_7\", yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # 3) Modelsï¼ˆã¾ãšã¯sklearnã§å …ãï¼‰\n",
    "# # =========================\n",
    "# models = {}\n",
    "\n",
    "# # Ridgeï¼ˆèª¬æ˜ã—ã‚„ã™ãã€å¤–ã‚Œã«å¼·ã‚ã€‚ã¾ãšç½®ãï¼‰\n",
    "# models[\"Ridge\"] = Pipeline([\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"model\", Ridge(alpha=1.0, random_state=42))\n",
    "# ])\n",
    "\n",
    "# # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆéç·šå½¢ãƒ»ç›¸äº’ä½œç”¨ï¼‰\n",
    "# models[\"RandomForest\"] = Pipeline([\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"model\", RandomForestRegressor(\n",
    "#         n_estimators=500,\n",
    "#         max_depth=None,\n",
    "#         min_samples_split=5,\n",
    "#         min_samples_leaf=2,\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# # ExtraTreesï¼ˆRFã‚ˆã‚Šå¼·ã„ã“ã¨ã‚‚å¤šã„ï¼‰\n",
    "# models[\"ExtraTrees\"] = Pipeline([\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"model\", ExtraTreesRegressor(\n",
    "#         n_estimators=800,\n",
    "#         max_depth=None,\n",
    "#         min_samples_split=5,\n",
    "#         min_samples_leaf=2,\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# # å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆå¤å…¸ï¼‰\n",
    "# models[\"GradientBoosting\"] = Pipeline([\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"model\", GradientBoostingRegressor(random_state=42))\n",
    "# ])\n",
    "\n",
    "# # HistGradientBoostingï¼ˆé«˜é€Ÿã§å¼·ã„ã“ã¨å¤šã„ï¼‰\n",
    "# models[\"HistGradientBoosting\"] = Pipeline([\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"model\", HistGradientBoostingRegressor(\n",
    "#         max_depth=None,\n",
    "#         learning_rate=0.05,\n",
    "#         max_iter=2000,\n",
    "#         random_state=42\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     print(f\"Training {name}...\")\n",
    "#     model.fit(X_train, y_train)\n",
    "#     pred = model.predict(X_test)\n",
    "#     add_result(name, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (348, 35), Validation set: (43, 35)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Optional: XGBoost / LightGBM / CatBoostï¼ˆå…¥ã£ã¦ãŸã‚‰è©¦ã™ï¼‰\n",
    "# =========================\n",
    "# æ™‚ç³»åˆ—ãªã®ã§ã€Œtrainã®æœ€å¾Œã®ä¸€éƒ¨ã€ã‚’validã«ã—ã¦early stoppingã™ã‚‹\n",
    "\n",
    "# train_features.csvã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã‚“ã§åˆ†å‰²\n",
    "train_full_for_split = pd.read_csv(\"../output/exp08/train_features.csv\", parse_dates=[\"cdr_date\"])\n",
    "\n",
    "def time_valid_split_from_df(df, valid_days=60):\n",
    "    \"\"\"\n",
    "    Holdout Validation: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§Train/Valã«åˆ†å‰²\n",
    "    \"\"\"\n",
    "    TARGET = \"target_next_day\"\n",
    "    DROP_COLS = [\"cdr_date\", \"call_num\", TARGET]\n",
    "    \n",
    "    cutoff = df['cdr_date'].max() - pd.Timedelta(days=valid_days)\n",
    "    \n",
    "    tr_df = df[df['cdr_date'] < cutoff].copy()\n",
    "    va_df = df[df['cdr_date'] >= cutoff].copy()\n",
    "    \n",
    "    X_tr = tr_df.drop(columns=DROP_COLS)\n",
    "    y_tr = tr_df[TARGET].astype(float)\n",
    "    X_va = va_df.drop(columns=DROP_COLS)\n",
    "    y_va = va_df[TARGET].astype(float)\n",
    "    \n",
    "    return X_tr, y_tr, X_va, y_va\n",
    "\n",
    "X_tr, y_tr, X_va, y_va = time_valid_split_from_df(train_full_for_split, valid_days=60)\n",
    "\n",
    "print(f\"Training set: {X_tr.shape}, Validation set: {X_va.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        verbose=False\n",
    "    )\n",
    "    pred = xgb.predict(X_test)\n",
    "    add_result(\"XGBoost\", pred)\n",
    "except Exception as e:\n",
    "    print(\"[skip] XGBoost:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2196\n",
      "[LightGBM] [Info] Number of data points in the train set: 348, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 136.114943\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    lgbm = lgb.LGBMRegressor(\n",
    "        n_estimators=20000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print(\"Training LightGBM...\")\n",
    "    lgbm.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"l1\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "    )\n",
    "    pred = lgbm.predict(X_test)\n",
    "    add_result(\"LightGBM\", pred)\n",
    "except Exception as e:\n",
    "    print(\"[skip] LightGBM:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CatBoost...\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "\n",
    "    cat = CatBoostRegressor(\n",
    "        iterations=20000,\n",
    "        learning_rate=0.03,\n",
    "        depth=8,\n",
    "        loss_function=\"MAE\",\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"Training CatBoost...\")\n",
    "    cat.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n",
    "    pred = cat.predict(X_test)\n",
    "    add_result(\"CatBoost\", pred)\n",
    "except Exception as e:\n",
    "    print(\"[skip] CatBoost:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "=== Results (sorted by MAE) ===\n",
      "================================================================================\n",
      "                            Model       MAE      RMSE        R2      WAPE\n",
      "                         CatBoost 31.593947 39.773787  0.606597 32.319901\n",
      "                         LightGBM 45.754600 53.691686  0.283100 46.805933\n",
      "                          XGBoost 47.608386 59.560069  0.117825 48.702315\n",
      "Baseline: 0.7*current + 0.3*lag_7 48.972308 68.757881 -0.175680 50.097576\n",
      "    Baseline: current_day (ä»Šæ—¥=æ˜æ—¥) 50.415385 74.360763 -0.375092 51.573812\n",
      "          Baseline: ma_7 (éå»7æ—¥å¹³å‡) 57.749451 67.932329 -0.147618 59.076397\n",
      "          Baseline: lag_7 (å…ˆé€±åŒæ›œæ—¥) 67.861538 86.400053 -0.856403 69.420837\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "=== Results (sorted by WAPE) ===\n",
      "================================================================================\n",
      "                            Model       MAE      RMSE        R2      WAPE\n",
      "                         CatBoost 31.593947 39.773787  0.606597 32.319901\n",
      "                         LightGBM 45.754600 53.691686  0.283100 46.805933\n",
      "                          XGBoost 47.608386 59.560069  0.117825 48.702315\n",
      "Baseline: 0.7*current + 0.3*lag_7 48.972308 68.757881 -0.175680 50.097576\n",
      "    Baseline: current_day (ä»Šæ—¥=æ˜æ—¥) 50.415385 74.360763 -0.375092 51.573812\n",
      "          Baseline: ma_7 (éå»7æ—¥å¹³å‡) 57.749451 67.932329 -0.147618 59.076397\n",
      "          Baseline: lag_7 (å…ˆé€±åŒæ›œæ—¥) 67.861538 86.400053 -0.856403 69.420837\n",
      "================================================================================\n",
      "\n",
      "âœ… Saved: ../output/exp08/model_results.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Show resultsï¼ˆWAPEå¯¾å¿œç‰ˆï¼‰\n",
    "# =========================\n",
    "res_df = pd.DataFrame(results).sort_values(\"MAE\").reset_index(drop=True)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== Results (sorted by MAE) ===\")\n",
    "print(\"=\"*80)\n",
    "print(res_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# WAPEã§ã‚‚ã‚½ãƒ¼ãƒˆè¡¨ç¤º\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== Results (sorted by WAPE) ===\")\n",
    "print(\"=\"*80)\n",
    "res_df_wape = res_df.sort_values(\"WAPE\").reset_index(drop=True)\n",
    "print(res_df_wape.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ä¿å­˜\n",
    "import os\n",
    "output_dir = '../output/exp08'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "res_df.to_csv(f\"{output_dir}/model_results.csv\", index=False)\n",
    "print(f\"\\nâœ… Saved: {output_dir}/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: CatBoost\n",
      "Test MAE : 31.594\n",
      "Test RMSE: 39.774\n",
      "Test R2  : 0.607\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8332/3684638398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# å›³1ï¼šæ™‚ç³»åˆ—\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"actual\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Actual (Next Day)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Predicted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) Visualization\n",
    "# =========================\n",
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å–å¾—\n",
    "try:\n",
    "    best_pred = cat.predict(X_test)\n",
    "    model_name = \"CatBoost\"\n",
    "except:\n",
    "    try:\n",
    "        best_pred = lgbm.predict(X_test)\n",
    "        model_name = \"LightGBM\"\n",
    "    except:\n",
    "        try:\n",
    "            best_pred = xgb.predict(X_test)\n",
    "            model_name = \"XGBoost\"\n",
    "        except:\n",
    "            best_pred = models[\"Ridge\"].predict(X_test)\n",
    "            model_name = \"Ridge\"\n",
    "\n",
    "mae = mean_absolute_error(y_test, best_pred)\n",
    "rmse = mean_squared_error(y_test, best_pred, squared=False)\n",
    "r2 = r2_score(y_test, best_pred)\n",
    "\n",
    "print(f\"\\nBest Model: {model_name}\")\n",
    "print(f\"Test MAE : {mae:.3f}\")\n",
    "print(f\"Test RMSE: {rmse:.3f}\")\n",
    "print(f\"Test R2  : {r2:.3f}\")\n",
    "\n",
    "# ğŸš¨ ä¿®æ­£: test_meta ã§ã¯ãªã test_full ã‚’ä½¿ã†\n",
    "plot_df = pd.DataFrame({\n",
    "    \"date\": test_full['cdr_date'].values,  # â† ã“ã“ã‚’ä¿®æ­£\n",
    "    \"actual\": y_test.values,\n",
    "    \"pred\": np.asarray(best_pred, dtype=float)\n",
    "}).sort_values(\"date\")\n",
    "\n",
    "# å›³1ï¼šæ™‚ç³»åˆ—\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(plot_df[\"date\"], plot_df[\"actual\"], label=\"Actual (Next Day)\", marker='o')\n",
    "plt.plot(plot_df[\"date\"], plot_df[\"pred\"], label=\"Predicted\", marker='x')\n",
    "plt.title(f\"exp03: Next-Day Prediction | {model_name} | MAE={mae:.1f}, RMSE={rmse:.1f}, R2={r2:.2f}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Calls (Next Day)\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/prediction_timeseries.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# å›³2ï¼šæ•£å¸ƒå›³\n",
    "plt.figure(figsize=(5.5, 5.5))\n",
    "plt.scatter(plot_df[\"actual\"], plot_df[\"pred\"], alpha=0.6)\n",
    "mn = min(plot_df[\"actual\"].min(), plot_df[\"pred\"].min())\n",
    "mx = max(plot_df[\"actual\"].max(), plot_df[\"pred\"].max())\n",
    "plt.plot([mn, mx], [mn, mx], 'r--')\n",
    "plt.title(\"Predicted vs Actual (Next Day)\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/prediction_scatter.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Plots saved to {output_dir}/\")\n",
    "print(\"=\"*60)\n",
    "print(\"exp03 complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… exp03å®Œäº†\n",
    "\n",
    "### exp01ã¨ã®é•ã„\n",
    "- **ç›®çš„å¤‰æ•°**: `call_num` â†’ `call_num.shift(-1)` ï¼ˆç¿Œæ—¥äºˆæ¸¬ï¼‰\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: `lag_1` â†’ `call_num`ï¼ˆå½“æ—¥ã®å€¤ï¼‰\n",
    "- **ç‰¹å¾´é‡**: å®Œå…¨ã«åŒã˜\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: å®Œå…¨ã«åŒã˜\n",
    "\n",
    "### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "- `output/exp03/train_features.csv` - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆtarget_next_dayä»˜ãï¼‰\n",
    "- `output/exp03/test_features.csv` - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆtarget_next_dayä»˜ãï¼‰\n",
    "- `output/exp03/feature_correlations.csv` - ç‰¹å¾´é‡ç›¸é–¢\n",
    "- `output/exp03/model_results.csv` - å…¨ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡çµæœ\n",
    "- `output/exp03/prediction_timeseries.png` - æ™‚ç³»åˆ—äºˆæ¸¬ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "- `output/exp03/prediction_scatter.png` - æ•£å¸ƒå›³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp08ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ãƒ»å¯è¦–åŒ–\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# CatBoostãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—\n",
    "# ï¼ˆæ—¢ã«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œæ¸ˆã¿ã®å‰æï¼‰\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"exp08 - CatBoost Feature Importance\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CatBoostãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "try:\n",
    "    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—\n",
    "    feature_importance = cat.feature_importances_\n",
    "    feature_names = X_test.columns.tolist()\n",
    "    \n",
    "    # DataFrameã«æ•´å½¢\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nTotal features: {len(importance_df)}\")\n",
    "    print(f\"\\nTop 20 features:\")\n",
    "    print(importance_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    import os\n",
    "    output_dir = '../output/exp08'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    importance_df.to_csv(f'{output_dir}/feature_importance.csv', index=False)\n",
    "    print(f\"\\nâœ… Saved to {output_dir}/feature_importance.csv\")\n",
    "    \n",
    "    # å¯è¦–åŒ–1: Top 20\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # æ¨ªæ£’ã‚°ãƒ©ãƒ•\n",
    "    ax = axes[0]\n",
    "    top20 = importance_df.head(20).sort_values('importance')\n",
    "    ax.barh(range(len(top20)), top20['importance'])\n",
    "    ax.set_yticks(range(len(top20)))\n",
    "    ax.set_yticklabels(top20['feature'])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'exp08 - Top 20 Features (Total: {len(importance_df)})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ£’ã‚°ãƒ©ãƒ•ï¼ˆå…¨ç‰¹å¾´é‡ï¼‰\n",
    "    ax = axes[1]\n",
    "    ax.bar(range(len(importance_df)), importance_df['importance'])\n",
    "    ax.set_xlabel('Feature Index (sorted by importance)')\n",
    "    ax.set_ylabel('Importance')\n",
    "    ax.set_title('exp08 - All Features Importance Distribution')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/feature_importance_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # å¯è¦–åŒ–2: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡è¦åº¦\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Feature Importance by Category\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡\n",
    "    categories = {\n",
    "        'Lag': [f'lag_{i}' for i in [1,2,3,5,7,14,30]],\n",
    "        'Moving Avg': ['ma_3', 'ma_7', 'ma_14', 'ma_30'],\n",
    "        'Moving Std': ['ma_std_3', 'ma_std_7', 'ma_std_14', 'ma_std_30'],\n",
    "        'Time': ['dow', 'day_of_month', 'month', 'quarter', 'year', \n",
    "                 'days_from_start', 'day_of_year', 'week_of_year',\n",
    "                 'is_month_start', 'is_month_end', 'woy', 'wom'],\n",
    "        'External': ['cm_flg', 'acc_get_cnt', 'search_cnt'],\n",
    "        'Aggregated': ['cm_7d', 'gt_ma_7', 'acc_ma_7', 'dow_avg'],\n",
    "        'Other': ['day_before_holiday_flag']\n",
    "    }\n",
    "    \n",
    "    category_importance = {}\n",
    "    for cat_name, features in categories.items():\n",
    "        cat_imp = importance_df[importance_df['feature'].isin(features)]['importance'].sum()\n",
    "        category_importance[cat_name] = cat_imp\n",
    "    \n",
    "    cat_df = pd.DataFrame({\n",
    "        'Category': category_importance.keys(),\n",
    "        'Total Importance': category_importance.values()\n",
    "    }).sort_values('Total Importance', ascending=False)\n",
    "    \n",
    "    print(cat_df.to_string(index=False))\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å¯è¦–åŒ–\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(cat_df['Category'], cat_df['Total Importance'])\n",
    "    ax.set_ylabel('Total Importance')\n",
    "    ax.set_title('exp08 - Feature Importance by Category')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/feature_importance_by_category.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except NameError:\n",
    "    print(\"âš ï¸ CatBoostãƒ¢ãƒ‡ãƒ«ï¼ˆcatï¼‰ãŒã¾ã å­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "except Exception as e:\n",
    "    print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# WAPEè©³ç´°åˆ†æ\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WAPE Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆMAEåŸºæº–ï¼‰\n",
    "best_mae_model = res_df.iloc[0]['Model']\n",
    "best_mae_wape = res_df.iloc[0]['WAPE']\n",
    "\n",
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆWAPEåŸºæº–ï¼‰\n",
    "best_wape_model = res_df_wape.iloc[0]['Model']\n",
    "best_wape_value = res_df_wape.iloc[0]['WAPE']\n",
    "\n",
    "print(f\"\\nBest model by MAE:  {best_mae_model} (WAPE: {best_mae_wape:.2f}%)\")\n",
    "print(f\"Best model by WAPE: {best_wape_model} (WAPE: {best_wape_value:.2f}%)\")\n",
    "\n",
    "# WAPEã®è§£é‡ˆ\n",
    "print(f\"\\nWAPE Interpretation:\")\n",
    "print(f\"  WAPE < 10%:  Excellent\")\n",
    "print(f\"  WAPE < 20%:  Good\")\n",
    "print(f\"  WAPE < 30%:  Fair\")\n",
    "print(f\"  WAPE >= 30%: Needs improvement\")\n",
    "\n",
    "if best_wape_value < 10:\n",
    "    grade = \"Excellent â­â­â­\"\n",
    "elif best_wape_value < 20:\n",
    "    grade = \"Good â­â­\"\n",
    "elif best_wape_value < 30:\n",
    "    grade = \"Fair â­\"\n",
    "else:\n",
    "    grade = \"Needs improvement\"\n",
    "\n",
    "print(f\"\\nBest model WAPE: {best_wape_value:.2f}% â†’ {grade}\")\n",
    "\n",
    "# å¯è¦–åŒ–: MAE vs WAPE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAEæ¯”è¼ƒ\n",
    "ax = axes[0]\n",
    "models = res_df['Model'].values\n",
    "mae_values = res_df['MAE'].values\n",
    "colors = ['green' if i == 0 else 'skyblue' for i in range(len(models))]\n",
    "bars = ax.barh(range(len(models)), mae_values, color=colors)\n",
    "ax.set_yticks(range(len(models)))\n",
    "ax.set_yticklabels(models, fontsize=8)\n",
    "ax.set_xlabel('MAE')\n",
    "ax.set_title('Model Comparison: MAE')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "# å€¤ãƒ©ãƒ™ãƒ«\n",
    "for i, (bar, val) in enumerate(zip(bars, mae_values)):\n",
    "    ax.text(val, i, f' {val:.1f}', va='center', fontsize=8)\n",
    "\n",
    "# WAPEæ¯”è¼ƒ\n",
    "ax = axes[1]\n",
    "wape_values = res_df_wape['WAPE'].values\n",
    "models_wape = res_df_wape['Model'].values\n",
    "colors = ['green' if i == 0 else 'lightcoral' for i in range(len(models_wape))]\n",
    "bars = ax.barh(range(len(models_wape)), wape_values, color=colors)\n",
    "ax.set_yticks(range(len(models_wape)))\n",
    "ax.set_yticklabels(models_wape, fontsize=8)\n",
    "ax.set_xlabel('WAPE (%)')\n",
    "ax.set_title('Model Comparison: WAPE')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "# å€¤ãƒ©ãƒ™ãƒ«\n",
    "for i, (bar, val) in enumerate(zip(bars, wape_values)):\n",
    "    ax.text(val, i, f' {val:.1f}%', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/metrics_comparison_with_wape.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Plot saved: {output_dir}/metrics_comparison_with_wape.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
