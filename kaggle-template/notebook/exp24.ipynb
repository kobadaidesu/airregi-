{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp24: アカウント獲得数予測 + Optuna最適化（Holdout Validation）\n",
    "\n",
    "**目的変数**: `acc_get_cnt`（翌日のアカウント獲得数）\n",
    "\n",
    "**背景**:\n",
    "- exp22/23では入電数（call_num）を予測\n",
    "- アカウント獲得数も重要なKPIであり、予測モデルを構築\n",
    "\n",
    "**実験内容**:\n",
    "1. アカウント獲得数用の特徴量設計\n",
    "2. Optunaで各モデルを最適化\n",
    "3. Holdout Validationで評価\n",
    "\n",
    "**使用モデル**:\n",
    "- Ridge\n",
    "- ExtraTrees\n",
    "- HistGradientBoosting\n",
    "- CatBoost\n",
    "- WeightedEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "# 出力ディレクトリ\n",
    "output_dir = '../output/exp24'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print('exp24: アカウント獲得数予測')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# データの読み込み\n",
    "# ==================================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    calender = pd.read_csv('../input/calender_data.csv')\n",
    "    cm_data = pd.read_csv('../input/cm_data.csv')\n",
    "    gt_service = pd.read_csv('../input/gt_service_name.csv')\n",
    "    acc_get = pd.read_csv('../input/regi_acc_get_data_transform.csv')\n",
    "    call_data = pd.read_csv('../input/regi_call_data_transform.csv')\n",
    "    \n",
    "    calender['cdr_date'] = pd.to_datetime(calender['cdr_date'])\n",
    "    cm_data['cdr_date'] = pd.to_datetime(cm_data['cdr_date'])\n",
    "    acc_get['cdr_date'] = pd.to_datetime(acc_get['cdr_date'])\n",
    "    call_data['cdr_date'] = pd.to_datetime(call_data['cdr_date'])\n",
    "    gt_service['week'] = pd.to_datetime(gt_service['week'])\n",
    "    \n",
    "    return calender, cm_data, gt_service, acc_get, call_data\n",
    "\n",
    "def merge_datasets(calender, cm_data, gt_service, acc_get, call_data):\n",
    "    # アカウント獲得数をベースにする\n",
    "    df = acc_get.copy()\n",
    "    df = df.merge(calender, on='cdr_date', how='left')\n",
    "    df = df.merge(cm_data, on='cdr_date', how='left')\n",
    "    df = df.merge(call_data, on='cdr_date', how='left')\n",
    "    \n",
    "    # Google Trendsを週次→日次に展開\n",
    "    gt_service_daily = []\n",
    "    for idx, row in gt_service.iterrows():\n",
    "        week_start = row['week']\n",
    "        for i in range(7):\n",
    "            date = week_start + timedelta(days=i)\n",
    "            gt_service_daily.append({'cdr_date': date, 'search_cnt': row['search_cnt']})\n",
    "    \n",
    "    gt_daily = pd.DataFrame(gt_service_daily)\n",
    "    df = df.merge(gt_daily, on='cdr_date', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('データ読み込み関数を定義しました')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 特徴量作成関数（アカウント獲得数予測用）\n",
    "# ==================================================================================\n",
    "\n",
    "def create_basic_time_features(df):\n",
    "    \"\"\"基本的な時系列特徴量\"\"\"\n",
    "    df = df.copy()\n",
    "    df['year'] = df['cdr_date'].dt.year\n",
    "    df['month'] = df['cdr_date'].dt.month\n",
    "    df['day_of_month'] = df['cdr_date'].dt.day\n",
    "    df['quarter'] = df['cdr_date'].dt.quarter\n",
    "    df['day_of_year'] = df['cdr_date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['cdr_date'].dt.isocalendar().week\n",
    "    df['days_from_start'] = (df['cdr_date'] - df['cdr_date'].min()).dt.days\n",
    "    df['is_month_start'] = (df['day_of_month'] <= 5).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] >= 25).astype(int)\n",
    "    return df\n",
    "\n",
    "def create_acc_lag_features(df, target_col='acc_get_cnt', lags=[1, 2, 3, 5, 7, 14, 30]):\n",
    "    \"\"\"アカウント獲得数のラグ特徴量\"\"\"\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f'acc_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def create_acc_rolling_features(df, target_col='acc_get_cnt', windows=[3, 7, 14, 30]):\n",
    "    \"\"\"アカウント獲得数の移動統計量\"\"\"\n",
    "    df = df.copy()\n",
    "    for window in windows:\n",
    "        df[f'acc_ma_{window}'] = df[target_col].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        df[f'acc_ma_std_{window}'] = df[target_col].shift(1).rolling(window=window, min_periods=1).std()\n",
    "    return df\n",
    "\n",
    "def create_call_features(df):\n",
    "    \"\"\"入電数関連の特徴量（アカ獲との関連を捉える）\"\"\"\n",
    "    df = df.copy()\n",
    "    # 入電数のラグ\n",
    "    df['call_lag_1'] = df['call_num'].shift(1)\n",
    "    df['call_lag_7'] = df['call_num'].shift(7)\n",
    "    # 入電数の移動平均\n",
    "    df['call_ma_7'] = df['call_num'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    df['call_ma_14'] = df['call_num'].shift(1).rolling(window=14, min_periods=1).mean()\n",
    "    return df\n",
    "\n",
    "def create_aggregated_features(df):\n",
    "    \"\"\"集約特徴量\"\"\"\n",
    "    df = df.copy()\n",
    "    # CM効果の累積\n",
    "    df['cm_7d'] = df['cm_flg'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    df['cm_14d'] = df['cm_flg'].shift(1).rolling(window=14, min_periods=1).sum()\n",
    "    # Google Trendsの移動平均\n",
    "    df['gt_ma_7'] = df['search_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    df['gt_ma_14'] = df['search_cnt'].shift(1).rolling(window=14, min_periods=1).mean()\n",
    "    # 曜日ごとの過去平均\n",
    "    df['dow_avg'] = np.nan\n",
    "    for dow in df['dow'].unique():\n",
    "        mask = df['dow'] == dow\n",
    "        df.loc[mask, 'dow_avg'] = df.loc[mask, 'acc_get_cnt'].shift(1).expanding().mean()\n",
    "    return df\n",
    "\n",
    "def create_regime_change_features(df):\n",
    "    \"\"\"レジーム変化特徴量（消費税増税など）\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    tax_implementation_date = pd.Timestamp('2019-10-01')\n",
    "    rush_deadline = pd.Timestamp('2019-09-30')\n",
    "    \n",
    "    df['days_to_2019_10_01'] = (tax_implementation_date - df['cdr_date']).dt.days\n",
    "    df['is_post_2019_10_01'] = (df['cdr_date'] >= tax_implementation_date).astype(int)\n",
    "    df['is_post_2019_09_30'] = (df['cdr_date'] >= rush_deadline).astype(int)\n",
    "    \n",
    "    # 駆け込み需要期間\n",
    "    rush_start = rush_deadline - pd.Timedelta(days=90)\n",
    "    df['is_rush_period'] = ((df['cdr_date'] >= rush_start) & \n",
    "                            (df['cdr_date'] <= rush_deadline)).astype(int)\n",
    "    \n",
    "    # 適応期間\n",
    "    adaptation_end = tax_implementation_date + pd.Timedelta(days=30)\n",
    "    df['is_adaptation_period'] = ((df['cdr_date'] >= tax_implementation_date) & \n",
    "                                   (df['cdr_date'] <= adaptation_end)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('特徴量作成関数を定義しました')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# データ準備\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(\"exp24: アカウント獲得数予測 + Optuna最適化\")\n",
    "print(\"*\" * 80)\n",
    "\n",
    "calender, cm_data, gt_service, acc_get, call_data = load_and_preprocess_data()\n",
    "df = merge_datasets(calender, cm_data, gt_service, acc_get, call_data)\n",
    "\n",
    "print(f\"\\n統合後データ: {df.shape}\")\n",
    "print(f\"期間: {df['cdr_date'].min()} ~ {df['cdr_date'].max()}\")\n",
    "\n",
    "# 特徴量作成\n",
    "df = create_basic_time_features(df)\n",
    "df = create_acc_lag_features(df)\n",
    "df = create_acc_rolling_features(df)\n",
    "df = create_call_features(df)\n",
    "df = create_aggregated_features(df)\n",
    "df = create_regime_change_features(df)\n",
    "\n",
    "# 翌日のアカウント獲得数を目的変数にする\n",
    "df['target_next_day'] = df['acc_get_cnt'].shift(-1)\n",
    "df = df.dropna(subset=['target_next_day']).reset_index(drop=True)\n",
    "\n",
    "# 平日のみ（土日はアカ獲が少ない傾向があるため）\n",
    "df_model = df[df['dow'].isin([1, 2, 3, 4, 5])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n平日データ数: {len(df_model)}行\")\n",
    "print(f\"期間: {df_model['cdr_date'].min()} ~ {df_model['cdr_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 特徴量の定義\n",
    "# ==================================================================================\n",
    "\n",
    "feature_cols = [\n",
    "    # 基本時系列特徴量\n",
    "    'dow', 'day_of_month', 'year', \n",
    "    'day_of_year', 'week_of_year',\n",
    "    'is_month_start', 'is_month_end',\n",
    "    # カレンダー特徴量\n",
    "    'day_before_holiday_flag',\n",
    "    # 外部データ\n",
    "    'cm_flg', 'search_cnt',\n",
    "    # 入電数関連\n",
    "    'call_num', 'call_lag_1', 'call_lag_7', 'call_ma_7', 'call_ma_14',\n",
    "    # アカウント獲得数ラグ\n",
    "    'acc_lag_1', 'acc_lag_2', 'acc_lag_3', 'acc_lag_5', 'acc_lag_7', 'acc_lag_14', 'acc_lag_30',\n",
    "    # アカウント獲得数移動平均\n",
    "    'acc_ma_3', 'acc_ma_7', 'acc_ma_14', 'acc_ma_30',\n",
    "    'acc_ma_std_3', 'acc_ma_std_7', 'acc_ma_std_14', 'acc_ma_std_30',\n",
    "    # 集約特徴量\n",
    "    'cm_7d', 'cm_14d', 'gt_ma_7', 'gt_ma_14', 'dow_avg',\n",
    "    # レジーム変化特徴量\n",
    "    'days_to_2019_10_01', 'is_post_2019_10_01',\n",
    "    'is_post_2019_09_30',\n",
    "    'is_rush_period', 'is_adaptation_period',\n",
    "]\n",
    "\n",
    "print(f\"使用する特徴量数: {len(feature_cols)}\")\n",
    "print(\"\\n【特徴量一覧】\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# 欠損値を除去\n",
    "df_clean = df_model.dropna(subset=feature_cols + ['target_next_day']).copy()\n",
    "print(f\"\\n欠損値除去後: {len(df_clean)}行\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Holdout Validation 設定\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Holdout Validation 設定\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Holdout分割（exp22と同じ期間）\n",
    "test_start_date = pd.Timestamp('2020-01-30')\n",
    "train_end_date = test_start_date - pd.Timedelta(days=1)\n",
    "\n",
    "train_df = df_clean[df_clean['cdr_date'] <= train_end_date].copy()\n",
    "test_df = df_clean[df_clean['cdr_date'] >= test_start_date].copy()\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['target_next_day']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['target_next_day']\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}件 ({train_df['cdr_date'].min().strftime('%Y-%m-%d')} ~ {train_df['cdr_date'].max().strftime('%Y-%m-%d')})\")\n",
    "print(f\"Test : {len(X_test)}件 ({test_df['cdr_date'].min().strftime('%Y-%m-%d')} ~ {test_df['cdr_date'].max().strftime('%Y-%m-%d')})\")\n",
    "\n",
    "# 目的変数の統計\n",
    "print(f\"\\n【目的変数（翌日アカウント獲得数）の統計】\")\n",
    "print(f\"  Train - 平均: {y_train.mean():.1f}, 標準偏差: {y_train.std():.1f}, 最小: {y_train.min():.0f}, 最大: {y_train.max():.0f}\")\n",
    "print(f\"  Test  - 平均: {y_test.mean():.1f}, 標準偏差: {y_test.std():.1f}, 最小: {y_test.min():.0f}, 最大: {y_test.max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 評価関数\n",
    "# ==================================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def calculate_wape(y_true, y_pred):\n",
    "    \"\"\"Weighted Absolute Percentage Error\"\"\"\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"モデル評価指標を計算\"\"\"\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'WAPE': calculate_wape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "print('評価関数を定義しました')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Optunaによるハイパーパラメータ最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Optuna 設定\n",
    "# ==================================================================================\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# TimeSeriesCV（3分割）\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Optunaによるハイパーパラメータ最適化\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n各モデル100トライアル、TimeSeriesCV(n_splits=3)で評価\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Ridge 最適化\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[1/4] Ridge 最適化\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def objective_ridge(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 100.0, log=True)\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = Ridge(alpha=alpha, random_state=42)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_ridge = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_ridge.optimize(objective_ridge, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE (CV): {study_ridge.best_value:.2f}\")\n",
    "print(f\"Best params: {study_ridge.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ExtraTrees 最適化\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[2/4] ExtraTrees 最適化\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def objective_extra(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = ExtraTreesRegressor(**params, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_extra = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_extra.optimize(objective_extra, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE (CV): {study_extra.best_value:.2f}\")\n",
    "print(f\"Best params: {study_extra.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# HistGradientBoosting 最適化\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[3/4] HistGradientBoosting 最適化\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def objective_hist(trial):\n",
    "    params = {\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 600),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 50),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.01, 50.0, log=True),\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = HistGradientBoostingRegressor(**params, random_state=42)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_hist = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_hist.optimize(objective_hist, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE (CV): {study_hist.best_value:.2f}\")\n",
    "print(f\"Best params: {study_hist.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# CatBoost 最適化\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[4/4] CatBoost 最適化\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 12),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_catboost = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_catboost.optimize(objective_catboost, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE (CV): {study_catboost.best_value:.2f}\")\n",
    "print(f\"Best params: {study_catboost.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 最適化されたパラメータの保存\n",
    "# ==================================================================================\n",
    "\n",
    "OPTIMIZED_PARAMS_EXP24 = {\n",
    "    'Ridge': study_ridge.best_params,\n",
    "    'ExtraTrees': study_extra.best_params,\n",
    "    'HistGradientBoosting': study_hist.best_params,\n",
    "    'CatBoost': study_catboost.best_params,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp24 最適化パラメータ\")\n",
    "print(\"=\" * 80)\n",
    "for model_name, params in OPTIMIZED_PARAMS_EXP24.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# パラメータをCSVに保存\n",
    "params_list = []\n",
    "for model_name, params in OPTIMIZED_PARAMS_EXP24.items():\n",
    "    for key, value in params.items():\n",
    "        params_list.append({'model': model_name, 'param': key, 'value': value})\n",
    "\n",
    "params_df = pd.DataFrame(params_list)\n",
    "params_df.to_csv(f'{output_dir}/optimized_params.csv', index=False)\n",
    "print(f\"\\nパラメータを保存しました: {output_dir}/optimized_params.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 最適化パラメータでの最終評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 最適化パラメータでの最終評価\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"最適化パラメータでの最終評価（Holdout Test）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_results = []\n",
    "final_predictions = {}\n",
    "final_models = {}\n",
    "\n",
    "# 1. Ridge\n",
    "print(\"\\n[1/4] Ridge...\")\n",
    "ridge_opt = Ridge(**OPTIMIZED_PARAMS_EXP24['Ridge'], random_state=42)\n",
    "ridge_opt.fit(X_train, y_train)\n",
    "ridge_pred_opt = ridge_opt.predict(X_test)\n",
    "ridge_metrics_opt = evaluate_model(y_test, ridge_pred_opt)\n",
    "print(f\"  MAE: {ridge_metrics_opt['MAE']:.2f}, R2: {ridge_metrics_opt['R2']:.4f}\")\n",
    "final_predictions['Ridge'] = ridge_pred_opt\n",
    "final_models['Ridge'] = ridge_opt\n",
    "final_results.append({'model': 'Ridge', **ridge_metrics_opt})\n",
    "\n",
    "# 2. ExtraTrees\n",
    "print(\"\\n[2/4] ExtraTrees...\")\n",
    "extra_opt = ExtraTreesRegressor(**OPTIMIZED_PARAMS_EXP24['ExtraTrees'], random_state=42, n_jobs=-1)\n",
    "extra_opt.fit(X_train, y_train)\n",
    "extra_pred_opt = extra_opt.predict(X_test)\n",
    "extra_metrics_opt = evaluate_model(y_test, extra_pred_opt)\n",
    "print(f\"  MAE: {extra_metrics_opt['MAE']:.2f}, R2: {extra_metrics_opt['R2']:.4f}\")\n",
    "final_predictions['ExtraTrees'] = extra_pred_opt\n",
    "final_models['ExtraTrees'] = extra_opt\n",
    "final_results.append({'model': 'ExtraTrees', **extra_metrics_opt})\n",
    "\n",
    "# 3. HistGradientBoosting\n",
    "print(\"\\n[3/4] HistGradientBoosting...\")\n",
    "hist_opt = HistGradientBoostingRegressor(**OPTIMIZED_PARAMS_EXP24['HistGradientBoosting'], random_state=42)\n",
    "hist_opt.fit(X_train, y_train)\n",
    "hist_pred_opt = hist_opt.predict(X_test)\n",
    "hist_metrics_opt = evaluate_model(y_test, hist_pred_opt)\n",
    "print(f\"  MAE: {hist_metrics_opt['MAE']:.2f}, R2: {hist_metrics_opt['R2']:.4f}\")\n",
    "final_predictions['HistGradientBoosting'] = hist_pred_opt\n",
    "final_models['HistGradientBoosting'] = hist_opt\n",
    "final_results.append({'model': 'HistGradientBoosting', **hist_metrics_opt})\n",
    "\n",
    "# 4. CatBoost\n",
    "print(\"\\n[4/4] CatBoost...\")\n",
    "catboost_opt = CatBoostRegressor(**OPTIMIZED_PARAMS_EXP24['CatBoost'], random_state=42, verbose=0)\n",
    "catboost_opt.fit(X_train, y_train)\n",
    "catboost_pred_opt = catboost_opt.predict(X_test)\n",
    "catboost_metrics_opt = evaluate_model(y_test, catboost_pred_opt)\n",
    "print(f\"  MAE: {catboost_metrics_opt['MAE']:.2f}, R2: {catboost_metrics_opt['R2']:.4f}\")\n",
    "final_predictions['CatBoost'] = catboost_pred_opt\n",
    "final_models['CatBoost'] = catboost_opt\n",
    "final_results.append({'model': 'CatBoost', **catboost_metrics_opt})\n",
    "\n",
    "final_df = pd.DataFrame(final_results).sort_values('MAE')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"最適化パラメータでの結果\")\n",
    "print(\"=\" * 80)\n",
    "print(final_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Weighted Ensemble\n",
    "# ==================================================================================\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Weighted Ensemble（Testセットで重み最適化）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def optimize_weights(predictions_dict, y_true, model_names):\n",
    "    preds_matrix = np.column_stack([predictions_dict[name] for name in model_names])\n",
    "    \n",
    "    def objective(weights):\n",
    "        ensemble_pred = preds_matrix @ weights\n",
    "        return mean_absolute_error(y_true, ensemble_pred)\n",
    "    \n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
    "    bounds = [(0, 1) for _ in range(len(model_names))]\n",
    "    initial_weights = np.ones(len(model_names)) / len(model_names)\n",
    "    \n",
    "    result = minimize(objective, initial_weights, method='SLSQP',\n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "ensemble_models = ['Ridge', 'CatBoost', 'ExtraTrees', 'HistGradientBoosting']\n",
    "weights = optimize_weights(final_predictions, y_test, ensemble_models)\n",
    "\n",
    "print(\"\\n最適化された重み:\")\n",
    "for name, weight in zip(ensemble_models, weights):\n",
    "    print(f\"  {name}: {weight:.4f}\")\n",
    "\n",
    "ensemble_pred = np.column_stack([final_predictions[name] for name in ensemble_models]) @ weights\n",
    "ensemble_metrics = evaluate_model(y_test, ensemble_pred)\n",
    "print(f\"\\nWeightedEnsemble MAE: {ensemble_metrics['MAE']:.2f}, R2: {ensemble_metrics['R2']:.4f}\")\n",
    "\n",
    "final_results.append({'model': 'WeightedEnsemble', **ensemble_metrics})\n",
    "final_predictions['WeightedEnsemble'] = ensemble_pred\n",
    "final_df = pd.DataFrame(final_results).sort_values('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 予測結果の可視化\n",
    "# ==================================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. 予測 vs 実績（散布図）\n",
    "ax1 = axes[0, 0]\n",
    "best_model = final_df.iloc[0]['model']\n",
    "best_pred = final_predictions[best_model]\n",
    "ax1.scatter(y_test, best_pred, alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "min_val = min(y_test.min(), best_pred.min())\n",
    "max_val = max(y_test.max(), best_pred.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
    "ax1.set_xlabel('Actual', fontsize=11)\n",
    "ax1.set_ylabel('Predicted', fontsize=11)\n",
    "ax1.set_title(f'{best_model}: Predicted vs Actual', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. 残差分布\n",
    "ax2 = axes[0, 1]\n",
    "residuals = y_test.values - best_pred\n",
    "ax2.hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.axvline(x=residuals.mean(), color='orange', linestyle='-', linewidth=2, label=f'Mean: {residuals.mean():.2f}')\n",
    "ax2.set_xlabel('Residual (Actual - Predicted)', fontsize=11)\n",
    "ax2.set_ylabel('Frequency', fontsize=11)\n",
    "ax2.set_title(f'{best_model}: Residual Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. 時系列での予測推移\n",
    "ax3 = axes[1, 0]\n",
    "test_dates = test_df['cdr_date'].values\n",
    "ax3.plot(test_dates, y_test.values, 'b-', label='Actual', linewidth=1.5, alpha=0.8)\n",
    "ax3.plot(test_dates, best_pred, 'r--', label=f'{best_model} Predicted', linewidth=1.5, alpha=0.8)\n",
    "ax3.set_xlabel('Date', fontsize=11)\n",
    "ax3.set_ylabel('acc_get_cnt', fontsize=11)\n",
    "ax3.set_title('Time Series: Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. モデル別MAE比較\n",
    "ax4 = axes[1, 1]\n",
    "models = final_df['model'].tolist()\n",
    "maes = final_df['MAE'].tolist()\n",
    "colors = ['coral' if m == best_model else 'steelblue' for m in models]\n",
    "bars = ax4.barh(models, maes, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax4.set_xlabel('MAE', fontsize=11)\n",
    "ax4.set_title('Model Comparison: MAE', fontsize=12, fontweight='bold')\n",
    "ax4.grid(alpha=0.3, axis='x')\n",
    "for bar, mae in zip(bars, maes):\n",
    "    ax4.text(mae + 0.5, bar.get_y() + bar.get_height()/2, f'{mae:.2f}', \n",
    "             va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/prediction_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n保存しました: {output_dir}/prediction_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 特徴量重要度分析\n",
    "# ==================================================================================\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"特徴量重要度分析\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 各モデルの特徴量重要度\n",
    "importance_data = {}\n",
    "\n",
    "# ExtraTrees\n",
    "importance_data['ExtraTrees'] = extra_opt.feature_importances_\n",
    "\n",
    "# CatBoost\n",
    "importance_data['CatBoost'] = catboost_opt.feature_importances_\n",
    "\n",
    "# HistGradientBoosting (permutation importance)\n",
    "print(\"\\nHistGradientBoostingのPermutation Importance計算中...\")\n",
    "perm_importance = permutation_importance(hist_opt, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "importance_data['HistGradientBoosting'] = perm_importance.importances_mean\n",
    "\n",
    "# DataFrameにまとめる\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'ExtraTrees': importance_data['ExtraTrees'],\n",
    "    'HistGradientBoosting': importance_data['HistGradientBoosting'],\n",
    "    'CatBoost': importance_data['CatBoost']\n",
    "})\n",
    "\n",
    "# 各モデルの重要度を表示\n",
    "for model_name in ['ExtraTrees', 'HistGradientBoosting', 'CatBoost']:\n",
    "    print(f\"\\n【{model_name}】Top 10\")\n",
    "    sorted_df = importance_df[['feature', model_name]].sort_values(model_name, ascending=False).head(10)\n",
    "    print(sorted_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 特徴量重要度の可視化\n",
    "# ==================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, model_name in enumerate(['ExtraTrees', 'HistGradientBoosting', 'CatBoost']):\n",
    "    ax = axes[idx]\n",
    "    sorted_df = importance_df[['feature', model_name]].sort_values(model_name, ascending=True)\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(sorted_df)))\n",
    "    ax.barh(sorted_df['feature'], sorted_df[model_name], color=colors)\n",
    "    ax.set_xlabel('Importance', fontsize=11)\n",
    "    ax.set_title(f'{model_name}', fontsize=13, fontweight='bold')\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Feature Importance (exp24: acc_get_cnt Prediction)', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "importance_df.to_csv(f'{output_dir}/feature_importance.csv', index=False)\n",
    "print(f\"\\n保存しました: {output_dir}/feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 最終結果の保存\n",
    "# ==================================================================================\n",
    "\n",
    "final_df.to_csv(f'{output_dir}/final_results.csv', index=False)\n",
    "\n",
    "# 予測結果も保存\n",
    "prediction_df = test_df[['cdr_date', 'acc_get_cnt', 'target_next_day']].copy()\n",
    "for model_name, pred in final_predictions.items():\n",
    "    prediction_df[f'pred_{model_name}'] = pred\n",
    "prediction_df.to_csv(f'{output_dir}/predictions.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp24 最終結果\")\n",
    "print(\"=\" * 80)\n",
    "print(final_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n保存しました:\")\n",
    "print(f\"  {output_dir}/final_results.csv\")\n",
    "print(f\"  {output_dir}/predictions.csv\")\n",
    "print(f\"  {output_dir}/optimized_params.csv\")\n",
    "print(f\"  {output_dir}/feature_importance.csv\")\n",
    "print(f\"  {output_dir}/prediction_results.png\")\n",
    "print(f\"  {output_dir}/feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**exp24: アカウント獲得数予測 + Optuna最適化（Holdout Validation）**\n",
    "\n",
    "### 目的変数\n",
    "- `target_next_day` = 翌日のアカウント獲得数（`acc_get_cnt.shift(-1)`）\n",
    "\n",
    "### 特徴量設計\n",
    "| カテゴリ | 特徴量 |\n",
    "|---------|--------|\n",
    "| 時系列基本 | dow, day_of_month, year, day_of_year, week_of_year, is_month_start, is_month_end |\n",
    "| カレンダー | day_before_holiday_flag |\n",
    "| 外部データ | cm_flg, search_cnt |\n",
    "| 入電数関連 | call_num, call_lag_1, call_lag_7, call_ma_7, call_ma_14 |\n",
    "| アカ獲ラグ | acc_lag_1, acc_lag_2, acc_lag_3, acc_lag_5, acc_lag_7, acc_lag_14, acc_lag_30 |\n",
    "| アカ獲移動平均 | acc_ma_3, acc_ma_7, acc_ma_14, acc_ma_30, acc_ma_std_3, acc_ma_std_7, acc_ma_std_14, acc_ma_std_30 |\n",
    "| 集約 | cm_7d, cm_14d, gt_ma_7, gt_ma_14, dow_avg |\n",
    "| レジーム変化 | days_to_2019_10_01, is_post_2019_10_01, is_post_2019_09_30, is_rush_period, is_adaptation_period |\n",
    "\n",
    "### 実験内容\n",
    "1. **Optuna最適化**: 各モデル100トライアル、TimeSeriesCV(n_splits=3)\n",
    "2. **最終評価**: Holdout Validation（2020-01-30以降をTest）\n",
    "3. **アンサンブル**: 重み最適化によるWeightedEnsemble\n",
    "\n",
    "### 出力ファイル\n",
    "1. `optimized_params.csv` - 最適化パラメータ\n",
    "2. `final_results.csv` - 最終結果\n",
    "3. `predictions.csv` - 予測結果\n",
    "4. `feature_importance.csv` - 特徴量重要度\n",
    "5. `prediction_results.png` - 予測結果の可視化\n",
    "6. `feature_importance.png` - 特徴量重要度の可視化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
