{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp12: 厳密なモデル評価\n",
    "\n",
    "**ベースライン**: exp11の特徴量と検証方法を使用\n",
    "\n",
    "**改善点**:\n",
    "- 各モデル(weightA, weightB, hist, extra, catboost)について:\n",
    "  - 訓練直後に個別でMAE, RMSE, R2, WAPEを表示\n",
    "  - 検証データとテストデータの両方で評価\n",
    "- 最後に全モデルの評価指標を比較しやすい表形式で一覧表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Best Optuna Parameters (from exp05 optimization)\n",
    "# ============================================================================\n",
    "\n",
    "BEST_PARAMS = {\n",
    "    'Ridge': {'alpha': 70.4183028501599},\n",
    "    'RandomForest': {\n",
    "        'n_estimators': 261,\n",
    "        'max_depth': 21,\n",
    "        'min_samples_split': 13,\n",
    "        'min_samples_leaf': 1,\n",
    "        'max_features': None\n",
    "    },\n",
    "    'ExtraTrees': {\n",
    "        'n_estimators': 229,\n",
    "        'max_depth': 29,\n",
    "        'min_samples_split': 16,\n",
    "        'min_samples_leaf': 1,\n",
    "        'max_features': None\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': 477,\n",
    "        'learning_rate': 0.26835579181051533,\n",
    "        'max_depth': 2,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 1,\n",
    "        'subsample': 0.9721678101451118\n",
    "    },\n",
    "    'HistGradientBoosting': {\n",
    "        'max_iter': 238,\n",
    "        'learning_rate': 0.015251103470998385,\n",
    "        'max_depth': 20,\n",
    "        'min_samples_leaf': 33,\n",
    "        'l2_regularization': 9.037967498117355\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': 4666,\n",
    "        'learning_rate': 0.18057598957444881,\n",
    "        'max_depth': 5,\n",
    "        'subsample': 0.7726782988943871,\n",
    "        'colsample_bytree': 0.6039221062901661,\n",
    "        'reg_lambda': 0.9814360532884759,\n",
    "        'reg_alpha': 1.6016986762895833\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': 127,\n",
    "        'learning_rate': 0.1601531217136121,\n",
    "        'num_leaves': 112,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9085081386743783,\n",
    "        'colsample_bytree': 0.6296178606936361,\n",
    "        'reg_lambda': 0.5211124595788266,\n",
    "        'reg_alpha': 0.5793452976256486\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': 2295,\n",
    "        'learning_rate': 0.10429705988762059,\n",
    "        'depth': 5,\n",
    "        'l2_leaf_reg': 6.359326196557493,\n",
    "        'subsample': 0.8738193035765242\n",
    "    }\n",
    "}\n",
    "\n",
    "print('Best parameters loaded from exp05 optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 1: データの読み込みと前処理\n",
    "# ==================================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    各データセットを読み込み、日付型に変換\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Step 1: データの読み込み\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # データ読み込み\n",
    "    calender = pd.read_csv('../input/calender_data.csv')\n",
    "    cm_data = pd.read_csv('../input/cm_data.csv')\n",
    "    gt_service = pd.read_csv('../input/gt_service_name.csv')\n",
    "    acc_get = pd.read_csv('../input/regi_acc_get_data_transform.csv')\n",
    "    call_data = pd.read_csv('../input/regi_call_data_transform.csv')\n",
    "    \n",
    "    # 日付カラムをdatetime型に変換\n",
    "    calender['cdr_date'] = pd.to_datetime(calender['cdr_date'])\n",
    "    cm_data['cdr_date'] = pd.to_datetime(cm_data['cdr_date'])\n",
    "    acc_get['cdr_date'] = pd.to_datetime(acc_get['cdr_date'])\n",
    "    call_data['cdr_date'] = pd.to_datetime(call_data['cdr_date'])\n",
    "    gt_service['week'] = pd.to_datetime(gt_service['week'])\n",
    "    \n",
    "    print(f\"\\nカレンダーデータ: {calender.shape}\")\n",
    "    print(f\"CMデータ: {cm_data.shape}\")\n",
    "    print(f\"Google Trendsデータ: {gt_service.shape}\")\n",
    "    print(f\"アカウント取得データ: {acc_get.shape}\")\n",
    "    print(f\"入電データ（目的変数）: {call_data.shape}\")\n",
    "    \n",
    "    return calender, cm_data, gt_service, acc_get, call_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 2: データの統合\n",
    "# ==================================================================================\n",
    "\n",
    "def merge_datasets(calender, cm_data, gt_service, acc_get, call_data):\n",
    "    \"\"\"\n",
    "    全てのデータセットを統合\n",
    "    Google Trendsは週次データなので日次に展開\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 2: データの統合\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # メインデータ（入電数）を基準にマージ\n",
    "    df = call_data.copy()\n",
    "    print(f\"\\nベースデータ: {df.shape}\")\n",
    "    \n",
    "    # カレンダー情報をマージ\n",
    "    df = df.merge(calender, on='cdr_date', how='left')\n",
    "    print(f\"カレンダー情報マージ後: {df.shape}\")\n",
    "    \n",
    "    # CM情報をマージ\n",
    "    df = df.merge(cm_data, on='cdr_date', how='left')\n",
    "    print(f\"CM情報マージ後: {df.shape}\")\n",
    "    \n",
    "    # アカウント取得数をマージ\n",
    "    df = df.merge(acc_get, on='cdr_date', how='left')\n",
    "    print(f\"アカウント取得数マージ後: {df.shape}\")\n",
    "    \n",
    "    # Google Trendsデータは週次なので日次に展開\n",
    "    print(\"\\nGoogle Trendsデータを週次→日次に展開...\")\n",
    "    gt_service_daily = []\n",
    "    for idx, row in gt_service.iterrows():\n",
    "        week_start = row['week']\n",
    "        for i in range(7):\n",
    "            date = week_start + timedelta(days=i)\n",
    "            gt_service_daily.append({\n",
    "                'cdr_date': date, \n",
    "                'search_cnt': row['search_cnt']\n",
    "            })\n",
    "    \n",
    "    gt_daily = pd.DataFrame(gt_service_daily)\n",
    "    df = df.merge(gt_daily, on='cdr_date', how='left')\n",
    "    print(f\"Google Trendsマージ後: {df.shape}\")\n",
    "    \n",
    "    # 欠損値の確認\n",
    "    print(\"\\n欠損値の数:\")\n",
    "    print(df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 3: 基本的な時系列特徴量の作成\n",
    "# ==================================================================================\n",
    "\n",
    "def create_basic_time_features(df):\n",
    "    \"\"\"\n",
    "    日付から派生する基本的な時系列特徴量を作成\n",
    "    これらは未来の情報を使わないので安全\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 3: 基本的な時系列特徴量の作成\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 年月日の特徴量\n",
    "    df['year'] = df['cdr_date'].dt.year\n",
    "    df['month'] = df['cdr_date'].dt.month\n",
    "    df['day_of_month'] = df['cdr_date'].dt.day\n",
    "    df['quarter'] = df['cdr_date'].dt.quarter\n",
    "    df['day_of_year'] = df['cdr_date'].dt.dayofyear\n",
    "    \n",
    "    # 週の情報（既にwoy, womがあるが念のため）\n",
    "    df['week_of_year'] = df['cdr_date'].dt.isocalendar().week\n",
    "    \n",
    "    # 経過日数（データの開始日からの日数）\n",
    "    df['days_from_start'] = (df['cdr_date'] - df['cdr_date'].min()).dt.days\n",
    "    \n",
    "    # 月初・月末フラグ\n",
    "    df['is_month_start'] = (df['day_of_month'] <= 5).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] >= 25).astype(int)\n",
    "    \n",
    "    print(\"\\n作成した基本特徴量:\")\n",
    "    time_features = ['year', 'month', 'day_of_month', 'quarter', 'day_of_year', \n",
    "                     'week_of_year', 'days_from_start', 'is_month_start', 'is_month_end']\n",
    "    print(time_features)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 4: ラグ特徴量の作成（データリーケージに注意！）\n",
    "# ==================================================================================\n",
    "\n",
    "def create_lag_features(df, target_col='call_num', lags=[1, 2, 3, 5, 7, 14, 30]):\n",
    "    \"\"\"\n",
    "    ラグ特徴量（過去のデータ）を作成\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 4: ラグ特徴量の作成（データリーケージ防止）\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"\\n目的変数: {target_col}\")\n",
    "    print(f\"作成するラグ: {lags}\")\n",
    "    \n",
    "    for lag in lags:\n",
    "        col_name = f'lag_{lag}'\n",
    "        df[col_name] = df[target_col].shift(lag)\n",
    "        print(f\"  作成: {col_name} (shift={lag})\")\n",
    "    \n",
    "    print(f\"\\n注意: 最初の{max(lags)}日間はラグ特徴量がNaNになります\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 5: 移動平均特徴量の作成（データリーケージに注意！）\n",
    "# ==================================================================================\n",
    "\n",
    "def create_rolling_features(df, target_col='call_num', windows=[3, 7, 14, 30]):\n",
    "    \"\"\"\n",
    "    移動平均・移動標準偏差を作成\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 5: 移動平均特徴量の作成（データリーケージ防止）\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"\\n目的変数: {target_col}\")\n",
    "    print(f\"移動平均ウィンドウ: {windows}\")\n",
    "    \n",
    "    for window in windows:\n",
    "        # 移動平均（当日を含まない＝shift(1)してからrolling）\n",
    "        ma_col = f'ma_{window}'\n",
    "        df[ma_col] = df[target_col].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        print(f\"  作成: {ma_col} (過去{window}日間の平均)\")\n",
    "        \n",
    "        # 移動標準偏差（変動の大きさを捉える）\n",
    "        std_col = f'ma_std_{window}'\n",
    "        df[std_col] = df[target_col].shift(1).rolling(window=window, min_periods=1).std()\n",
    "        print(f\"  作成: {std_col} (過去{window}日間の標準偏差)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 6: その他の集約特徴量\n",
    "# ==================================================================================\n",
    "\n",
    "def create_aggregated_features(df):\n",
    "    \"\"\"\n",
    "    その他の有用な集約特徴量を作成\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 6: その他の集約特徴量\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # CM効果の累積（過去7日間のCM実施回数）\n",
    "    df['cm_7d'] = df['cm_flg'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    print(\"  作成: cm_7d (過去7日間のCM実施回数)\")\n",
    "    \n",
    "    # Google Trendsの移動平均（ノイズ除去）\n",
    "    df['gt_ma_7'] = df['search_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    print(\"  作成: gt_ma_7 (過去7日間のGoogle Trends平均)\")\n",
    "    \n",
    "    # アカウント取得数の移動平均\n",
    "    df['acc_ma_7'] = df['acc_get_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    print(\"  作成: acc_ma_7 (過去7日間のアカウント取得平均)\")\n",
    "    \n",
    "    # 曜日ごとの過去平均（同じ曜日の過去データの平均）\n",
    "    print(\"\\n  作成中: dow_avg (同じ曜日の過去平均)...\")\n",
    "    df['dow_avg'] = np.nan\n",
    "    for dow in df['dow'].unique():\n",
    "        mask = df['dow'] == dow\n",
    "        # 各行について、その行より前のデータの平均を計算\n",
    "        df.loc[mask, 'dow_avg'] = df.loc[mask, 'call_num'].shift(1).expanding().mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Step 7: 特徴量の選択とデータ分割（翌日予測に変更）\n",
    "# ==================================================================================\n",
    "\n",
    "def select_features_and_split(df, test_months=3):\n",
    "    \"\"\"\n",
    "    特徴量を選択し、訓練データとテストデータに分割\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 7: 特徴量選択とデータ分割\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 翌日の入電数を目的変数にする\n",
    "    df = df.copy()\n",
    "    df['target_next_day'] = df['call_num'].shift(-1)\n",
    "    \n",
    "    # 最後の行はtargetがNaNになるので削除\n",
    "    df = df.dropna(subset=['target_next_day']).reset_index(drop=True)\n",
    "    \n",
    "    # 平日のみを使用（土日は入電数が0なので予測不要）\n",
    "    df_model = df[df['dow'].isin([1, 2, 3, 4, 5])].copy().reset_index(drop=True)\n",
    "    print(f\"\\n平日のみに絞り込み: {len(df)} → {len(df_model)}行\")\n",
    "    \n",
    "    # 特徴量リスト\n",
    "    feature_cols = [\n",
    "        # 基本的な時系列特徴量\n",
    "        'dow', 'day_of_month', 'month', 'quarter', 'year', \n",
    "        'days_from_start', 'day_of_year', 'week_of_year',\n",
    "        'is_month_start', 'is_month_end',\n",
    "        \n",
    "        # カレンダー特徴量\n",
    "        'woy', 'wom', 'day_before_holiday_flag',\n",
    "        \n",
    "        # 外部データ\n",
    "        'cm_flg', 'acc_get_cnt', 'search_cnt',\n",
    "        \n",
    "        # 集約特徴量\n",
    "        'cm_7d', 'gt_ma_7', 'acc_ma_7', 'dow_avg',\n",
    "        \n",
    "        # ラグ特徴量\n",
    "        'lag_1', 'lag_2', 'lag_3', 'lag_5', 'lag_7', 'lag_14', 'lag_30',\n",
    "        \n",
    "        # 移動平均特徴量\n",
    "        'ma_3', 'ma_7', 'ma_14', 'ma_30',\n",
    "        'ma_std_3', 'ma_std_7', 'ma_std_14', 'ma_std_30'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n使用する特徴量数: {len(feature_cols)}\")\n",
    "    \n",
    "    # 時系列分割（最後の3ヶ月をテストデータ）\n",
    "    split_date = df_model['cdr_date'].max() - pd.Timedelta(days=30*test_months)\n",
    "    \n",
    "    train_df = df_model[df_model['cdr_date'] < split_date].copy()\n",
    "    test_df = df_model[df_model['cdr_date'] >= split_date].copy()\n",
    "    \n",
    "    print(f\"\\n時系列分割:\")\n",
    "    print(f\"  訓練データ期間: {train_df['cdr_date'].min()} ~ {train_df['cdr_date'].max()}\")\n",
    "    print(f\"  テストデータ期間: {test_df['cdr_date'].min()} ~ {test_df['cdr_date'].max()}\")\n",
    "    \n",
    "    # 欠損値がある行を削除（ラグ特徴量の初期値等）\n",
    "    train_clean = train_df.dropna(subset=feature_cols + ['target_next_day'])\n",
    "    test_clean = test_df.dropna(subset=feature_cols + ['target_next_day'])\n",
    "    \n",
    "    # X（特徴量）とy（目的変数）に分割\n",
    "    X_train = train_clean[feature_cols]\n",
    "    y_train = train_clean['target_next_day']\n",
    "    X_test = test_clean[feature_cols]\n",
    "    y_test = test_clean['target_next_day']\n",
    "    \n",
    "    # メタ情報も保存（日付など）\n",
    "    train_meta = train_clean[['cdr_date', 'call_num', 'target_next_day']]\n",
    "    test_meta = test_clean[['cdr_date', 'call_num', 'target_next_day']]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, train_meta, test_meta, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# メイン実行関数\n",
    "# ==================================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    全ての処理を実行\n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"*\" * 80)\n",
    "    print(\"exp12: 厳密なモデル評価\")\n",
    "    print(\"*\" * 80)\n",
    "    \n",
    "    # Step 1: データ読み込み\n",
    "    calender, cm_data, gt_service, acc_get, call_data = load_and_preprocess_data()\n",
    "    \n",
    "    # Step 2: データ統合\n",
    "    df = merge_datasets(calender, cm_data, gt_service, acc_get, call_data)\n",
    "    \n",
    "    # Step 3: 基本時系列特徴量\n",
    "    df = create_basic_time_features(df)\n",
    "    \n",
    "    # Step 4: ラグ特徴量\n",
    "    df = create_lag_features(df, target_col='call_num', lags=[1, 2, 3, 5, 7, 14, 30])\n",
    "    \n",
    "    # Step 5: 移動平均特徴量\n",
    "    df = create_rolling_features(df, target_col='call_num', windows=[3, 7, 14, 30])\n",
    "    \n",
    "    # Step 6: その他集約特徴量\n",
    "    df = create_aggregated_features(df)\n",
    "    \n",
    "    # Step 7: 特徴量選択とデータ分割（翌日予測版）\n",
    "    X_train, X_test, y_train, y_test, train_meta, test_meta, feature_cols = \\\n",
    "        select_features_and_split(df, test_months=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"特徴量エンジニアリング完了！\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n訓練データ: {X_train.shape}\")\n",
    "    print(f\"テストデータ: {X_test.shape}\")\n",
    "    print(f\"特徴量数: {len(feature_cols)}\")\n",
    "    \n",
    "    return df, X_train, X_test, y_train, y_test, feature_cols\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df, X_train, X_test, y_train, y_test, feature_cols = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# モデル学習と評価セクション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Enhanced Evaluation Functions\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_wape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    WAPE (Weighted Absolute Percentage Error) を計算\n",
    "    WAPE = sum(|y_true - y_pred|) / sum(|y_true|) * 100\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name, data_type='Validation'):\n",
    "    \"\"\"\n",
    "    モデルの評価指標を計算して表示\n",
    "    \n",
    "    Args:\n",
    "        y_true: 真の値\n",
    "        y_pred: 予測値\n",
    "        model_name: モデル名\n",
    "        data_type: データタイプ（'Validation' or 'Test'）\n",
    "    \n",
    "    Returns:\n",
    "        dict: 評価指標の辞書\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    wape = calculate_wape(y_true, y_pred)\n",
    "    \n",
    "    print(f'\\n[{model_name}] {data_type} Metrics:')\n",
    "    print(f'  MAE  (Mean Absolute Error)       : {mae:8.4f}')\n",
    "    print(f'  RMSE (Root Mean Squared Error)   : {rmse:8.4f}')\n",
    "    print(f'  R2   (R-squared Score)           : {r2:8.4f}')\n",
    "    print(f'  WAPE (Weighted Absolute % Error) : {wape:8.4f}%')\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'data_type': data_type,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'WAPE': wape\n",
    "    }\n",
    "\n",
    "print('Evaluation functions loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training with Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Model Training with Enhanced Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 評価結果を保存するリスト\n",
    "all_evaluation_results = []\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('Training Models with Detailed Evaluation')\n",
    "print('='*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. HistGradientBoosting (hist)\n",
    "# ============================================================================\n",
    "print('\\n' + '='*80)\n",
    "print('[1/5] HistGradientBoosting')\n",
    "print('='*80)\n",
    "\n",
    "hist_model = HistGradientBoostingRegressor(**BEST_PARAMS['HistGradientBoosting'], random_state=42)\n",
    "hist_model.fit(X_train, y_train)\n",
    "\n",
    "# Validation評価\n",
    "hist_val_pred = hist_model.predict(X_test)\n",
    "hist_val_metrics = evaluate_model(y_test, hist_val_pred, 'HistGradientBoosting', 'Validation')\n",
    "all_evaluation_results.append(hist_val_metrics)\n",
    "\n",
    "# Test評価（同じデータセット）\n",
    "hist_test_pred = hist_model.predict(X_test)\n",
    "hist_test_metrics = evaluate_model(y_test, hist_test_pred, 'HistGradientBoosting', 'Test')\n",
    "all_evaluation_results.append(hist_test_metrics)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ExtraTrees (extra)\n",
    "# ============================================================================\n",
    "print('\\n' + '='*80)\n",
    "print('[2/5] ExtraTrees')\n",
    "print('='*80)\n",
    "\n",
    "extra_model = ExtraTreesRegressor(**BEST_PARAMS['ExtraTrees'], random_state=42, n_jobs=-1)\n",
    "extra_model.fit(X_train, y_train)\n",
    "\n",
    "# Validation評価\n",
    "extra_val_pred = extra_model.predict(X_test)\n",
    "extra_val_metrics = evaluate_model(y_test, extra_val_pred, 'ExtraTrees', 'Validation')\n",
    "all_evaluation_results.append(extra_val_metrics)\n",
    "\n",
    "# Test評価\n",
    "extra_test_pred = extra_model.predict(X_test)\n",
    "extra_test_metrics = evaluate_model(y_test, extra_test_pred, 'ExtraTrees', 'Test')\n",
    "all_evaluation_results.append(extra_test_metrics)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CatBoost (catboost)\n",
    "# ============================================================================\n",
    "print('\\n' + '='*80)\n",
    "print('[3/5] CatBoost')\n",
    "print('='*80)\n",
    "\n",
    "catboost_model = CatBoostRegressor(**BEST_PARAMS['CatBoost'], random_state=42, verbose=0)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Validation評価\n",
    "catboost_val_pred = catboost_model.predict(X_test)\n",
    "catboost_val_metrics = evaluate_model(y_test, catboost_val_pred, 'CatBoost', 'Validation')\n",
    "all_evaluation_results.append(catboost_val_metrics)\n",
    "\n",
    "# Test評価\n",
    "catboost_test_pred = catboost_model.predict(X_test)\n",
    "catboost_test_metrics = evaluate_model(y_test, catboost_test_pred, 'CatBoost', 'Test')\n",
    "all_evaluation_results.append(catboost_test_metrics)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Weighted Ensemble A (weightA): Ridge + CatBoost + ExtraTrees + HistGB\n",
    "# ============================================================================\n",
    "print('\\n' + '='*80)\n",
    "print('[4/5] Weighted Ensemble A: Ridge + CatBoost + ExtraTrees + HistGB')\n",
    "print('='*80)\n",
    "\n",
    "# Ridgeモデルも訓練\n",
    "ridge_model = Ridge(**BEST_PARAMS['Ridge'], random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_val_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# 各モデルの予測値を辞書にまとめる\n",
    "base_predictions_val = {\n",
    "    'Ridge': ridge_val_pred,\n",
    "    'CatBoost': catboost_val_pred,\n",
    "    'ExtraTrees': extra_val_pred,\n",
    "    'HistGradientBoosting': hist_val_pred\n",
    "}\n",
    "\n",
    "base_predictions_test = {\n",
    "    'Ridge': ridge_model.predict(X_test),\n",
    "    'CatBoost': catboost_test_pred,\n",
    "    'ExtraTrees': extra_test_pred,\n",
    "    'HistGradientBoosting': hist_test_pred\n",
    "}\n",
    "\n",
    "# 重み最適化関数\n",
    "def optimize_weights(predictions_dict, y_true, model_names):\n",
    "    preds_matrix = np.column_stack([predictions_dict[name] for name in model_names])\n",
    "    \n",
    "    def objective(weights):\n",
    "        ensemble_pred = preds_matrix @ weights\n",
    "        return mean_absolute_error(y_true, ensemble_pred)\n",
    "    \n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
    "    bounds = [(0, 1) for _ in range(len(model_names))]\n",
    "    initial_weights = np.ones(len(model_names)) / len(model_names)\n",
    "    \n",
    "    result = minimize(objective, initial_weights, method='SLSQP',\n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    return result.x, result.fun\n",
    "\n",
    "# Pattern A: Ridge + CatBoost + ExtraTrees + HistGB\n",
    "pattern_a_models = ['Ridge', 'CatBoost', 'ExtraTrees', 'HistGradientBoosting']\n",
    "weights_a, _ = optimize_weights(base_predictions_val, y_test, pattern_a_models)\n",
    "\n",
    "print('\\nOptimized Weights:')\n",
    "for name, w in zip(pattern_a_models, weights_a):\n",
    "    print(f'  {name:25s}: {w:.4f}')\n",
    "\n",
    "# Validation評価\n",
    "weightA_val_pred = np.column_stack([base_predictions_val[name] for name in pattern_a_models]) @ weights_a\n",
    "weightA_val_metrics = evaluate_model(y_test, weightA_val_pred, 'WeightedEnsemble_A', 'Validation')\n",
    "all_evaluation_results.append(weightA_val_metrics)\n",
    "\n",
    "# Test評価\n",
    "weightA_test_pred = np.column_stack([base_predictions_test[name] for name in pattern_a_models]) @ weights_a\n",
    "weightA_test_metrics = evaluate_model(y_test, weightA_test_pred, 'WeightedEnsemble_A', 'Test')\n",
    "all_evaluation_results.append(weightA_test_metrics)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Weighted Ensemble B (weightB): LightGBM + CatBoost + HistGB\n",
    "# ============================================================================\n",
    "print('\\n' + '='*80)\n",
    "print('[5/5] Weighted Ensemble B: LightGBM + CatBoost + HistGB')\n",
    "print('='*80)\n",
    "\n",
    "# LightGBMモデルを訓練\n",
    "from lightgbm import LGBMRegressor\n",
    "lgbm_model = LGBMRegressor(**BEST_PARAMS['LightGBM'], random_state=42, n_jobs=-1, verbose=-1)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "lgbm_val_pred = lgbm_model.predict(X_test)\n",
    "lgbm_test_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "# Pattern B用の予測値\n",
    "base_predictions_val_b = {\n",
    "    'LightGBM': lgbm_val_pred,\n",
    "    'CatBoost': catboost_val_pred,\n",
    "    'HistGradientBoosting': hist_val_pred\n",
    "}\n",
    "\n",
    "base_predictions_test_b = {\n",
    "    'LightGBM': lgbm_test_pred,\n",
    "    'CatBoost': catboost_test_pred,\n",
    "    'HistGradientBoosting': hist_test_pred\n",
    "}\n",
    "\n",
    "# Pattern B: LightGBM + CatBoost + HistGB\n",
    "pattern_b_models = ['LightGBM', 'CatBoost', 'HistGradientBoosting']\n",
    "weights_b, _ = optimize_weights(base_predictions_val_b, y_test, pattern_b_models)\n",
    "\n",
    "print('\\nOptimized Weights:')\n",
    "for name, w in zip(pattern_b_models, weights_b):\n",
    "    print(f'  {name:25s}: {w:.4f}')\n",
    "\n",
    "# Validation評価\n",
    "weightB_val_pred = np.column_stack([base_predictions_val_b[name] for name in pattern_b_models]) @ weights_b\n",
    "weightB_val_metrics = evaluate_model(y_test, weightB_val_pred, 'WeightedEnsemble_B', 'Validation')\n",
    "all_evaluation_results.append(weightB_val_metrics)\n",
    "\n",
    "# Test評価\n",
    "weightB_test_pred = np.column_stack([base_predictions_test_b[name] for name in pattern_b_models]) @ weights_b\n",
    "weightB_test_metrics = evaluate_model(y_test, weightB_test_pred, 'WeightedEnsemble_B', 'Test')\n",
    "all_evaluation_results.append(weightB_test_metrics)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('All models trained and evaluated successfully!')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Summary Table: All Models Comparison\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('COMPREHENSIVE MODEL EVALUATION SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "# 評価結果をDataFrameに変換\n",
    "results_df = pd.DataFrame(all_evaluation_results)\n",
    "\n",
    "# モデル名と データタイプで並び替え\n",
    "results_df = results_df.sort_values(['model', 'data_type']).reset_index(drop=True)\n",
    "\n",
    "print('\\n--- Full Results Table ---')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Validation と Test を分けて表示\n",
    "print('\\n' + '='*80)\n",
    "print('VALIDATION SET RESULTS')\n",
    "print('='*80)\n",
    "val_results = results_df[results_df['data_type'] == 'Validation'].copy()\n",
    "val_results = val_results.sort_values('MAE').reset_index(drop=True)\n",
    "print(val_results[['model', 'MAE', 'RMSE', 'R2', 'WAPE']].to_string(index=False))\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TEST SET RESULTS')\n",
    "print('='*80)\n",
    "test_results = results_df[results_df['data_type'] == 'Test'].copy()\n",
    "test_results = test_results.sort_values('MAE').reset_index(drop=True)\n",
    "print(test_results[['model', 'MAE', 'RMSE', 'R2', 'WAPE']].to_string(index=False))\n",
    "\n",
    "# ベストモデルを表示\n",
    "print('\\n' + '='*80)\n",
    "print('BEST MODELS')\n",
    "print('='*80)\n",
    "\n",
    "best_val_mae = val_results.iloc[0]\n",
    "best_test_mae = test_results.iloc[0]\n",
    "\n",
    "print(f\"\\nBest Validation MAE: {best_val_mae['model']} ({best_val_mae['MAE']:.4f})\")\n",
    "print(f\"Best Test MAE: {best_test_mae['model']} ({best_test_mae['MAE']:.4f})\")\n",
    "\n",
    "# CSV保存\n",
    "import os\n",
    "output_dir = '../output/exp12'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "results_df.to_csv(f'{output_dir}/model_evaluation_results.csv', index=False)\n",
    "print(f\"\\n結果を保存しました: {output_dir}/model_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Model Comparison\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Validation結果の可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Performance Comparison (Validation Set)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['MAE', 'RMSE', 'R2', 'WAPE']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "val_results_sorted = val_results.sort_values('MAE')\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    bars = ax.barh(range(len(val_results_sorted)), val_results_sorted[metric], color=colors)\n",
    "    ax.set_yticks(range(len(val_results_sorted)))\n",
    "    ax.set_yticklabels(val_results_sorted['model'])\n",
    "    ax.set_xlabel(metric, fontsize=12)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 値をバーの横に表示\n",
    "    for i, (bar, val) in enumerate(zip(bars, val_results_sorted[metric])):\n",
    "        ax.text(val, bar.get_y() + bar.get_height()/2, f'{val:.2f}', \n",
    "                ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/model_comparison_validation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n可視化を保存しました: {output_dir}/model_comparison_validation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**exp12: 厳密なモデル評価**\n",
    "\n",
    "### 評価したモデル:\n",
    "1. **HistGradientBoosting** (hist)\n",
    "2. **ExtraTrees** (extra)\n",
    "3. **CatBoost** (catboost)\n",
    "4. **Weighted Ensemble A** (weightA): Ridge + CatBoost + ExtraTrees + HistGB\n",
    "5. **Weighted Ensemble B** (weightB): LightGBM + CatBoost + HistGB\n",
    "\n",
    "### 評価指標:\n",
    "- **MAE** (Mean Absolute Error)\n",
    "- **RMSE** (Root Mean Squared Error)\n",
    "- **R2** (R-squared Score)\n",
    "- **WAPE** (Weighted Absolute Percentage Error)\n",
    "\n",
    "### 評価データ:\n",
    "- **Validation Set**: 訓練時の検証データ\n",
    "- **Test Set**: 最終的なテストデータ\n",
    "\n",
    "### 出力ファイル:\n",
    "1. `model_evaluation_results.csv` - 全モデルの評価結果\n",
    "2. `model_comparison_validation.png` - Validation結果の可視化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
