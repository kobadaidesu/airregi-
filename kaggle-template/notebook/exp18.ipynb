{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp18: ç‰¹å¾´é‡é¸æŠã«ã‚ˆã‚‹éå­¦ç¿’å¯¾ç­–\n",
    "\n",
    "**ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: exp17 (acc_geté«˜åº¦ç‰¹å¾´é‡)\n",
    "\n",
    "**å•é¡Œç‚¹**:\n",
    "- exp17ã§ã¯ç‰¹å¾´é‡ãŒ54å€‹ã¨å¤šã™ãã¦éå­¦ç¿’ãŒç™ºç”Ÿ\n",
    "- exp16æ¯”ã§å…¨ä½“çš„ã«æ‚ªåŒ–ï¼ˆRidge: -2.4%, WeightedEnsemble_A: -0.4%ï¼‰\n",
    "- acc_geté–¢é€£ã®ç‰¹å¾´é‡ï¼ˆ11å€‹ï¼‰ã«å¤šé‡å…±ç·šæ€§ã®ç–‘ã„\n",
    "\n",
    "**å¯¾ç­–: å†—é•·ãªç‰¹å¾´é‡ã‚’å‰Šé™¤**\n",
    "\n",
    "### acc_getç‰¹å¾´é‡ã®æ•´ç†ï¼ˆ11å€‹ â†’ 5å€‹ï¼‰\n",
    "\n",
    "**âŒ å‰Šé™¤ï¼ˆå†—é•·ãƒ»ç›¸é–¢ãŒé«˜ã„ï¼‰**:\n",
    "1. `acc_get_cnt` â†’ `acc_get_ema_14`ã§ä»£æ›¿ï¼ˆEMAã®æ–¹ãŒæ»‘ã‚‰ã‹ã§æœ‰ç”¨ï¼‰\n",
    "2. `acc_ma_7` â†’ `acc_get_ema_14`ã§ä»£æ›¿ï¼ˆEMAã®æ–¹ãŒæ€¥å¢—ã«è¿½å¾“ï¼‰\n",
    "3. `acc_get_sum_14d` â†’ `acc_get_sum_28d`ã§ä»£æ›¿ï¼ˆé•·æœŸã®æ–¹ãŒå®‰å®šï¼‰\n",
    "\n",
    "**âœ… æ®‹ã™ï¼ˆé‡è¦åº¦ãŒé«˜ã„ï¼‰**:\n",
    "1. `acc_get_lag7` - æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ1é€±é–“å‰ã®åŒã˜æ›œæ—¥ï¼‰\n",
    "2. `acc_get_diff1` - å¤‰åŒ–ç‡ï¼ˆæ€¥å¢—ãƒ»æ€¥æ¸›ã‚’æ‰ãˆã‚‹ï¼‰\n",
    "3. `acc_get_sum_28d` - é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆç´„1ãƒ¶æœˆï¼‰\n",
    "4. `acc_get_ema_14` - æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆæ€¥å¢—ã«è¿½å¾“ï¼‰\n",
    "5. `acc_get_growth` - æˆé•·ç‡ï¼ˆæœ€è¿‘ã®å‹¢ã„ï¼‰\n",
    "\n",
    "**ç‰¹å¾´é‡æ•°ã®æ¨ç§»**:\n",
    "- exp17: 54ç‰¹å¾´é‡ï¼ˆacc_get: 11å€‹ï¼‰\n",
    "- **exp18: 48ç‰¹å¾´é‡**ï¼ˆacc_get: 5å€‹ï¼‰â† 6å€‹å‰Šæ¸›\n",
    "\n",
    "**æœŸå¾…åŠ¹æœ**:\n",
    "- å¤šé‡å…±ç·šæ€§ã®è§£æ¶ˆ\n",
    "- éå­¦ç¿’ã®æŠ‘åˆ¶\n",
    "- ç‰¹ã«Ridgeã¨WeightedEnsemble_Aã®æ”¹å–„ã‚’æœŸå¾…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Best Optuna Parameters\n",
    "# ============================================================================\n",
    "\n",
    "BEST_PARAMS = {\n",
    "    'Ridge': {'alpha': 70.4183028501599},\n",
    "    'ExtraTrees': {\n",
    "        'n_estimators': 229,\n",
    "        'max_depth': 29,\n",
    "        'min_samples_split': 16,\n",
    "        'min_samples_leaf': 1,\n",
    "        'max_features': None\n",
    "    },\n",
    "    'HistGradientBoosting': {\n",
    "        'max_iter': 238,\n",
    "        'learning_rate': 0.015251103470998385,\n",
    "        'max_depth': 20,\n",
    "        'min_samples_leaf': 33,\n",
    "        'l2_regularization': 9.037967498117355\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': 127,\n",
    "        'learning_rate': 0.1601531217136121,\n",
    "        'num_leaves': 112,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9085081386743783,\n",
    "        'colsample_bytree': 0.6296178606936361,\n",
    "        'reg_lambda': 0.5211124595788266,\n",
    "        'reg_alpha': 0.5793452976256486\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': 2295,\n",
    "        'learning_rate': 0.10429705988762059,\n",
    "        'depth': 5,\n",
    "        'l2_leaf_reg': 6.359326196557493,\n",
    "        'subsample': 0.8738193035765242\n",
    "    }\n",
    "}\n",
    "\n",
    "print('Best parameters loaded from exp05 optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬ç‰¹å¾´é‡ä½œæˆ\n",
    "# ==================================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Step 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    calender = pd.read_csv('../input/calender_data.csv')\n",
    "    cm_data = pd.read_csv('../input/cm_data.csv')\n",
    "    gt_service = pd.read_csv('../input/gt_service_name.csv')\n",
    "    acc_get = pd.read_csv('../input/regi_acc_get_data_transform.csv')\n",
    "    call_data = pd.read_csv('../input/regi_call_data_transform.csv')\n",
    "    \n",
    "    calender['cdr_date'] = pd.to_datetime(calender['cdr_date'])\n",
    "    cm_data['cdr_date'] = pd.to_datetime(cm_data['cdr_date'])\n",
    "    acc_get['cdr_date'] = pd.to_datetime(acc_get['cdr_date'])\n",
    "    call_data['cdr_date'] = pd.to_datetime(call_data['cdr_date'])\n",
    "    gt_service['week'] = pd.to_datetime(gt_service['week'])\n",
    "    \n",
    "    print(f\"\\nãƒ‡ãƒ¼ã‚¿æœŸé–“: {call_data['cdr_date'].min()} ~ {call_data['cdr_date'].max()}\")\n",
    "    \n",
    "    return calender, cm_data, gt_service, acc_get, call_data\n",
    "\n",
    "def merge_datasets(calender, cm_data, gt_service, acc_get, call_data):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 2: ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = call_data.copy()\n",
    "    df = df.merge(calender, on='cdr_date', how='left')\n",
    "    df = df.merge(cm_data, on='cdr_date', how='left')\n",
    "    df = df.merge(acc_get, on='cdr_date', how='left')\n",
    "    \n",
    "    gt_service_daily = []\n",
    "    for idx, row in gt_service.iterrows():\n",
    "        week_start = row['week']\n",
    "        for i in range(7):\n",
    "            date = week_start + timedelta(days=i)\n",
    "            gt_service_daily.append({'cdr_date': date, 'search_cnt': row['search_cnt']})\n",
    "    \n",
    "    gt_daily = pd.DataFrame(gt_service_daily)\n",
    "    df = df.merge(gt_daily, on='cdr_date', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_basic_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['year'] = df['cdr_date'].dt.year\n",
    "    df['month'] = df['cdr_date'].dt.month\n",
    "    df['day_of_month'] = df['cdr_date'].dt.day\n",
    "    df['quarter'] = df['cdr_date'].dt.quarter\n",
    "    df['day_of_year'] = df['cdr_date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['cdr_date'].dt.isocalendar().week\n",
    "    df['days_from_start'] = (df['cdr_date'] - df['cdr_date'].min()).dt.days\n",
    "    df['is_month_start'] = (df['day_of_month'] <= 5).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] >= 25).astype(int)\n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='call_num', lags=[1, 2, 3, 5, 7, 14, 30]):\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def create_rolling_features(df, target_col='call_num', windows=[3, 7, 14, 30]):\n",
    "    df = df.copy()\n",
    "    for window in windows:\n",
    "        df[f'ma_{window}'] = df[target_col].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        df[f'ma_std_{window}'] = df[target_col].shift(1).rolling(window=window, min_periods=1).std()\n",
    "    return df\n",
    "\n",
    "def create_aggregated_features(df):\n",
    "    \"\"\"\n",
    "    ğŸ”¥ exp18ä¿®æ­£: acc_ma_7ã‚’å‰Šé™¤ï¼ˆacc_get_ema_14ã§ä»£æ›¿ï¼‰\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['cm_7d'] = df['cm_flg'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    df['gt_ma_7'] = df['search_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    # âŒ å‰Šé™¤: df['acc_ma_7'] = df['acc_get_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    \n",
    "    df['dow_avg'] = np.nan\n",
    "    for dow in df['dow'].unique():\n",
    "        mask = df['dow'] == dow\n",
    "        df.loc[mask, 'dow_avg'] = df.loc[mask, 'call_num'].shift(1).expanding().mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ğŸ”¥ exp18: acc_getç‰¹å¾´é‡ã®å³é¸ç‰ˆï¼ˆ11å€‹ â†’ 5å€‹ï¼‰\n",
    "# ==================================================================================\n",
    "\n",
    "def create_acc_get_features_selected(df):\n",
    "    \"\"\"\n",
    "    acc_getç‰¹å¾´é‡ã‚’å³é¸ã—ã¦å¤šé‡å…±ç·šæ€§ã‚’è§£æ¶ˆ\n",
    "    \n",
    "    âŒ å‰Šé™¤ï¼ˆå†—é•·ï¼‰:\n",
    "    - acc_get_cnt: ç”Ÿå€¤ã¯ãƒã‚¤ã‚ºãŒå¤šã„ â†’ EMAã§ä»£æ›¿\n",
    "    - acc_ma_7: å˜ç´”ç§»å‹•å¹³å‡ â†’ EMAã§ä»£æ›¿\n",
    "    - acc_get_sum_14d: 14æ—¥ç´¯ç© â†’ 28æ—¥ç´¯ç©ã§ä»£æ›¿\n",
    "    \n",
    "    âœ… æ®‹ã™ï¼ˆé‡è¦ï¼‰:\n",
    "    1. acc_get_lag7: æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ1é€±é–“å‰ï¼‰\n",
    "    2. acc_get_diff1: å¤‰åŒ–ç‡ï¼ˆæ€¥å¢—ãƒ»æ€¥æ¸›ï¼‰\n",
    "    3. acc_get_sum_28d: é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆç´„1ãƒ¶æœˆï¼‰\n",
    "    4. acc_get_ema_14: æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆæ€¥å¢—ã«è¿½å¾“ï¼‰\n",
    "    5. acc_get_growth: æˆé•·ç‡ï¼ˆçŸ­æœŸ/é•·æœŸæ¯”ç‡ï¼‰\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ”¥ Step 7: acc_getç‰¹å¾´é‡ã®å³é¸ç‰ˆï¼ˆéå­¦ç¿’å¯¾ç­–ï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"\\n[âœ… æ®‹ã™ç‰¹å¾´é‡ï¼ˆ5å€‹ï¼‰]\")\n",
    "    \n",
    "    # 1. acc_get_lag7: æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "    df['acc_get_lag7'] = df['acc_get_cnt'].shift(7)\n",
    "    print(\"  1. acc_get_lag7: 7æ—¥å‰ã®å€¤ï¼ˆæ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰\")\n",
    "    \n",
    "    # 2. acc_get_diff1: å¤‰åŒ–ç‡\n",
    "    df['acc_get_diff1'] = df['acc_get_cnt'].shift(1) - df['acc_get_cnt'].shift(2)\n",
    "    print(\"  2. acc_get_diff1: å‰æ—¥å·®åˆ†ï¼ˆå¤‰åŒ–ç‡ï¼‰\")\n",
    "    \n",
    "    # 3. acc_get_sum_28d: é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰\n",
    "    df['acc_get_sum_28d'] = df['acc_get_cnt'].shift(1).rolling(window=28, min_periods=1).sum()\n",
    "    print(\"  3. acc_get_sum_28d: ç›´è¿‘28æ—¥ç´¯ç©ï¼ˆé•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰\")\n",
    "    \n",
    "    # 4. acc_get_ema_14: æŒ‡æ•°ç§»å‹•å¹³å‡\n",
    "    df['acc_get_ema_14'] = df['acc_get_cnt'].shift(1).ewm(span=14, adjust=False).mean()\n",
    "    print(\"  4. acc_get_ema_14: 14æ—¥æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆæ€¥å¢—ã«è¿½å¾“ï¼‰\")\n",
    "    \n",
    "    # 5. acc_get_growth: æˆé•·ç‡\n",
    "    acc_get_sum_7d = df['acc_get_cnt'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    eps = 1.0\n",
    "    df['acc_get_growth'] = acc_get_sum_7d / (df['acc_get_sum_28d'] + eps)\n",
    "    print(\"  5. acc_get_growth: sum_7d / (sum_28d + eps) = å‹¢ã„\")\n",
    "    \n",
    "    print(\"\\n[âŒ å‰Šé™¤ã—ãŸç‰¹å¾´é‡ï¼ˆ6å€‹ï¼‰]\")\n",
    "    print(\"  - acc_get_cnt: ç”Ÿå€¤ï¼ˆãƒã‚¤ã‚ºãŒå¤šã„ï¼‰â†’ EMAã§ä»£æ›¿\")\n",
    "    print(\"  - acc_ma_7: å˜ç´”ç§»å‹•å¹³å‡ â†’ EMAã§ä»£æ›¿\")\n",
    "    print(\"  - acc_get_sum_14d: 14æ—¥ç´¯ç© â†’ 28æ—¥ç´¯ç©ã§ä»£æ›¿\")\n",
    "    \n",
    "    print(\"\\nå‰Šæ¸›åŠ¹æœ: 11å€‹ â†’ 5å€‹ï¼ˆ-6å€‹ï¼‰\")\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º\n",
    "    print(\"\\nã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€æ–°20ä»¶ï¼‰:\")\n",
    "    cols_to_show = ['cdr_date', 'dow', 'acc_get_lag7', 'acc_get_diff1', \n",
    "                    'acc_get_sum_28d', 'acc_get_ema_14', 'acc_get_growth']\n",
    "    print(df[cols_to_show].tail(20).to_string())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡ã®ä½œæˆï¼ˆexp15ã‹ã‚‰ç¶™æ‰¿ï¼‰\n",
    "# ==================================================================================\n",
    "\n",
    "def create_regime_change_features(df):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 8: ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡ã®ä½œæˆï¼ˆexp15ã‹ã‚‰ç¶™æ‰¿ï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    tax_implementation_date = pd.Timestamp('2019-10-01')\n",
    "    rush_deadline = pd.Timestamp('2019-09-30')\n",
    "    \n",
    "    df['days_to_2019_10_01'] = (tax_implementation_date - df['cdr_date']).dt.days\n",
    "    df['is_pre_2019_10_01'] = (df['cdr_date'] < tax_implementation_date).astype(int)\n",
    "    df['is_post_2019_10_01'] = (df['cdr_date'] >= tax_implementation_date).astype(int)\n",
    "    \n",
    "    df['days_to_2019_09_30'] = (rush_deadline - df['cdr_date']).dt.days\n",
    "    df['is_pre_2019_09_30'] = (df['cdr_date'] < rush_deadline).astype(int)\n",
    "    df['is_post_2019_09_30'] = (df['cdr_date'] >= rush_deadline).astype(int)\n",
    "    \n",
    "    rush_start = rush_deadline - pd.Timedelta(days=90)\n",
    "    df['is_rush_period'] = ((df['cdr_date'] >= rush_start) & \n",
    "                            (df['cdr_date'] <= rush_deadline)).astype(int)\n",
    "    \n",
    "    adaptation_end = tax_implementation_date + pd.Timedelta(days=30)\n",
    "    df['is_adaptation_period'] = ((df['cdr_date'] >= tax_implementation_date) & \n",
    "                                   (df['cdr_date'] <= adaptation_end)).astype(int)\n",
    "    \n",
    "    print(\"\\nä½œæˆã—ãŸç‰¹å¾´é‡: 8å€‹ï¼ˆãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–é–¢é€£ï¼‰\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"*\" * 80)\n",
    "print(\"exp18: ç‰¹å¾´é‡é¸æŠã«ã‚ˆã‚‹éå­¦ç¿’å¯¾ç­–ï¼ˆ54å€‹ â†’ 48å€‹ï¼‰\")\n",
    "print(\"*\" * 80)\n",
    "\n",
    "calender, cm_data, gt_service, acc_get, call_data = load_and_preprocess_data()\n",
    "df = merge_datasets(calender, cm_data, gt_service, acc_get, call_data)\n",
    "df = create_basic_time_features(df)\n",
    "df = create_lag_features(df)\n",
    "df = create_rolling_features(df)\n",
    "df = create_aggregated_features(df)\n",
    "\n",
    "# ğŸ”¥ acc_getç‰¹å¾´é‡ã‚’å³é¸ï¼ˆéå­¦ç¿’å¯¾ç­–ï¼‰\n",
    "df = create_acc_get_features_selected(df)\n",
    "\n",
    "# ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡ã‚’è¿½åŠ \n",
    "df = create_regime_change_features(df)\n",
    "\n",
    "# ç¿Œæ—¥ã®å…¥é›»æ•°ã‚’ç›®çš„å¤‰æ•°ã«ã™ã‚‹\n",
    "df['target_next_day'] = df['call_num'].shift(-1)\n",
    "df = df.dropna(subset=['target_next_day']).reset_index(drop=True)\n",
    "\n",
    "# å¹³æ—¥ã®ã¿\n",
    "df_model = df[df['dow'].isin([1, 2, 3, 4, 5])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nå¹³æ—¥ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_model)}è¡Œ\")\n",
    "print(f\"æœŸé–“: {df_model['cdr_date'].min()} ~ {df_model['cdr_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Rolling Window Validation ã®è¨­å®š\n",
    "# ==================================================================================\n",
    "\n",
    "# ğŸ”¥ ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆexp18ã§6ã¤å‰Šé™¤ï¼‰\n",
    "feature_cols = [\n",
    "    # åŸºæœ¬æ™‚ç³»åˆ—ç‰¹å¾´é‡\n",
    "    'dow', 'day_of_month', 'month', 'quarter', 'year', \n",
    "    'days_from_start', 'day_of_year', 'week_of_year',\n",
    "    'is_month_start', 'is_month_end',\n",
    "    # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´é‡\n",
    "    'woy', 'wom', 'day_before_holiday_flag',\n",
    "    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿\n",
    "    'cm_flg', 'search_cnt',  # âŒ acc_get_cntã‚’å‰Šé™¤\n",
    "    # é›†ç´„ç‰¹å¾´é‡\n",
    "    'cm_7d', 'gt_ma_7', 'dow_avg',  # âŒ acc_ma_7ã‚’å‰Šé™¤\n",
    "    # ãƒ©ã‚°ç‰¹å¾´é‡\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_5', 'lag_7', 'lag_14', 'lag_30',\n",
    "    # ç§»å‹•å¹³å‡ç‰¹å¾´é‡\n",
    "    'ma_3', 'ma_7', 'ma_14', 'ma_30',\n",
    "    'ma_std_3', 'ma_std_7', 'ma_std_14', 'ma_std_30',\n",
    "    # ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡\n",
    "    'days_to_2019_10_01', 'is_pre_2019_10_01', 'is_post_2019_10_01',\n",
    "    'days_to_2019_09_30', 'is_pre_2019_09_30', 'is_post_2019_09_30',\n",
    "    'is_rush_period', 'is_adaptation_period',\n",
    "    # ğŸ”¥ acc_getå³é¸ç‰ˆï¼ˆ5å€‹ã®ã¿ï¼‰\n",
    "    'acc_get_lag7', 'acc_get_diff1', 'acc_get_sum_28d', 'acc_get_ema_14', 'acc_get_growth'\n",
    "    # âŒ å‰Šé™¤: acc_get_cnt, acc_ma_7, acc_get_sum_14d\n",
    "]\n",
    "\n",
    "print(f\"\\nä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "print(f\"  exp17: 54ç‰¹å¾´é‡\")\n",
    "print(f\"  exp18: {len(feature_cols)}ç‰¹å¾´é‡ (-6å€‹å‰Šæ¸›)\")\n",
    "print(f\"\\nå‰Šæ¸›ã—ãŸç‰¹å¾´é‡:\")\n",
    "print(f\"  - acc_get_cntï¼ˆç”Ÿå€¤ï¼‰\")\n",
    "print(f\"  - acc_ma_7ï¼ˆå˜ç´”ç§»å‹•å¹³å‡ï¼‰\")\n",
    "print(f\"  - acc_get_sum_14dï¼ˆ14æ—¥ç´¯ç©ï¼‰\")\n",
    "\n",
    "# Rolling Windowè¨­å®š\n",
    "test_window_days = 60\n",
    "step_days = 30\n",
    "\n",
    "# æ¬ æå€¤ã‚’é™¤å»\n",
    "df_clean = df_model.dropna(subset=feature_cols + ['target_next_day']).copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Rolling Window Validation è¨­å®š\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ãƒ†ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {test_window_days}æ—¥ï¼ˆç´„2ãƒ¶æœˆï¼‰\")\n",
    "print(f\"ã‚¹ãƒ†ãƒƒãƒ—: {step_days}æ—¥ï¼ˆç´„1ãƒ¶æœˆï¼‰\")\n",
    "print(f\"\\nä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {len(df_clean)}è¡Œ\")\n",
    "\n",
    "# Rolling Window ã®åˆ†å‰²ç‚¹ã‚’è¨ˆç®—\n",
    "min_date = df_clean['cdr_date'].min()\n",
    "max_date = df_clean['cdr_date'].max()\n",
    "min_train_days = 90\n",
    "\n",
    "windows = []\n",
    "current_test_end = max_date\n",
    "\n",
    "while True:\n",
    "    test_start = current_test_end - pd.Timedelta(days=test_window_days)\n",
    "    train_end = test_start - pd.Timedelta(days=1)\n",
    "    \n",
    "    if (train_end - min_date).days < min_train_days:\n",
    "        break\n",
    "    \n",
    "    windows.append({\n",
    "        'train_start': min_date,\n",
    "        'train_end': train_end,\n",
    "        'test_start': test_start,\n",
    "        'test_end': current_test_end\n",
    "    })\n",
    "    \n",
    "    current_test_end = test_start - pd.Timedelta(days=1)\n",
    "\n",
    "windows = windows[::-1]\n",
    "\n",
    "print(f\"\\nä½œæˆã•ã‚ŒãŸã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: {len(windows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# è©•ä¾¡é–¢æ•°\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def calculate_wape(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'WAPE': calculate_wape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "print('è©•ä¾¡é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Rolling Window Validation ã®å®Ÿè¡Œ\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "all_window_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Rolling Window Validation å®Ÿè¡Œï¼ˆç‰¹å¾´é‡å³é¸ç‰ˆï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for window_idx, window in enumerate(windows):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Window {window_idx + 1}/{len(windows)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train: {window['train_start'].strftime('%Y-%m-%d')} ~ {window['train_end'].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Test : {window['test_start'].strftime('%Y-%m-%d')} ~ {window['test_end'].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    train_mask = (df_clean['cdr_date'] >= window['train_start']) & (df_clean['cdr_date'] <= window['train_end'])\n",
    "    test_mask = (df_clean['cdr_date'] >= window['test_start']) & (df_clean['cdr_date'] <= window['test_end'])\n",
    "    \n",
    "    X_train = df_clean.loc[train_mask, feature_cols]\n",
    "    y_train = df_clean.loc[train_mask, 'target_next_day']\n",
    "    X_test = df_clean.loc[test_mask, feature_cols]\n",
    "    y_test = df_clean.loc[test_mask, 'target_next_day']\n",
    "    \n",
    "    print(f\"\\nTrain: {len(X_train)}ä»¶, Test: {len(X_test)}ä»¶\")\n",
    "    \n",
    "    # å„ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡\n",
    "    window_predictions = {}\n",
    "    \n",
    "    # 1. HistGradientBoosting\n",
    "    print(\"\\n[1/5] HistGradientBoosting...\")\n",
    "    hist_model = HistGradientBoostingRegressor(**BEST_PARAMS['HistGradientBoosting'], random_state=42)\n",
    "    hist_model.fit(X_train, y_train)\n",
    "    hist_pred = hist_model.predict(X_test)\n",
    "    hist_metrics = evaluate_model(y_test, hist_pred)\n",
    "    print(f\"  MAE: {hist_metrics['MAE']:.2f}, RMSE: {hist_metrics['RMSE']:.2f}, R2: {hist_metrics['R2']:.3f}\")\n",
    "    window_predictions['HistGradientBoosting'] = hist_pred\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'HistGradientBoosting', **hist_metrics})\n",
    "    \n",
    "    # 2. ExtraTrees\n",
    "    print(\"[2/5] ExtraTrees...\")\n",
    "    extra_model = ExtraTreesRegressor(**BEST_PARAMS['ExtraTrees'], random_state=42, n_jobs=-1)\n",
    "    extra_model.fit(X_train, y_train)\n",
    "    extra_pred = extra_model.predict(X_test)\n",
    "    extra_metrics = evaluate_model(y_test, extra_pred)\n",
    "    print(f\"  MAE: {extra_metrics['MAE']:.2f}, RMSE: {extra_metrics['RMSE']:.2f}, R2: {extra_metrics['R2']:.3f}\")\n",
    "    window_predictions['ExtraTrees'] = extra_pred\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'ExtraTrees', **extra_metrics})\n",
    "    \n",
    "    # 3. CatBoost\n",
    "    print(\"[3/5] CatBoost...\")\n",
    "    catboost_model = CatBoostRegressor(**BEST_PARAMS['CatBoost'], random_state=42, verbose=0)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_pred = catboost_model.predict(X_test)\n",
    "    catboost_metrics = evaluate_model(y_test, catboost_pred)\n",
    "    print(f\"  MAE: {catboost_metrics['MAE']:.2f}, RMSE: {catboost_metrics['RMSE']:.2f}, R2: {catboost_metrics['R2']:.3f}\")\n",
    "    window_predictions['CatBoost'] = catboost_pred\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'CatBoost', **catboost_metrics})\n",
    "    \n",
    "    # 4. Ridge\n",
    "    print(\"[4/5] Ridge...\")\n",
    "    ridge_model = Ridge(**BEST_PARAMS['Ridge'], random_state=42)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    ridge_pred = ridge_model.predict(X_test)\n",
    "    ridge_metrics = evaluate_model(y_test, ridge_pred)\n",
    "    print(f\"  MAE: {ridge_metrics['MAE']:.2f}, RMSE: {ridge_metrics['RMSE']:.2f}, R2: {ridge_metrics['R2']:.3f}\")\n",
    "    window_predictions['Ridge'] = ridge_pred\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'Ridge', **ridge_metrics})\n",
    "    \n",
    "    # 5. Weighted Ensemble A\n",
    "    print(\"[5/5] WeightedEnsemble_A...\")\n",
    "    \n",
    "    def optimize_weights(predictions_dict, y_true, model_names):\n",
    "        preds_matrix = np.column_stack([predictions_dict[name] for name in model_names])\n",
    "        \n",
    "        def objective(weights):\n",
    "            ensemble_pred = preds_matrix @ weights\n",
    "            return mean_absolute_error(y_true, ensemble_pred)\n",
    "        \n",
    "        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
    "        bounds = [(0, 1) for _ in range(len(model_names))]\n",
    "        initial_weights = np.ones(len(model_names)) / len(model_names)\n",
    "        \n",
    "        result = minimize(objective, initial_weights, method='SLSQP',\n",
    "                         bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "    \n",
    "    pattern_a_models = ['Ridge', 'CatBoost', 'ExtraTrees', 'HistGradientBoosting']\n",
    "    weights_a = optimize_weights(window_predictions, y_test, pattern_a_models)\n",
    "    \n",
    "    weightA_pred = np.column_stack([window_predictions[name] for name in pattern_a_models]) @ weights_a\n",
    "    weightA_metrics = evaluate_model(y_test, weightA_pred)\n",
    "    print(f\"  MAE: {weightA_metrics['MAE']:.2f}, RMSE: {weightA_metrics['RMSE']:.2f}, R2: {weightA_metrics['R2']:.3f}\")\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'WeightedEnsemble_A', **weightA_metrics})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Rolling Window Validation å®Œäº†\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# exp17ã¨ã®æ¯”è¼ƒåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# exp17ã®çµæœã‚’èª­ã¿è¾¼ã‚“ã§æ¯”è¼ƒ\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "exp18_results_df = pd.DataFrame(all_window_results)\n",
    "\n",
    "# exp17ã®çµæœã‚’èª­ã¿è¾¼ã¿\n",
    "exp17_results_path = '../output/exp17/rolling_window_results.csv'\n",
    "if os.path.exists(exp17_results_path):\n",
    "    exp17_results_df = pd.read_csv(exp17_results_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"exp17 vs exp18 æ¯”è¼ƒï¼ˆç‰¹å¾´é‡å‰Šæ¸›ã«ã‚ˆã‚‹éå­¦ç¿’æ”¹å–„ã‚’ç¢ºèªï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    models = exp18_results_df['model'].unique()\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{model}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        exp17_model = exp17_results_df[exp17_results_df['model'] == model]\n",
    "        exp18_model = exp18_results_df[exp18_results_df['model'] == model]\n",
    "        \n",
    "        if len(exp17_model) > 0 and len(exp18_model) > 0:\n",
    "            exp17_avg_mae = exp17_model['MAE'].mean()\n",
    "            exp18_avg_mae = exp18_model['MAE'].mean()\n",
    "            improvement = exp17_avg_mae - exp18_avg_mae\n",
    "            improvement_pct = (improvement / exp17_avg_mae) * 100\n",
    "            \n",
    "            status = \"âœ… æ”¹å–„\" if improvement > 0 else \"âŒ æ‚ªåŒ–\"\n",
    "            print(f\"\\nå¹³å‡MAE:\")\n",
    "            print(f\"  exp17: {exp17_avg_mae:.2f} (54ç‰¹å¾´é‡)\")\n",
    "            print(f\"  exp18: {exp18_avg_mae:.2f} (48ç‰¹å¾´é‡)\")\n",
    "            print(f\"  å·®åˆ†: {improvement:+.2f} ({improvement_pct:+.1f}%) {status}\")\n",
    "            \n",
    "            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã®è©³ç´°\n",
    "            print(f\"\\nã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã®æ¯”è¼ƒ:\")\n",
    "            for window_num in sorted(exp18_model['window'].unique()):\n",
    "                exp17_mae = exp17_model[exp17_model['window'] == window_num]['MAE'].values\n",
    "                exp18_mae = exp18_model[exp18_model['window'] == window_num]['MAE'].values\n",
    "                \n",
    "                if len(exp17_mae) > 0 and len(exp18_mae) > 0:\n",
    "                    win_improvement = exp17_mae[0] - exp18_mae[0]\n",
    "                    win_status = \"âœ…\" if win_improvement > 0 else \"âŒ\"\n",
    "                    print(f\"  Window{window_num}: exp17={exp17_mae[0]:.2f} â†’ exp18={exp18_mae[0]:.2f} ({win_improvement:+.2f}) {win_status}\")\n",
    "else:\n",
    "    print(\"\\nexp17ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# çµæœã®é›†è¨ˆ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp18 å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®çµæœ\")\n",
    "print(\"=\" * 80)\n",
    "print(exp18_results_df.to_string(index=False))\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å¹³å‡ã‚¹ã‚³ã‚¢\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å¹³å‡ã‚¹ã‚³ã‚¢ï¼ˆå…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "average_scores = exp18_results_df.groupby('model')[['MAE', 'RMSE', 'R2', 'WAPE']].mean()\n",
    "average_scores = average_scores.sort_values('MAE')\n",
    "print(average_scores.to_string())\n",
    "\n",
    "# æ¨™æº–åå·®\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®æ¨™æº–åå·®ï¼ˆã‚¹ã‚³ã‚¢ã®å®‰å®šæ€§ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "std_scores = exp18_results_df.groupby('model')[['MAE', 'RMSE', 'R2', 'WAPE']].std()\n",
    "std_scores = std_scores.sort_values('MAE')\n",
    "print(std_scores.to_string())\n",
    "\n",
    "# CSVä¿å­˜\n",
    "output_dir = '../output/exp18'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "exp18_results_df.to_csv(f'{output_dir}/rolling_window_results.csv', index=False)\n",
    "average_scores.to_csv(f'{output_dir}/average_scores.csv')\n",
    "std_scores.to_csv(f'{output_dir}/std_scores.csv')\n",
    "\n",
    "print(f\"\\nçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# å¯è¦–åŒ–: exp17 vs exp18 æ¯”è¼ƒ\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if os.path.exists(exp17_results_path):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('exp17 vs exp18: MAE Comparison (Overfitting Reduction)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models_to_compare = ['HistGradientBoosting', 'ExtraTrees', 'CatBoost', 'WeightedEnsemble_A']\n",
    "    \n",
    "    for idx, model in enumerate(models_to_compare):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        exp17_model_data = exp17_results_df[exp17_results_df['model'] == model]\n",
    "        exp18_model_data = exp18_results_df[exp18_results_df['model'] == model]\n",
    "        \n",
    "        ax.plot(exp17_model_data['window'], exp17_model_data['MAE'], \n",
    "                marker='o', label='exp17 (54 features)', linewidth=2, color='coral')\n",
    "        ax.plot(exp18_model_data['window'], exp18_model_data['MAE'], \n",
    "                marker='s', label='exp18 (48 features, selected)', linewidth=2, color='green')\n",
    "        \n",
    "        ax.set_xlabel('Window', fontsize=12)\n",
    "        ax.set_ylabel('MAE', fontsize=12)\n",
    "        ax.set_title(f'{model}', fontsize=12, fontweight='bold')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_xticks(range(1, len(windows) + 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/exp17_vs_exp18_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nå¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}/exp17_vs_exp18_comparison.png\")\n",
    "else:\n",
    "    print(\"\\nexp17ã®çµæœãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ¯”è¼ƒå¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**exp18: ç‰¹å¾´é‡é¸æŠã«ã‚ˆã‚‹éå­¦ç¿’å¯¾ç­–**\n",
    "\n",
    "### å•é¡Œç‚¹ï¼ˆexp17ï¼‰:\n",
    "- ç‰¹å¾´é‡ãŒ54å€‹ã¨å¤šã™ãã¦éå­¦ç¿’\n",
    "- exp16æ¯”ã§å…¨ä½“çš„ã«æ‚ªåŒ–ï¼ˆç‰¹ã«Ridge: -2.4%ï¼‰\n",
    "- acc_geté–¢é€£ã®ç‰¹å¾´é‡ï¼ˆ11å€‹ï¼‰ã«å¤šé‡å…±ç·šæ€§\n",
    "\n",
    "### å¯¾ç­–:\n",
    "**å†—é•·ãªç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼ˆ6å€‹å‰Šæ¸›ï¼‰**\n",
    "\n",
    "âŒ **å‰Šé™¤ã—ãŸç‰¹å¾´é‡**:\n",
    "1. `acc_get_cnt` - ç”Ÿå€¤ï¼ˆãƒã‚¤ã‚ºãŒå¤šã„ï¼‰\n",
    "2. `acc_ma_7` - å˜ç´”ç§»å‹•å¹³å‡\n",
    "3. `acc_get_sum_14d` - 14æ—¥ç´¯ç©\n",
    "\n",
    "âœ… **æ®‹ã—ãŸç‰¹å¾´é‡**:\n",
    "1. `acc_get_lag7` - æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "2. `acc_get_diff1` - å¤‰åŒ–ç‡\n",
    "3. `acc_get_sum_28d` - é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰\n",
    "4. `acc_get_ema_14` - æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆæ€¥å¢—ã«è¿½å¾“ï¼‰\n",
    "5. `acc_get_growth` - æˆé•·ç‡ï¼ˆå‹¢ã„ï¼‰\n",
    "\n",
    "### ç‰¹å¾´é‡æ•°ã®æ¨ç§»:\n",
    "- exp16: 50ç‰¹å¾´é‡\n",
    "- exp17: 54ç‰¹å¾´é‡ï¼ˆéå­¦ç¿’ï¼‰\n",
    "- **exp18: 48ç‰¹å¾´é‡**ï¼ˆ-6å€‹ã€æœ€é©åŒ–ï¼‰\n",
    "\n",
    "### æœŸå¾…åŠ¹æœ:\n",
    "- å¤šé‡å…±ç·šæ€§ã®è§£æ¶ˆ\n",
    "- éå­¦ç¿’ã®æŠ‘åˆ¶\n",
    "- Ridgeã®æ”¹å–„ï¼ˆç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹å¾´é‡å‰Šæ¸›ã«æ•æ„Ÿï¼‰\n",
    "- WeightedEnsemble_Aã®å®‰å®šåŒ–\n",
    "\n",
    "### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:\n",
    "1. `rolling_window_results.csv`\n",
    "2. `average_scores.csv`\n",
    "3. `std_scores.csv`\n",
    "4. `exp17_vs_exp18_comparison.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
