import json
import copy

# exp05を読み込み
with open(r'c:\Users\PC_User\Documents\gci_airregi\kaggle-template\notebook\exp05.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

# タイトルを変更
nb['cells'][0]['source'] = [
    '# exp11: アンサンブル手法の比較\n',
    '\n',
    '**ベースライン**: exp05のBest Optunaパラメータ使用\n',
    '\n',
    '**アンサンブルパターン**:\n',
    '1. 重み付けアンサンブル (最適化あり)\n',
    '   - Pattern A: Ridge + CatBoost + ExtraTrees + HistGB\n',
    '   - Pattern B: LightGBM + CatBoost + HistGB\n',
    '\n',
    '2. スタッキング\n',
    '   - Pattern A: [Ridge, CatBoost, HistGB] → Ridge meta\n',
    '   - Pattern B1: [Ridge, LightGBM, CatBoost] → Ridge meta\n',
    '   - Pattern B2: [Ridge, LightGBM, CatBoost, HistGB] → Ridge meta\n'
]

# Best Optunaパラメータセル（exp10と同じ）
best_params_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# Best Optuna Parameters (from exp05 optimization)\n",
        "# ============================================================================\n",
        "\n",
        "BEST_PARAMS = {\n",
        "    'Ridge': {'alpha': 70.4183028501599},\n",
        "    'RandomForest': {\n",
        "        'n_estimators': 261,\n",
        "        'max_depth': 21,\n",
        "        'min_samples_split': 13,\n",
        "        'min_samples_leaf': 1,\n",
        "        'max_features': None\n",
        "    },\n",
        "    'ExtraTrees': {\n",
        "        'n_estimators': 229,\n",
        "        'max_depth': 29,\n",
        "        'min_samples_split': 16,\n",
        "        'min_samples_leaf': 1,\n",
        "        'max_features': None\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'n_estimators': 477,\n",
        "        'learning_rate': 0.26835579181051533,\n",
        "        'max_depth': 2,\n",
        "        'min_samples_split': 5,\n",
        "        'min_samples_leaf': 1,\n",
        "        'subsample': 0.9721678101451118\n",
        "    },\n",
        "    'HistGradientBoosting': {\n",
        "        'max_iter': 238,\n",
        "        'learning_rate': 0.015251103470998385,\n",
        "        'max_depth': 20,\n",
        "        'min_samples_leaf': 33,\n",
        "        'l2_regularization': 9.037967498117355\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': 4666,\n",
        "        'learning_rate': 0.18057598957444881,\n",
        "        'max_depth': 5,\n",
        "        'subsample': 0.7726782988943871,\n",
        "        'colsample_bytree': 0.6039221062901661,\n",
        "        'reg_lambda': 0.9814360532884759,\n",
        "        'reg_alpha': 1.6016986762895833\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'n_estimators': 127,\n",
        "        'learning_rate': 0.1601531217136121,\n",
        "        'num_leaves': 112,\n",
        "        'max_depth': 12,\n",
        "        'subsample': 0.9085081386743783,\n",
        "        'colsample_bytree': 0.6296178606936361,\n",
        "        'reg_lambda': 0.5211124595788266,\n",
        "        'reg_alpha': 0.5793452976256486\n",
        "    },\n",
        "    'CatBoost': {\n",
        "        'iterations': 2295,\n",
        "        'learning_rate': 0.10429705988762059,\n",
        "        'depth': 5,\n",
        "        'l2_leaf_reg': 6.359326196557493,\n",
        "        'subsample': 0.8738193035765242\n",
        "    }\n",
        "}\n",
        "\n",
        "print('Best parameters loaded from exp05 optimization')\n"
    ]
}

# exp05のCell 0-12を保持（データ準備部分）
new_cells = nb['cells'][:13]  # Cell 0-12を保持
new_cells.insert(2, best_params_cell)  # Best paramsをCell 2に挿入

# ベースモデル訓練セル
base_models_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# Base Models Training\n",
        "# ============================================================================\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "base_models = {}\n",
        "base_predictions_val = {}  # Validation予測値\n",
        "base_predictions_test = {}  # Test予測値\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('Training Base Models with Best Optuna Parameters')\n",
        "print('='*80)\n",
        "\n",
        "# Ridge\n",
        "print('\\n[1/8] Ridge')\n",
        "model = Ridge(**BEST_PARAMS['Ridge'], random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "val_pred = model.predict(X_test)\n",
        "test_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, val_pred)\n",
        "print(f'  Validation MAE: {mae:.4f}')\n",
        "base_models['Ridge'] = model\n",
        "base_predictions_val['Ridge'] = val_pred\n",
        "base_predictions_test['Ridge'] = test_pred\n",
        "\n",
        "# RandomForest\n",
        "print('\\n[2/8] RandomForest')\n",
        "model = RandomForestRegressor(**BEST_PARAMS['RandomForest'], random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "val_pred = model.predict(X_test)\n",
        "test_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, val_pred)\n",
        "print(f'  Validation MAE: {mae:.4f}')\n",
        "base_models['RandomForest'] = model\n",
        "base_predictions_val['RandomForest'] = val_pred\n",
        "base_predictions_test['RandomForest'] = test_pred\n",
        "\n",
        "# ExtraTrees\n",
        "print('\\n[3/8] ExtraTrees')\n",
        "model = ExtraTreesRegressor(**BEST_PARAMS['ExtraTrees'], random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "val_pred = model.predict(X_test)\n",
        "test_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, val_pred)\n",
        "print(f'  Validation MAE: {mae:.4f}')\n",
        "base_models['ExtraTrees'] = model\n",
        "base_predictions_val['ExtraTrees'] = val_pred\n",
        "base_predictions_test['ExtraTrees'] = test_pred\n",
        "\n",
        "# GradientBoosting\n",
        "print('\\n[4/8] GradientBoosting')\n",
        "model = GradientBoostingRegressor(**BEST_PARAMS['GradientBoosting'], random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "val_pred = model.predict(X_test)\n",
        "test_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, val_pred)\n",
        "print(f'  Validation MAE: {mae:.4f}')\n",
        "base_models['GradientBoosting'] = model\n",
        "base_predictions_val['GradientBoosting'] = val_pred\n",
        "base_predictions_test['GradientBoosting'] = test_pred\n",
        "\n",
        "# HistGradientBoosting\n",
        "print('\\n[5/8] HistGradientBoosting')\n",
        "model = HistGradientBoostingRegressor(**BEST_PARAMS['HistGradientBoosting'], random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "val_pred = model.predict(X_test)\n",
        "test_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, val_pred)\n",
        "print(f'  Validation MAE: {mae:.4f}')\n",
        "base_models['HistGradientBoosting'] = model\n",
        "base_predictions_val['HistGradientBoosting'] = val_pred\n",
        "base_predictions_test['HistGradientBoosting'] = test_pred\n",
        "\n",
        "# XGBoost\n",
        "print('\\n[6/8] XGBoost')\n",
        "model = XGBRegressor(**BEST_PARAMS['XGBoost'], random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "val_pred = model.predict(X_test)\n",
        "test_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, val_pred)\n",
        "print(f'  Validation MAE: {mae:.4f}')\n",
        "base_models['XGBoost'] = model\n",
        "base_predictions_val['XGBoost'] = val_pred\n",
        "base_predictions_test['XGBoost'] = test_pred\n",
        "\n",
        "# LightGBM\n",
        "print('\\n[7/8] LightGBM')\n",
        "model = LGBMRegressor(**BEST_PARAMS['LightGBM'], random_state=42, n_jobs=-1, verbose=-1)\n",
        "model.fit(X_train, y_train)\n",
        "val_pred = model.predict(X_test)\n",
        "test_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, val_pred)\n",
        "print(f'  Validation MAE: {mae:.4f}')\n",
        "base_models['LightGBM'] = model\n",
        "base_predictions_val['LightGBM'] = val_pred\n",
        "base_predictions_test['LightGBM'] = test_pred\n",
        "\n",
        "# CatBoost\n",
        "print('\\n[8/8] CatBoost')\n",
        "model = CatBoostRegressor(**BEST_PARAMS['CatBoost'], random_state=42, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "val_pred = model.predict(X_test)\n",
        "test_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, val_pred)\n",
        "print(f'  Validation MAE: {mae:.4f}')\n",
        "base_models['CatBoost'] = model\n",
        "base_predictions_val['CatBoost'] = val_pred\n",
        "base_predictions_test['CatBoost'] = test_pred\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print(f'All {len(base_models)} base models trained successfully!')\n",
        "print('='*80)\n"
    ]
}

# 重み付けアンサンブル最適化セル
weighted_ensemble_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# Weighted Ensemble (with Optimization)\n",
        "# ============================================================================\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def optimize_weights(predictions_dict, y_true, model_names):\n",
        "    \"\"\"\n",
        "    重み付けアンサンブルの最適な重みを探索\n",
        "    制約: 重みの合計=1, 各重み>=0\n",
        "    \"\"\"\n",
        "    # 予測値を行列に変換\n",
        "    preds_matrix = np.column_stack([predictions_dict[name] for name in model_names])\n",
        "    \n",
        "    # 最小化する目的関数（MAE）\n",
        "    def objective(weights):\n",
        "        ensemble_pred = preds_matrix @ weights\n",
        "        return mean_absolute_error(y_true, ensemble_pred)\n",
        "    \n",
        "    # 制約条件: 重みの合計=1\n",
        "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
        "    \n",
        "    # 境界条件: 各重み >= 0\n",
        "    bounds = [(0, 1) for _ in range(len(model_names))]\n",
        "    \n",
        "    # 初期値: 均等な重み\n",
        "    initial_weights = np.ones(len(model_names)) / len(model_names)\n",
        "    \n",
        "    # 最適化実行\n",
        "    result = minimize(objective, initial_weights, method='SLSQP',\n",
        "                     bounds=bounds, constraints=constraints)\n",
        "    \n",
        "    return result.x, result.fun\n",
        "\n",
        "# Pattern A: Ridge + CatBoost + ExtraTrees + HistGB\n",
        "print('\\n' + '='*80)\n",
        "print('[Weighted Ensemble Pattern A]')\n",
        "print('Models: Ridge + CatBoost + ExtraTrees + HistGB')\n",
        "print('='*80)\n",
        "\n",
        "pattern_a_models = ['Ridge', 'CatBoost', 'ExtraTrees', 'HistGradientBoosting']\n",
        "weights_a, mae_a = optimize_weights(base_predictions_val, y_test, pattern_a_models)\n",
        "\n",
        "print('\\nOptimized Weights:')\n",
        "for name, w in zip(pattern_a_models, weights_a):\n",
        "    print(f'  {name:25s}: {w:.4f}')\n",
        "print(f'\\nValidation MAE: {mae_a:.4f}')\n",
        "\n",
        "# Test予測\n",
        "test_preds_a = np.column_stack([base_predictions_test[name] for name in pattern_a_models])\n",
        "weighted_test_a = test_preds_a @ weights_a\n",
        "\n",
        "# Pattern B: LightGBM + CatBoost + HistGB\n",
        "print('\\n' + '='*80)\n",
        "print('[Weighted Ensemble Pattern B]')\n",
        "print('Models: LightGBM + CatBoost + HistGB')\n",
        "print('='*80)\n",
        "\n",
        "pattern_b_models = ['LightGBM', 'CatBoost', 'HistGradientBoosting']\n",
        "weights_b, mae_b = optimize_weights(base_predictions_val, y_test, pattern_b_models)\n",
        "\n",
        "print('\\nOptimized Weights:')\n",
        "for name, w in zip(pattern_b_models, weights_b):\n",
        "    print(f'  {name:25s}: {w:.4f}')\n",
        "print(f'\\nValidation MAE: {mae_b:.4f}')\n",
        "\n",
        "# Test予測\n",
        "test_preds_b = np.column_stack([base_predictions_test[name] for name in pattern_b_models])\n",
        "weighted_test_b = test_preds_b @ weights_b\n"
    ]
}

# スタッキングアンサンブルセル
stacking_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# Stacking Ensemble\n",
        "# ============================================================================\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import Ridge as MetaRidge\n",
        "\n",
        "def create_stacking_ensemble(base_models_list, model_names, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    スタッキングアンサンブルを作成\n",
        "    \n",
        "    Args:\n",
        "        base_models_list: ベースモデルのリスト\n",
        "        model_names: モデル名のリスト\n",
        "        X_train, y_train: 訓練データ\n",
        "        X_test, y_test: テストデータ\n",
        "    \n",
        "    Returns:\n",
        "        meta_model: 学習済みメタモデル\n",
        "        val_mae: Validation MAE\n",
        "        test_pred: テスト予測値\n",
        "    \"\"\"\n",
        "    # Step 1: 各ベースモデルのOOF予測値を作成（5-fold CV）\n",
        "    print('  Creating OOF predictions with 5-fold CV...')\n",
        "    oof_train = np.zeros((len(X_train), len(base_models_list)))\n",
        "    \n",
        "    for i, (model, name) in enumerate(zip(base_models_list, model_names)):\n",
        "        print(f'    [{i+1}/{len(base_models_list)}] {name}', end=' ')\n",
        "        oof_train[:, i] = cross_val_predict(model, X_train, y_train, cv=5, n_jobs=-1)\n",
        "        print('Done')\n",
        "    \n",
        "    # Step 2: 各ベースモデルを全データで再訓練してテスト予測を作成\n",
        "    print('\\n  Training base models on full training data...')\n",
        "    test_preds = np.zeros((len(X_test), len(base_models_list)))\n",
        "    \n",
        "    for i, (model, name) in enumerate(zip(base_models_list, model_names)):\n",
        "        print(f'    [{i+1}/{len(base_models_list)}] {name}', end=' ')\n",
        "        model.fit(X_train, y_train)\n",
        "        test_preds[:, i] = model.predict(X_test)\n",
        "        print('Done')\n",
        "    \n",
        "    # Step 3: メタモデル（Ridge）を訓練\n",
        "    print('\\n  Training meta-model (Ridge)...')\n",
        "    meta_model = MetaRidge(alpha=1.0, random_state=42)\n",
        "    meta_model.fit(oof_train, y_train)\n",
        "    \n",
        "    # Step 4: メタモデルでテスト予測\n",
        "    final_test_pred = meta_model.predict(test_preds)\n",
        "    val_mae = mean_absolute_error(y_test, final_test_pred)\n",
        "    \n",
        "    # メタモデルの係数を表示\n",
        "    print('\\n  Meta-model coefficients:')\n",
        "    for name, coef in zip(model_names, meta_model.coef_):\n",
        "        print(f'    {name:25s}: {coef:.4f}')\n",
        "    print(f'    Intercept: {meta_model.intercept_:.4f}')\n",
        "    \n",
        "    return meta_model, val_mae, final_test_pred\n",
        "\n",
        "# Pattern A: Ridge + CatBoost + HistGB\n",
        "print('\\n' + '='*80)\n",
        "print('[Stacking Pattern A: Ridge + CatBoost + HistGB]')\n",
        "print('='*80)\n",
        "\n",
        "stacking_a_names = ['Ridge', 'CatBoost', 'HistGradientBoosting']\n",
        "stacking_a_models = [\n",
        "    Ridge(**BEST_PARAMS['Ridge'], random_state=42),\n",
        "    CatBoostRegressor(**BEST_PARAMS['CatBoost'], random_state=42, verbose=0),\n",
        "    HistGradientBoostingRegressor(**BEST_PARAMS['HistGradientBoosting'], random_state=42)\n",
        "]\n",
        "\n",
        "meta_a, mae_stack_a, test_stack_a = create_stacking_ensemble(\n",
        "    stacking_a_models, stacking_a_names, X_train, y_train, X_test, y_test\n",
        ")\n",
        "print(f'\\nValidation MAE: {mae_stack_a:.4f}')\n",
        "\n",
        "# Pattern B1: Ridge + LightGBM + CatBoost\n",
        "print('\\n' + '='*80)\n",
        "print('[Stacking Pattern B1: Ridge + LightGBM + CatBoost]')\n",
        "print('='*80)\n",
        "\n",
        "stacking_b1_names = ['Ridge', 'LightGBM', 'CatBoost']\n",
        "stacking_b1_models = [\n",
        "    Ridge(**BEST_PARAMS['Ridge'], random_state=42),\n",
        "    LGBMRegressor(**BEST_PARAMS['LightGBM'], random_state=42, n_jobs=-1, verbose=-1),\n",
        "    CatBoostRegressor(**BEST_PARAMS['CatBoost'], random_state=42, verbose=0)\n",
        "]\n",
        "\n",
        "meta_b1, mae_stack_b1, test_stack_b1 = create_stacking_ensemble(\n",
        "    stacking_b1_models, stacking_b1_names, X_train, y_train, X_test, y_test\n",
        ")\n",
        "print(f'\\nValidation MAE: {mae_stack_b1:.4f}')\n",
        "\n",
        "# Pattern B2: Ridge + LightGBM + CatBoost + HistGB\n",
        "print('\\n' + '='*80)\n",
        "print('[Stacking Pattern B2: Ridge + LightGBM + CatBoost + HistGB]')\n",
        "print('='*80)\n",
        "\n",
        "stacking_b2_names = ['Ridge', 'LightGBM', 'CatBoost', 'HistGradientBoosting']\n",
        "stacking_b2_models = [\n",
        "    Ridge(**BEST_PARAMS['Ridge'], random_state=42),\n",
        "    LGBMRegressor(**BEST_PARAMS['LightGBM'], random_state=42, n_jobs=-1, verbose=-1),\n",
        "    CatBoostRegressor(**BEST_PARAMS['CatBoost'], random_state=42, verbose=0),\n",
        "    HistGradientBoostingRegressor(**BEST_PARAMS['HistGradientBoosting'], random_state=42)\n",
        "]\n",
        "\n",
        "meta_b2, mae_stack_b2, test_stack_b2 = create_stacking_ensemble(\n",
        "    stacking_b2_models, stacking_b2_names, X_train, y_train, X_test, y_test\n",
        ")\n",
        "print(f'\\nValidation MAE: {mae_stack_b2:.4f}')\n"
    ]
}

# 結果比較セル
comparison_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# Ensemble Results Comparison\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 全アンサンブル結果をまとめる\n",
        "ensemble_results = [\n",
        "    {'Method': 'Weighted A', 'Models': 'Ridge+Cat+Extra+Hist', 'MAE': mae_a},\n",
        "    {'Method': 'Weighted B', 'Models': 'LGBM+Cat+Hist', 'MAE': mae_b},\n",
        "    {'Method': 'Stacking A', 'Models': 'Ridge+Cat+Hist', 'MAE': mae_stack_a},\n",
        "    {'Method': 'Stacking B1', 'Models': 'Ridge+LGBM+Cat', 'MAE': mae_stack_b1},\n",
        "    {'Method': 'Stacking B2', 'Models': 'Ridge+LGBM+Cat+Hist', 'MAE': mae_stack_b2},\n",
        "]\n",
        "\n",
        "# ベースモデルの結果も追加\n",
        "for name, pred in base_predictions_val.items():\n",
        "    mae = mean_absolute_error(y_test, pred)\n",
        "    ensemble_results.append({'Method': 'Base Model', 'Models': name, 'MAE': mae})\n",
        "\n",
        "results_df = pd.DataFrame(ensemble_results)\n",
        "results_df = results_df.sort_values('MAE').reset_index(drop=True)\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('ENSEMBLE RESULTS COMPARISON')\n",
        "print('='*80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# アンサンブル手法のみを抽出してプロット\n",
        "ensemble_only = results_df[results_df['Method'] != 'Base Model'].copy()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(range(len(ensemble_only)), ensemble_only['MAE'], \n",
        "               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "plt.xticks(range(len(ensemble_only)), \n",
        "           [f\"{row['Method']}\\n{row['Models']}\" for _, row in ensemble_only.iterrows()],\n",
        "           rotation=15, ha='right')\n",
        "plt.ylabel('Validation MAE', fontsize=12)\n",
        "plt.title('Ensemble Methods Comparison', fontsize=14, pad=20)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 値をバーの上に表示\n",
        "for i, (bar, mae) in enumerate(zip(bars, ensemble_only['MAE'])):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f'{mae:.2f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_dir}/ensemble_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# CSV保存\n",
        "results_df.to_csv(f'{output_dir}/ensemble_results.csv', index=False)\n",
        "print(f'\\nResults saved to: {output_dir}/ensemble_results.csv')\n"
    ]
}

# 提出ファイル作成セル
submission_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# Create Submission Files for All Ensembles\n",
        "# ============================================================================\n",
        "\n",
        "# アンサンブル予測値を辞書にまとめる\n",
        "ensemble_predictions = {\n",
        "    'weighted_a_ridge_cat_extra_hist': weighted_test_a,\n",
        "    'weighted_b_lgbm_cat_hist': weighted_test_b,\n",
        "    'stacking_a_ridge_cat_hist': test_stack_a,\n",
        "    'stacking_b1_ridge_lgbm_cat': test_stack_b1,\n",
        "    'stacking_b2_ridge_lgbm_cat_hist': test_stack_b2,\n",
        "}\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('Creating submission files...')\n",
        "print('='*80)\n",
        "\n",
        "# 仮のIDを作成（実際のtest_dfにIDがあればそれを使用）\n",
        "test_ids = range(len(weighted_test_a))\n",
        "\n",
        "for name, pred in ensemble_predictions.items():\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        'target_next_day': pred\n",
        "    })\n",
        "    \n",
        "    filepath = f'{output_dir}/submission_{name}.csv'\n",
        "    submission.to_csv(filepath, index=False)\n",
        "    print(f'  Saved: submission_{name}.csv')\n",
        "\n",
        "print('\\nAll submission files created successfully!')\n"
    ]
}

# 新しいセル構造を構築
new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["---\n", "## Base Models Training\n"]
})
new_cells.append(base_models_cell)

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["---\n", "## Weighted Ensemble (Optimized)\n"]
})
new_cells.append(weighted_ensemble_cell)

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["---\n", "## Stacking Ensemble\n"]
})
new_cells.append(stacking_cell)

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["---\n", "## Results Comparison\n"]
})
new_cells.append(comparison_cell)
new_cells.append(submission_cell)

# サマリーセル
new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "**exp11: Ensemble Methods Comparison**\n",
        "\n",
        "### Ensemble Patterns Tested:\n",
        "\n",
        "#### 1. Weighted Ensemble (Optimized)\n",
        "- **Pattern A**: Ridge + CatBoost + ExtraTrees + HistGB\n",
        "- **Pattern B**: LightGBM + CatBoost + HistGB\n",
        "\n",
        "#### 2. Stacking Ensemble (Ridge as Meta-learner)\n",
        "- **Pattern A**: Ridge + CatBoost + HistGB\n",
        "- **Pattern B1**: Ridge + LightGBM + CatBoost\n",
        "- **Pattern B2**: Ridge + LightGBM + CatBoost + HistGB\n",
        "\n",
        "### Key Outputs:\n",
        "1. `ensemble_results.csv` - All ensemble results comparison\n",
        "2. `ensemble_comparison.png` - Visual comparison bar chart\n",
        "3. `submission_*.csv` - Submission files for each ensemble method\n"
    ]
})

# ノートブックを更新
nb['cells'] = new_cells

# exp11として保存
with open(r'c:\Users\PC_User\Documents\gci_airregi\kaggle-template\notebook\exp11.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print('exp11.ipynb created successfully!')
print('\nKey features:')
print('1. Weighted Ensemble with optimization (2 patterns)')
print('2. Stacking Ensemble with Ridge meta-learner (3 patterns)')
print('3. Total 5 ensemble methods compared')
print('4. Automatic weight optimization using scipy.optimize')
print('5. Visual comparison chart and detailed results CSV')
