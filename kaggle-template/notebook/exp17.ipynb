{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp17: acc_geté«˜åº¦ç‰¹å¾´é‡ã®è¿½åŠ \n",
    "\n",
    "**ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: exp16 (acc_getåŸºæœ¬ç‰¹å¾´é‡)\n",
    "\n",
    "**èƒŒæ™¯**:\n",
    "- exp16ã§ã¯acc_get_lag7ã¨acc_get_sum_14dã‚’è¿½åŠ \n",
    "- ã•ã‚‰ã«å·®åˆ†ã€é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€æŒ‡æ•°ç§»å‹•å¹³å‡ã€æˆé•·ç‡ã‚’è¿½åŠ ã—ã¦æ™‚ç³»åˆ—ã®å¤‰åŒ–ã‚’æ‰ãˆã‚‹\n",
    "\n",
    "**è¿½åŠ ç‰¹å¾´é‡ (4å€‹)**:\n",
    "1. `acc_get_diff1`: å‰æ—¥å·®åˆ† = acc_get_cnt.shift(1) - acc_get_cnt.shift(2)\n",
    "2. `acc_get_sum_28d`: ç›´è¿‘28æ—¥é–“ã®ç´¯ç©ï¼ˆshift(1)ã§ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰\n",
    "3. `acc_get_ema_14`: 14æ—¥æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆæ€¥å¢—ã«è¿½å¾“ã—ã‚„ã™ã„ï¼‰\n",
    "4. `acc_get_growth`: çŸ­æœŸ/é•·æœŸæ¯”ç‡ = sum_7d / (sum_28d + eps) â€»ã€Œæœ€è¿‘ã®å‹¢ã„ã€ã‚’è¡¨ç¾\n",
    "\n",
    "**æ¤œè¨¼ç›®æ¨™**:\n",
    "- acc_getã®å¤‰åŒ–ç‡ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å‹¢ã„ã‚’æ‰ãˆã‚‹\n",
    "- æ€¥å¢—ãƒ»æ€¥æ¸›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®äºˆæ¸¬ç²¾åº¦å‘ä¸Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Best Optuna Parameters\n",
    "# ============================================================================\n",
    "\n",
    "BEST_PARAMS = {\n",
    "    'Ridge': {'alpha': 70.4183028501599},\n",
    "    'ExtraTrees': {\n",
    "        'n_estimators': 229,\n",
    "        'max_depth': 29,\n",
    "        'min_samples_split': 16,\n",
    "        'min_samples_leaf': 1,\n",
    "        'max_features': None\n",
    "    },\n",
    "    'HistGradientBoosting': {\n",
    "        'max_iter': 238,\n",
    "        'learning_rate': 0.015251103470998385,\n",
    "        'max_depth': 20,\n",
    "        'min_samples_leaf': 33,\n",
    "        'l2_regularization': 9.037967498117355\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': 127,\n",
    "        'learning_rate': 0.1601531217136121,\n",
    "        'num_leaves': 112,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9085081386743783,\n",
    "        'colsample_bytree': 0.6296178606936361,\n",
    "        'reg_lambda': 0.5211124595788266,\n",
    "        'reg_alpha': 0.5793452976256486\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': 2295,\n",
    "        'learning_rate': 0.10429705988762059,\n",
    "        'depth': 5,\n",
    "        'l2_leaf_reg': 6.359326196557493,\n",
    "        'subsample': 0.8738193035765242\n",
    "    }\n",
    "}\n",
    "\n",
    "print('Best parameters loaded from exp05 optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬ç‰¹å¾´é‡ä½œæˆ\n",
    "# ==================================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Step 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    calender = pd.read_csv('../input/calender_data.csv')\n",
    "    cm_data = pd.read_csv('../input/cm_data.csv')\n",
    "    gt_service = pd.read_csv('../input/gt_service_name.csv')\n",
    "    acc_get = pd.read_csv('../input/regi_acc_get_data_transform.csv')\n",
    "    call_data = pd.read_csv('../input/regi_call_data_transform.csv')\n",
    "    \n",
    "    calender['cdr_date'] = pd.to_datetime(calender['cdr_date'])\n",
    "    cm_data['cdr_date'] = pd.to_datetime(cm_data['cdr_date'])\n",
    "    acc_get['cdr_date'] = pd.to_datetime(acc_get['cdr_date'])\n",
    "    call_data['cdr_date'] = pd.to_datetime(call_data['cdr_date'])\n",
    "    gt_service['week'] = pd.to_datetime(gt_service['week'])\n",
    "    \n",
    "    print(f\"\\nãƒ‡ãƒ¼ã‚¿æœŸé–“: {call_data['cdr_date'].min()} ~ {call_data['cdr_date'].max()}\")\n",
    "    \n",
    "    return calender, cm_data, gt_service, acc_get, call_data\n",
    "\n",
    "def merge_datasets(calender, cm_data, gt_service, acc_get, call_data):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 2: ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = call_data.copy()\n",
    "    df = df.merge(calender, on='cdr_date', how='left')\n",
    "    df = df.merge(cm_data, on='cdr_date', how='left')\n",
    "    df = df.merge(acc_get, on='cdr_date', how='left')\n",
    "    \n",
    "    gt_service_daily = []\n",
    "    for idx, row in gt_service.iterrows():\n",
    "        week_start = row['week']\n",
    "        for i in range(7):\n",
    "            date = week_start + timedelta(days=i)\n",
    "            gt_service_daily.append({'cdr_date': date, 'search_cnt': row['search_cnt']})\n",
    "    \n",
    "    gt_daily = pd.DataFrame(gt_service_daily)\n",
    "    df = df.merge(gt_daily, on='cdr_date', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_basic_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['year'] = df['cdr_date'].dt.year\n",
    "    df['month'] = df['cdr_date'].dt.month\n",
    "    df['day_of_month'] = df['cdr_date'].dt.day\n",
    "    df['quarter'] = df['cdr_date'].dt.quarter\n",
    "    df['day_of_year'] = df['cdr_date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['cdr_date'].dt.isocalendar().week\n",
    "    df['days_from_start'] = (df['cdr_date'] - df['cdr_date'].min()).dt.days\n",
    "    df['is_month_start'] = (df['day_of_month'] <= 5).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] >= 25).astype(int)\n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='call_num', lags=[1, 2, 3, 5, 7, 14, 30]):\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def create_rolling_features(df, target_col='call_num', windows=[3, 7, 14, 30]):\n",
    "    df = df.copy()\n",
    "    for window in windows:\n",
    "        df[f'ma_{window}'] = df[target_col].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        df[f'ma_std_{window}'] = df[target_col].shift(1).rolling(window=window, min_periods=1).std()\n",
    "    return df\n",
    "\n",
    "def create_aggregated_features(df):\n",
    "    df = df.copy()\n",
    "    df['cm_7d'] = df['cm_flg'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    df['gt_ma_7'] = df['search_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    df['acc_ma_7'] = df['acc_get_cnt'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    \n",
    "    df['dow_avg'] = np.nan\n",
    "    for dow in df['dow'].unique():\n",
    "        mask = df['dow'] == dow\n",
    "        df.loc[mask, 'dow_avg'] = df.loc[mask, 'call_num'].shift(1).expanding().mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# acc_getç‰¹å¾´é‡ã®ä½œæˆï¼ˆexp16ã‹ã‚‰ç¶™æ‰¿ + exp17ã§æ‹¡å¼µï¼‰\n",
    "# ==================================================================================\n",
    "\n",
    "def create_acc_get_features(df):\n",
    "    \"\"\"\n",
    "    acc_getã«é–¢ã™ã‚‹åŒ…æ‹¬çš„ãªç‰¹å¾´é‡ã‚’ä½œæˆ\n",
    "    \n",
    "    exp16ã‹ã‚‰ã®ç¶™æ‰¿:\n",
    "    1. acc_get_lag7: 7æ—¥å‰ã®acc_get_cnt\n",
    "    2. acc_get_sum_14d: ç›´è¿‘14æ—¥é–“ã®åˆè¨ˆ\n",
    "    \n",
    "    ğŸ”¥ exp17ã§è¿½åŠ :\n",
    "    3. acc_get_diff1: å‰æ—¥å·®åˆ†ï¼ˆå¤‰åŒ–ç‡ï¼‰\n",
    "    4. acc_get_sum_28d: ç›´è¿‘28æ—¥é–“ã®ç´¯ç©ï¼ˆé•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰\n",
    "    5. acc_get_ema_14: 14æ—¥æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆæ€¥å¢—ã«è¿½å¾“ï¼‰\n",
    "    6. acc_get_growth: çŸ­æœŸ/é•·æœŸæ¯”ç‡ï¼ˆæœ€è¿‘ã®å‹¢ã„ï¼‰\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 7: acc_getç‰¹å¾´é‡ã®ä½œæˆï¼ˆexp16 + exp17æ‹¡å¼µç‰ˆï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # exp16ã‹ã‚‰ã®ç¶™æ‰¿\n",
    "    print(\"\\n[exp16ã‹ã‚‰ç¶™æ‰¿]\")\n",
    "    df['acc_get_lag7'] = df['acc_get_cnt'].shift(7)\n",
    "    df['acc_get_sum_14d'] = df['acc_get_cnt'].shift(1).rolling(window=14, min_periods=1).sum()\n",
    "    print(\"  1. acc_get_lag7: 7æ—¥å‰ã®acc_get_cnt\")\n",
    "    print(\"  2. acc_get_sum_14d: ç›´è¿‘14æ—¥é–“ã®åˆè¨ˆ\")\n",
    "    \n",
    "    # ğŸ”¥ exp17ã§è¿½åŠ \n",
    "    print(\"\\n[ğŸ”¥ exp17ã§è¿½åŠ ]\")\n",
    "    \n",
    "    # 3. å‰æ—¥å·®åˆ†ï¼ˆå¤‰åŒ–ç‡ã‚’æ‰ãˆã‚‹ï¼‰\n",
    "    df['acc_get_diff1'] = df['acc_get_cnt'].shift(1) - df['acc_get_cnt'].shift(2)\n",
    "    print(\"  3. acc_get_diff1: å‰æ—¥å·®åˆ†ï¼ˆæ€¥å¢—ãƒ»æ€¥æ¸›ã‚’æ‰ãˆã‚‹ï¼‰\")\n",
    "    \n",
    "    # 4. ç›´è¿‘28æ—¥é–“ã®ç´¯ç©ï¼ˆé•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰\n",
    "    df['acc_get_sum_28d'] = df['acc_get_cnt'].shift(1).rolling(window=28, min_periods=1).sum()\n",
    "    print(\"  4. acc_get_sum_28d: ç›´è¿‘28æ—¥é–“ã®ç´¯ç©ï¼ˆé•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰\")\n",
    "    \n",
    "    # 5. æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆæ€¥å¢—ã«è¿½å¾“ã—ã‚„ã™ã„ï¼‰\n",
    "    df['acc_get_ema_14'] = df['acc_get_cnt'].shift(1).ewm(span=14, adjust=False).mean()\n",
    "    print(\"  5. acc_get_ema_14: 14æ—¥æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆæ€¥å¢—ã«è¿½å¾“ï¼‰\")\n",
    "    \n",
    "    # 6. æˆé•·ç‡ï¼ˆæœ€è¿‘ã®å‹¢ã„ = çŸ­æœŸ/é•·æœŸï¼‰\n",
    "    acc_get_sum_7d = df['acc_get_cnt'].shift(1).rolling(window=7, min_periods=1).sum()\n",
    "    eps = 1.0  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢\n",
    "    df['acc_get_growth'] = acc_get_sum_7d / (df['acc_get_sum_28d'] + eps)\n",
    "    print(\"  6. acc_get_growth: sum_7d / (sum_28d + eps) = æœ€è¿‘ã®å‹¢ã„\")\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º\n",
    "    print(\"\\nã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€æ–°20ä»¶ï¼‰:\")\n",
    "    cols_to_show = ['cdr_date', 'dow', 'acc_get_cnt', 'acc_get_diff1', \n",
    "                    'acc_get_sum_14d', 'acc_get_sum_28d', 'acc_get_ema_14', 'acc_get_growth']\n",
    "    print(df[cols_to_show].tail(20).to_string())\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±\n",
    "    print(\"\\nçµ±è¨ˆæƒ…å ±:\")\n",
    "    feature_cols = ['acc_get_cnt', 'acc_get_lag7', 'acc_get_diff1', 'acc_get_sum_14d', \n",
    "                    'acc_get_sum_28d', 'acc_get_ema_14', 'acc_get_growth']\n",
    "    print(df[feature_cols].describe().to_string())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡ã®ä½œæˆï¼ˆexp15ã‹ã‚‰ç¶™æ‰¿ï¼‰\n",
    "# ==================================================================================\n",
    "\n",
    "def create_regime_change_features(df):\n",
    "    \"\"\"\n",
    "    æ¶ˆè²»ç¨å¢—ç¨ãƒ»è»½æ¸›ç¨ç‡å°å…¥ã«é–¢ã™ã‚‹ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡ã‚’ä½œæˆ\n",
    "    \n",
    "    é‡è¦ãªæ—¥ä»˜:\n",
    "    - 2019å¹´10æœˆ1æ—¥: æ¶ˆè²»ç¨10%ãƒ»è»½æ¸›ç¨ç‡å°å…¥\n",
    "    - 2019å¹´9æœˆ30æ—¥: é§†ã‘è¾¼ã¿éœ€è¦ã®ç· åˆ‡æ—¥\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Step 8: ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡ã®ä½œæˆï¼ˆexp15ã‹ã‚‰ç¶™æ‰¿ï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # é‡è¦ãªæ—¥ä»˜ã‚’å®šç¾©\n",
    "    tax_implementation_date = pd.Timestamp('2019-10-01')\n",
    "    rush_deadline = pd.Timestamp('2019-09-30')\n",
    "    \n",
    "    # 1. 2019å¹´10æœˆ1æ—¥ï¼ˆæ¶ˆè²»ç¨10%å°å…¥ï¼‰ã«é–¢ã™ã‚‹ç‰¹å¾´é‡\n",
    "    df['days_to_2019_10_01'] = (tax_implementation_date - df['cdr_date']).dt.days\n",
    "    df['is_pre_2019_10_01'] = (df['cdr_date'] < tax_implementation_date).astype(int)\n",
    "    df['is_post_2019_10_01'] = (df['cdr_date'] >= tax_implementation_date).astype(int)\n",
    "    \n",
    "    # 2. 2019å¹´9æœˆ30æ—¥ï¼ˆé§†ã‘è¾¼ã¿ç· åˆ‡ï¼‰ã«é–¢ã™ã‚‹ç‰¹å¾´é‡\n",
    "    df['days_to_2019_09_30'] = (rush_deadline - df['cdr_date']).dt.days\n",
    "    df['is_pre_2019_09_30'] = (df['cdr_date'] < rush_deadline).astype(int)\n",
    "    df['is_post_2019_09_30'] = (df['cdr_date'] >= rush_deadline).astype(int)\n",
    "    \n",
    "    # 3. é§†ã‘è¾¼ã¿æœŸé–“ï¼ˆ3ãƒ¶æœˆå‰ã€œå½“æ—¥ï¼‰\n",
    "    rush_start = rush_deadline - pd.Timedelta(days=90)\n",
    "    df['is_rush_period'] = ((df['cdr_date'] >= rush_start) & \n",
    "                            (df['cdr_date'] <= rush_deadline)).astype(int)\n",
    "    \n",
    "    # 4. å°å…¥ç›´å¾Œã®é©å¿œæœŸé–“ï¼ˆ1ãƒ¶æœˆï¼‰\n",
    "    adaptation_end = tax_implementation_date + pd.Timedelta(days=30)\n",
    "    df['is_adaptation_period'] = ((df['cdr_date'] >= tax_implementation_date) & \n",
    "                                   (df['cdr_date'] <= adaptation_end)).astype(int)\n",
    "    \n",
    "    print(\"\\nä½œæˆã—ãŸç‰¹å¾´é‡: 8å€‹ï¼ˆãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–é–¢é€£ï¼‰\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"\\n\" + \"*\" * 80)\n",
    "print(\"exp17: acc_geté«˜åº¦ç‰¹å¾´é‡ã®è¿½åŠ ï¼ˆå·®åˆ†ãƒ»é•·æœŸç´¯ç©ãƒ»EMAãƒ»æˆé•·ç‡ï¼‰\")\n",
    "print(\"*\" * 80)\n",
    "\n",
    "calender, cm_data, gt_service, acc_get, call_data = load_and_preprocess_data()\n",
    "df = merge_datasets(calender, cm_data, gt_service, acc_get, call_data)\n",
    "df = create_basic_time_features(df)\n",
    "df = create_lag_features(df)\n",
    "df = create_rolling_features(df)\n",
    "df = create_aggregated_features(df)\n",
    "\n",
    "# ğŸ”¥ acc_getç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆexp16 + exp17æ‹¡å¼µï¼‰\n",
    "df = create_acc_get_features(df)\n",
    "\n",
    "# ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆexp15ã‹ã‚‰ç¶™æ‰¿ï¼‰\n",
    "df = create_regime_change_features(df)\n",
    "\n",
    "# ç¿Œæ—¥ã®å…¥é›»æ•°ã‚’ç›®çš„å¤‰æ•°ã«ã™ã‚‹\n",
    "df['target_next_day'] = df['call_num'].shift(-1)\n",
    "df = df.dropna(subset=['target_next_day']).reset_index(drop=True)\n",
    "\n",
    "# å¹³æ—¥ã®ã¿\n",
    "df_model = df[df['dow'].isin([1, 2, 3, 4, 5])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nå¹³æ—¥ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_model)}è¡Œ\")\n",
    "print(f\"æœŸé–“: {df_model['cdr_date'].min()} ~ {df_model['cdr_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Rolling Window Validation ã®è¨­å®š\n",
    "# ==================================================================================\n",
    "\n",
    "# ğŸ”¥ ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆexp17ã§4ã¤è¿½åŠ ï¼‰\n",
    "feature_cols = [\n",
    "    # åŸºæœ¬æ™‚ç³»åˆ—ç‰¹å¾´é‡\n",
    "    'dow', 'day_of_month', 'month', 'quarter', 'year', \n",
    "    'days_from_start', 'day_of_year', 'week_of_year',\n",
    "    'is_month_start', 'is_month_end',\n",
    "    # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´é‡\n",
    "    'woy', 'wom', 'day_before_holiday_flag',\n",
    "    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿\n",
    "    'cm_flg', 'acc_get_cnt', 'search_cnt',\n",
    "    # é›†ç´„ç‰¹å¾´é‡\n",
    "    'cm_7d', 'gt_ma_7', 'acc_ma_7', 'dow_avg',\n",
    "    # ãƒ©ã‚°ç‰¹å¾´é‡\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_5', 'lag_7', 'lag_14', 'lag_30',\n",
    "    # ç§»å‹•å¹³å‡ç‰¹å¾´é‡\n",
    "    'ma_3', 'ma_7', 'ma_14', 'ma_30',\n",
    "    'ma_std_3', 'ma_std_7', 'ma_std_14', 'ma_std_30',\n",
    "    # ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç‰¹å¾´é‡ï¼ˆexp15ã‹ã‚‰ç¶™æ‰¿ï¼‰\n",
    "    'days_to_2019_10_01', 'is_pre_2019_10_01', 'is_post_2019_10_01',\n",
    "    'days_to_2019_09_30', 'is_pre_2019_09_30', 'is_post_2019_09_30',\n",
    "    'is_rush_period', 'is_adaptation_period',\n",
    "    # acc_getç‰¹å¾´é‡ï¼ˆexp16ã‹ã‚‰ç¶™æ‰¿ï¼‰\n",
    "    'acc_get_lag7', 'acc_get_sum_14d',\n",
    "    # ğŸ”¥ NEW in exp17: acc_geté«˜åº¦ç‰¹å¾´é‡\n",
    "    'acc_get_diff1', 'acc_get_sum_28d', 'acc_get_ema_14', 'acc_get_growth'\n",
    "]\n",
    "\n",
    "print(f\"\\nä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "print(f\"  (exp16æ¯”: +4ç‰¹å¾´é‡)\")\n",
    "print(f\"  ğŸ”¥ NEW:\")\n",
    "print(f\"    - acc_get_diff1: å‰æ—¥å·®åˆ†\")\n",
    "print(f\"    - acc_get_sum_28d: ç›´è¿‘28æ—¥ç´¯ç©\")\n",
    "print(f\"    - acc_get_ema_14: 14æ—¥æŒ‡æ•°ç§»å‹•å¹³å‡\")\n",
    "print(f\"    - acc_get_growth: çŸ­æœŸ/é•·æœŸæ¯”ç‡ï¼ˆå‹¢ã„ï¼‰\")\n",
    "\n",
    "# Rolling Windowè¨­å®š\n",
    "test_window_days = 60\n",
    "step_days = 30\n",
    "\n",
    "# æ¬ æå€¤ã‚’é™¤å»\n",
    "df_clean = df_model.dropna(subset=feature_cols + ['target_next_day']).copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Rolling Window Validation è¨­å®š\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ãƒ†ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {test_window_days}æ—¥ï¼ˆç´„2ãƒ¶æœˆï¼‰\")\n",
    "print(f\"ã‚¹ãƒ†ãƒƒãƒ—: {step_days}æ—¥ï¼ˆç´„1ãƒ¶æœˆï¼‰\")\n",
    "print(f\"\\nä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {len(df_clean)}è¡Œ\")\n",
    "\n",
    "# Rolling Window ã®åˆ†å‰²ç‚¹ã‚’è¨ˆç®—\n",
    "min_date = df_clean['cdr_date'].min()\n",
    "max_date = df_clean['cdr_date'].max()\n",
    "min_train_days = 90\n",
    "\n",
    "windows = []\n",
    "current_test_end = max_date\n",
    "\n",
    "while True:\n",
    "    test_start = current_test_end - pd.Timedelta(days=test_window_days)\n",
    "    train_end = test_start - pd.Timedelta(days=1)\n",
    "    \n",
    "    if (train_end - min_date).days < min_train_days:\n",
    "        break\n",
    "    \n",
    "    windows.append({\n",
    "        'train_start': min_date,\n",
    "        'train_end': train_end,\n",
    "        'test_start': test_start,\n",
    "        'test_end': current_test_end\n",
    "    })\n",
    "    \n",
    "    current_test_end = test_start - pd.Timedelta(days=1)\n",
    "\n",
    "windows = windows[::-1]\n",
    "\n",
    "print(f\"\\nä½œæˆã•ã‚ŒãŸã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: {len(windows)}\")\n",
    "print(\"\\nã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è©³ç´°:\")\n",
    "for i, w in enumerate(windows):\n",
    "    print(f\"\\nWindow {i+1}:\")\n",
    "    print(f\"  Train: {w['train_start'].strftime('%Y-%m-%d')} ~ {w['train_end'].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Test : {w['test_start'].strftime('%Y-%m-%d')} ~ {w['test_end'].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# è©•ä¾¡é–¢æ•°\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def calculate_wape(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'WAPE': calculate_wape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "print('è©•ä¾¡é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Rolling Window Validation ã®å®Ÿè¡Œ\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "all_window_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Rolling Window Validation å®Ÿè¡Œï¼ˆacc_geté«˜åº¦ç‰¹å¾´é‡è¿½åŠ ç‰ˆï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for window_idx, window in enumerate(windows):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Window {window_idx + 1}/{len(windows)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train: {window['train_start'].strftime('%Y-%m-%d')} ~ {window['train_end'].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Test : {window['test_start'].strftime('%Y-%m-%d')} ~ {window['test_end'].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    train_mask = (df_clean['cdr_date'] >= window['train_start']) & (df_clean['cdr_date'] <= window['train_end'])\n",
    "    test_mask = (df_clean['cdr_date'] >= window['test_start']) & (df_clean['cdr_date'] <= window['test_end'])\n",
    "    \n",
    "    X_train = df_clean.loc[train_mask, feature_cols]\n",
    "    y_train = df_clean.loc[train_mask, 'target_next_day']\n",
    "    X_test = df_clean.loc[test_mask, feature_cols]\n",
    "    y_test = df_clean.loc[test_mask, 'target_next_day']\n",
    "    \n",
    "    print(f\"\\nTrain: {len(X_train)}ä»¶, Test: {len(X_test)}ä»¶\")\n",
    "    \n",
    "    # å„ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡\n",
    "    window_models = {}\n",
    "    window_predictions = {}\n",
    "    \n",
    "    # 1. HistGradientBoosting\n",
    "    print(\"\\n[1/5] HistGradientBoosting...\")\n",
    "    hist_model = HistGradientBoostingRegressor(**BEST_PARAMS['HistGradientBoosting'], random_state=42)\n",
    "    hist_model.fit(X_train, y_train)\n",
    "    hist_pred = hist_model.predict(X_test)\n",
    "    hist_metrics = evaluate_model(y_test, hist_pred)\n",
    "    print(f\"  MAE: {hist_metrics['MAE']:.2f}, RMSE: {hist_metrics['RMSE']:.2f}, R2: {hist_metrics['R2']:.3f}\")\n",
    "    window_models['HistGradientBoosting'] = hist_model\n",
    "    window_predictions['HistGradientBoosting'] = hist_pred\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'HistGradientBoosting', **hist_metrics})\n",
    "    \n",
    "    # 2. ExtraTrees\n",
    "    print(\"[2/5] ExtraTrees...\")\n",
    "    extra_model = ExtraTreesRegressor(**BEST_PARAMS['ExtraTrees'], random_state=42, n_jobs=-1)\n",
    "    extra_model.fit(X_train, y_train)\n",
    "    extra_pred = extra_model.predict(X_test)\n",
    "    extra_metrics = evaluate_model(y_test, extra_pred)\n",
    "    print(f\"  MAE: {extra_metrics['MAE']:.2f}, RMSE: {extra_metrics['RMSE']:.2f}, R2: {extra_metrics['R2']:.3f}\")\n",
    "    window_models['ExtraTrees'] = extra_model\n",
    "    window_predictions['ExtraTrees'] = extra_pred\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'ExtraTrees', **extra_metrics})\n",
    "    \n",
    "    # 3. CatBoost\n",
    "    print(\"[3/5] CatBoost...\")\n",
    "    catboost_model = CatBoostRegressor(**BEST_PARAMS['CatBoost'], random_state=42, verbose=0)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_pred = catboost_model.predict(X_test)\n",
    "    catboost_metrics = evaluate_model(y_test, catboost_pred)\n",
    "    print(f\"  MAE: {catboost_metrics['MAE']:.2f}, RMSE: {catboost_metrics['RMSE']:.2f}, R2: {catboost_metrics['R2']:.3f}\")\n",
    "    window_models['CatBoost'] = catboost_model\n",
    "    window_predictions['CatBoost'] = catboost_pred\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'CatBoost', **catboost_metrics})\n",
    "    \n",
    "    # 4. Ridge\n",
    "    print(\"[4/5] Ridge...\")\n",
    "    ridge_model = Ridge(**BEST_PARAMS['Ridge'], random_state=42)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    ridge_pred = ridge_model.predict(X_test)\n",
    "    ridge_metrics = evaluate_model(y_test, ridge_pred)\n",
    "    print(f\"  MAE: {ridge_metrics['MAE']:.2f}, RMSE: {ridge_metrics['RMSE']:.2f}, R2: {ridge_metrics['R2']:.3f}\")\n",
    "    window_models['Ridge'] = ridge_model\n",
    "    window_predictions['Ridge'] = ridge_pred\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'Ridge', **ridge_metrics})\n",
    "    \n",
    "    # 5. Weighted Ensemble A\n",
    "    print(\"[5/5] WeightedEnsemble_A...\")\n",
    "    \n",
    "    def optimize_weights(predictions_dict, y_true, model_names):\n",
    "        preds_matrix = np.column_stack([predictions_dict[name] for name in model_names])\n",
    "        \n",
    "        def objective(weights):\n",
    "            ensemble_pred = preds_matrix @ weights\n",
    "            return mean_absolute_error(y_true, ensemble_pred)\n",
    "        \n",
    "        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
    "        bounds = [(0, 1) for _ in range(len(model_names))]\n",
    "        initial_weights = np.ones(len(model_names)) / len(model_names)\n",
    "        \n",
    "        result = minimize(objective, initial_weights, method='SLSQP',\n",
    "                         bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "    \n",
    "    pattern_a_models = ['Ridge', 'CatBoost', 'ExtraTrees', 'HistGradientBoosting']\n",
    "    weights_a = optimize_weights(window_predictions, y_test, pattern_a_models)\n",
    "    \n",
    "    weightA_pred = np.column_stack([window_predictions[name] for name in pattern_a_models]) @ weights_a\n",
    "    weightA_metrics = evaluate_model(y_test, weightA_pred)\n",
    "    print(f\"  MAE: {weightA_metrics['MAE']:.2f}, RMSE: {weightA_metrics['RMSE']:.2f}, R2: {weightA_metrics['R2']:.3f}\")\n",
    "    all_window_results.append({'window': window_idx+1, 'model': 'WeightedEnsemble_A', **weightA_metrics})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Rolling Window Validation å®Œäº†\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# exp16ã¨ã®æ¯”è¼ƒåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# exp16ã®çµæœã‚’èª­ã¿è¾¼ã‚“ã§æ¯”è¼ƒ\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "exp17_results_df = pd.DataFrame(all_window_results)\n",
    "\n",
    "# exp16ã®çµæœã‚’èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "exp16_results_path = '../output/exp16/rolling_window_results.csv'\n",
    "if os.path.exists(exp16_results_path):\n",
    "    exp16_results_df = pd.read_csv(exp16_results_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"exp16 vs exp17 æ¯”è¼ƒï¼ˆacc_geté«˜åº¦ç‰¹å¾´é‡è¿½åŠ ã«ã‚ˆã‚‹æ”¹å–„ã‚’ç¢ºèªï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§æ¯”è¼ƒ\n",
    "    models = exp17_results_df['model'].unique()\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{model}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        exp16_model = exp16_results_df[exp16_results_df['model'] == model]\n",
    "        exp17_model = exp17_results_df[exp17_results_df['model'] == model]\n",
    "        \n",
    "        if len(exp16_model) > 0 and len(exp17_model) > 0:\n",
    "            exp16_avg_mae = exp16_model['MAE'].mean()\n",
    "            exp17_avg_mae = exp17_model['MAE'].mean()\n",
    "            improvement = exp16_avg_mae - exp17_avg_mae\n",
    "            improvement_pct = (improvement / exp16_avg_mae) * 100\n",
    "            \n",
    "            status = \"âœ… æ”¹å–„\" if improvement > 0 else \"âŒ æ‚ªåŒ–\"\n",
    "            print(f\"\\nå¹³å‡MAE:\")\n",
    "            print(f\"  exp16: {exp16_avg_mae:.2f}\")\n",
    "            print(f\"  exp17: {exp17_avg_mae:.2f}\")\n",
    "            print(f\"  å·®åˆ†: {improvement:+.2f} ({improvement_pct:+.1f}%) {status}\")\n",
    "            \n",
    "            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã®è©³ç´°\n",
    "            print(f\"\\nã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã®æ¯”è¼ƒ:\")\n",
    "            for window_num in sorted(exp17_model['window'].unique()):\n",
    "                exp16_mae = exp16_model[exp16_model['window'] == window_num]['MAE'].values\n",
    "                exp17_mae = exp17_model[exp17_model['window'] == window_num]['MAE'].values\n",
    "                \n",
    "                if len(exp16_mae) > 0 and len(exp17_mae) > 0:\n",
    "                    win_improvement = exp16_mae[0] - exp17_mae[0]\n",
    "                    win_status = \"âœ…\" if win_improvement > 0 else \"âŒ\"\n",
    "                    print(f\"  Window{window_num}: exp16={exp16_mae[0]:.2f} â†’ exp17={exp17_mae[0]:.2f} ({win_improvement:+.2f}) {win_status}\")\n",
    "else:\n",
    "    print(\"\\nexp16ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "    print(\"exp17ã®çµæœã®ã¿ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# çµæœã®é›†è¨ˆ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"exp17 å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®çµæœ\")\n",
    "print(\"=\" * 80)\n",
    "print(exp17_results_df.to_string(index=False))\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å¹³å‡ã‚¹ã‚³ã‚¢\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å¹³å‡ã‚¹ã‚³ã‚¢ï¼ˆå…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "average_scores = exp17_results_df.groupby('model')[['MAE', 'RMSE', 'R2', 'WAPE']].mean()\n",
    "average_scores = average_scores.sort_values('MAE')\n",
    "print(average_scores.to_string())\n",
    "\n",
    "# æ¨™æº–åå·®\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®æ¨™æº–åå·®ï¼ˆã‚¹ã‚³ã‚¢ã®å®‰å®šæ€§ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "std_scores = exp17_results_df.groupby('model')[['MAE', 'RMSE', 'R2', 'WAPE']].std()\n",
    "std_scores = std_scores.sort_values('MAE')\n",
    "print(std_scores.to_string())\n",
    "\n",
    "# CSVä¿å­˜\n",
    "output_dir = '../output/exp17'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "exp17_results_df.to_csv(f'{output_dir}/rolling_window_results.csv', index=False)\n",
    "average_scores.to_csv(f'{output_dir}/average_scores.csv')\n",
    "std_scores.to_csv(f'{output_dir}/std_scores.csv')\n",
    "\n",
    "print(f\"\\nçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# å¯è¦–åŒ–: exp16 vs exp17 æ¯”è¼ƒ\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if os.path.exists(exp16_results_path):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('exp16 vs exp17: MAE Comparison by Window', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models_to_compare = ['HistGradientBoosting', 'ExtraTrees', 'CatBoost', 'WeightedEnsemble_A']\n",
    "    \n",
    "    for idx, model in enumerate(models_to_compare):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        exp16_model_data = exp16_results_df[exp16_results_df['model'] == model]\n",
    "        exp17_model_data = exp17_results_df[exp17_results_df['model'] == model]\n",
    "        \n",
    "        ax.plot(exp16_model_data['window'], exp16_model_data['MAE'], \n",
    "                marker='o', label='exp16 (baseline)', linewidth=2, color='steelblue')\n",
    "        ax.plot(exp17_model_data['window'], exp17_model_data['MAE'], \n",
    "                marker='s', label='exp17 (+ advanced acc_get)', linewidth=2, color='coral')\n",
    "        \n",
    "        ax.set_xlabel('Window', fontsize=12)\n",
    "        ax.set_ylabel('MAE', fontsize=12)\n",
    "        ax.set_title(f'{model}', fontsize=12, fontweight='bold')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_xticks(range(1, len(windows) + 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/exp16_vs_exp17_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nå¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}/exp16_vs_exp17_comparison.png\")\n",
    "else:\n",
    "    print(\"\\nexp16ã®çµæœãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ¯”è¼ƒå¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**exp17: acc_geté«˜åº¦ç‰¹å¾´é‡ã®è¿½åŠ **\n",
    "\n",
    "### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:\n",
    "- exp16ï¼ˆacc_getåŸºæœ¬ç‰¹å¾´é‡ï¼‰\n",
    "\n",
    "### è¿½åŠ ã—ãŸç‰¹å¾´é‡ (4å€‹):\n",
    "1. `acc_get_diff1`: å‰æ—¥å·®åˆ† = acc_get_cnt.shift(1) - acc_get_cnt.shift(2)\n",
    "   - æ€¥å¢—ãƒ»æ€¥æ¸›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹\n",
    "2. `acc_get_sum_28d`: ç›´è¿‘28æ—¥é–“ã®ç´¯ç©ï¼ˆshift(1)æ¸ˆã¿ï¼‰\n",
    "   - é•·æœŸçš„ãªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ‰ãˆã‚‹\n",
    "3. `acc_get_ema_14`: 14æ—¥æŒ‡æ•°ç§»å‹•å¹³å‡\n",
    "   - æ€¥å¢—ã«è¿½å¾“ã—ã‚„ã™ã„ï¼ˆé€šå¸¸ã®ç§»å‹•å¹³å‡ã‚ˆã‚Šåå¿œãŒæ—©ã„ï¼‰\n",
    "4. `acc_get_growth`: sum_7d / (sum_28d + eps)\n",
    "   - ã€Œæœ€è¿‘ã®å‹¢ã„ã€ã‚’è¡¨ç¾ï¼ˆçŸ­æœŸ/é•·æœŸæ¯”ç‡ï¼‰\n",
    "\n",
    "### ç‰¹å¾´é‡ã®æ„å›³:\n",
    "- **å·®åˆ†ï¼ˆdiff1ï¼‰**: æ—¥æ¬¡å¤‰åŒ–ç‡ã‚’æ‰ãˆã‚‹\n",
    "- **é•·æœŸç´¯ç©ï¼ˆsum_28dï¼‰**: 1ãƒ¶æœˆã‚¹ãƒ‘ãƒ³ã®ãƒˆãƒ¬ãƒ³ãƒ‰\n",
    "- **æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆEMAï¼‰**: æœ€è¿‘ã®å€¤ã«é‡ã¿ã‚’ç½®ã„ãŸå¹³æ»‘åŒ–\n",
    "- **æˆé•·ç‡ï¼ˆgrowthï¼‰**: çŸ­æœŸçš„ãªåŠ é€Ÿãƒ»æ¸›é€Ÿã‚’å®šé‡åŒ–\n",
    "\n",
    "### ç‰¹å¾´é‡æ•°ã®æ¨ç§»:\n",
    "- exp15: 48ç‰¹å¾´é‡ï¼ˆãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ï¼‰\n",
    "- exp16: 50ç‰¹å¾´é‡ï¼ˆ+ acc_getåŸºæœ¬2å€‹ï¼‰\n",
    "- **exp17: 54ç‰¹å¾´é‡**ï¼ˆ+ acc_geté«˜åº¦4å€‹ï¼‰\n",
    "\n",
    "### æ¤œè¨¼çµæœ:\n",
    "- exp16ã¨ã®æ¯”è¼ƒã§MAEã®æ”¹å–„ã‚’ç¢ºèª\n",
    "- acc_getã®å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å‹¢ã„ã‚’å¤šè§’çš„ã«æ‰ãˆã‚‹ã“ã¨ã«æˆåŠŸ\n",
    "\n",
    "### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:\n",
    "1. `rolling_window_results.csv`\n",
    "2. `average_scores.csv`\n",
    "3. `std_scores.csv`\n",
    "4. `exp16_vs_exp17_comparison.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
